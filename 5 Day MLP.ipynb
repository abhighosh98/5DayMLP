{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib as tb\n",
    "from talib import MA_Type,RSI, MACD, STOCH\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pandas_datareader import data, wb\n",
    "import pandas_datareader.data as web\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from math import floor\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, LeakyReLU, Flatten, Conv1D, MaxPool1D, LSTM, Input\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, accuracy_score\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_start_date = datetime.datetime(2020,1,1)\n",
    "# test_end_date = datetime.datetime(2022,4,1)\n",
    "train_start_date = datetime.datetime(1980,1,1)\n",
    "test_end_date = datetime.datetime(2022,5,10)\n",
    "# model = load_model('RSI_Model.h5')\n",
    "lookAheadPeriod = 1\n",
    "cutOff = 14   \n",
    "cutOff = 17\n",
    "macdCutOff = 33\n",
    "threshold = 0.5\n",
    "all_model = load_model('ALL_model.h5')\n",
    "pred_cols = ['rsi0', 'rsi1', 'rsi2', 'rsi3', 'rsi4', 'stochk0', 'stochk1', 'stochk2',\n",
    "       'stochk3', 'stochk4', 'stochd0', 'stochd1', 'stochd2', 'stochd3',\n",
    "       'stochd4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning_RSI(ticker_symbol, n):\n",
    "#     ticker_symbol.index = [i.date() for i in ticker_symbol.index]\n",
    "    dur = 5\n",
    "    ticker_symbol.index = [i for i in ticker_symbol.index]\n",
    "    closePrice = ticker_symbol['Close'].to_numpy()\n",
    "    dateTimePrice = ticker_symbol.index.values\n",
    "    \n",
    "    shiftDateTime = dateTimePrice[:-lookAheadPeriod]\n",
    "    shiftClosePrice = closePrice[lookAheadPeriod:]\n",
    "\n",
    "    closePrice = closePrice[:-lookAheadPeriod]\n",
    "    RSI_14 = RSI(closePrice, timeperiod=14)\n",
    "    RSI_14 = RSI_14[cutOff:]\n",
    "    \n",
    "    closeDiff = shiftClosePrice - closePrice\n",
    "    closeDiffLength = len(closeDiff)\n",
    "    \n",
    "    longOP = np.zeros(closeDiffLength)\n",
    "    longOP[closeDiff >= 0] = 1\n",
    "\n",
    "    # Sell if closing price is lesser in the end\n",
    "    shortOP = np.zeros(closeDiffLength)\n",
    "    shortOP[closeDiff < 0] = 1\n",
    "    \n",
    "    RSI_df = pd.DataFrame()\n",
    "    a = 0\n",
    "    for i in range(n,len(RSI_14),n):\n",
    "        RSI_df = pd.concat([RSI_df,pd.DataFrame(RSI_14[a:i].reshape(1,n))])\n",
    "        a = i\n",
    "    RSI_df = RSI_df.reset_index(drop=True)\n",
    "    newClosePrice = closePrice[14:]\n",
    "    store_diff = []\n",
    "    first_index = 0\n",
    "    for i in range(n,len(newClosePrice) - dur, n):\n",
    "        store_diff.append(newClosePrice[i + dur] - newClosePrice[first_index + 1])\n",
    "        first_index = i\n",
    "    newCloseDiffLength = len(store_diff)\n",
    "    newLongOP = np.zeros(newCloseDiffLength)\n",
    "    newLongOP[np.array(store_diff) >= 0] = 1\n",
    "    RSI_with_period = pd.concat([RSI_df, pd.DataFrame(newLongOP,columns=['long_or_short'])],axis=1)\n",
    "    return RSI_with_period\n",
    "\n",
    "def make_dfs(df,n):\n",
    "    df_ind = pd.DataFrame()\n",
    "    a = 0\n",
    "    for i in range(n,len(df), n):\n",
    "        df_ind = pd.concat([df_ind,pd.DataFrame(df[a:i].reshape(1,n))])\n",
    "        a = i\n",
    "    df_ind = df_ind.reset_index(drop=True)\n",
    "    return df_ind\n",
    "\n",
    "def make_dfs_abhi(df,n):\n",
    "    df_ind = pd.DataFrame()\n",
    "    a = 0\n",
    "    for i in range(n,len(df)):\n",
    "        df_ind = pd.concat([df_ind,pd.DataFrame(df[a:i].reshape(1,n))])\n",
    "        a = i - 4\n",
    "    df_ind = df_ind.reset_index(drop=True)\n",
    "    return df_ind\n",
    "\n",
    "\n",
    "def data_cleaning_for_all_indicators_abhi(ticker_symbol, n):\n",
    "    dur = 5\n",
    "    closePrice = ticker_symbol['Close'].to_numpy()\n",
    "    RSI_14 = RSI(ticker_symbol['Close'], timeperiod=14)\n",
    "    RSI_14 = RSI_14[cutOff:].to_numpy()\n",
    "    STOCH14K, STOCH14D = STOCH(\n",
    "            ticker_symbol['High'], ticker_symbol['Low'], ticker_symbol['Close'], fastk_period=14, slowk_period=3, slowd_period=3)\n",
    "    STOCH14K = STOCH14K[cutOff:].to_numpy()\n",
    "    STOCH14D = STOCH14D[cutOff:].to_numpy()\n",
    "    RSI_with_period = make_dfs_abhi(RSI_14,n)\n",
    "    STOCH14K_with_period = make_dfs_abhi(STOCH14K,n)\n",
    "    STOCH14D_with_period = make_dfs_abhi(STOCH14D,n)\n",
    "    close_with_period = make_dfs_abhi(closePrice[17 : ],n)\n",
    "    for i in range(n):\n",
    "        RSI_with_period = RSI_with_period.rename(columns={i:'rsi'+str(i)})\n",
    "        STOCH14K_with_period = STOCH14K_with_period.rename(columns={i:'stochk'+str(i)})\n",
    "        STOCH14D_with_period = STOCH14D_with_period.rename(columns={i:'stochd'+str(i)})\n",
    "        close_with_period = close_with_period.rename(columns={i:'close'+str(i)})\n",
    "    final_df_all_inds = pd.concat([RSI_with_period, STOCH14K_with_period, STOCH14D_with_period, close_with_period],axis=1)\n",
    "    newClosePrice = closePrice[17:]\n",
    "    store_diff = []\n",
    "    first_index = 0\n",
    "    for i in range(n,len(newClosePrice) - dur):\n",
    "#         print(i + dur - 1, newClosePrice[i + dur - 1], first_index + 1, newClosePrice[first_index + 1])\n",
    "        store_diff.append(newClosePrice[i + dur] - newClosePrice[first_index + 1])\n",
    "        first_index = i\n",
    "    newCloseDiffLength = len(store_diff)\n",
    "    newLongOP = np.zeros(newCloseDiffLength)\n",
    "    newLongOP[np.array(store_diff) >= 0] = 1\n",
    "    final_df = pd.concat([final_df_all_inds, pd.DataFrame(newLongOP,columns=['long_or_short'])],axis=1)\n",
    "    return final_df\n",
    "\n",
    "def data_cleaning_for_all_indicators(ticker_symbol, n):\n",
    "    dur = 5\n",
    "#     ticker_symbol.index = [i.date() for i in ticker_symbol.index]\n",
    "    closePrice = ticker_symbol['Close'].to_numpy()\n",
    "    dateTimePrice = ticker_symbol.index.values\n",
    "    highList = ticker_symbol['High'].to_numpy()\n",
    "    lowList = ticker_symbol['Low'].to_numpy()\n",
    "    shiftDateTime = dateTimePrice[:-lookAheadPeriod]\n",
    "    shiftClosePrice = closePrice[lookAheadPeriod:]\n",
    "    highList = highList[:-lookAheadPeriod]\n",
    "    lowList = lowList[:-lookAheadPeriod]\n",
    "    closePrice = closePrice[:-lookAheadPeriod]\n",
    "    RSI_14 = RSI(closePrice, timeperiod=14)\n",
    "    RSI_14 = RSI_14[cutOff:]\n",
    "    STOCH14K, STOCH14D = STOCH(\n",
    "            highList, lowList, closePrice, fastk_period=14, slowk_period=3, slowd_period=3)\n",
    "    STOCH14K = STOCH14K[cutOff:]\n",
    "    STOCH14D = STOCH14D[cutOff:]\n",
    "    closeDiff = shiftClosePrice - closePrice\n",
    "    closeDiffLength = len(closeDiff)\n",
    "    longOP = np.zeros(closeDiffLength)\n",
    "    longOP[closeDiff >= 0] = 1\n",
    "    # Sell if closing price is lesser in the end\n",
    "    shortOP = np.zeros(closeDiffLength)\n",
    "    shortOP[closeDiff < 0] = 1\n",
    "    RSI_with_period = make_dfs(RSI_14,n)\n",
    "    STOCH14K_with_period = make_dfs(STOCH14K,n)\n",
    "    STOCH14D_with_period = make_dfs(STOCH14D,n)\n",
    "    for i in range(n):\n",
    "        RSI_with_period = RSI_with_period.rename(columns={i:'rsi'+str(i)})\n",
    "        STOCH14K_with_period = STOCH14K_with_period.rename(columns={i:'stochk'+str(i)})\n",
    "        STOCH14D_with_period = STOCH14D_with_period.rename(columns={i:'stochd'+str(i)})\n",
    "    final_df_all_inds = pd.concat([RSI_with_period, STOCH14K_with_period, STOCH14D_with_period],axis=1)\n",
    "    newClosePrice = closePrice[17:]\n",
    "    store_diff = []\n",
    "    first_index = 0\n",
    "    for i in range(n,len(newClosePrice) - dur, n):\n",
    "#         print(i + dur, first_index + 5)\n",
    "        store_diff.append(newClosePrice[i + dur] - newClosePrice[first_index + 1])\n",
    "        first_index = i\n",
    "    newCloseDiffLength = len(store_diff)\n",
    "    newLongOP = np.zeros(newCloseDiffLength)\n",
    "    newLongOP[np.array(store_diff) >= 0] = 1\n",
    "    final_df = pd.concat([final_df_all_inds, pd.DataFrame(newLongOP,columns=['long_or_short'])],axis=1)\n",
    "    return final_df\n",
    "\n",
    "def get_backtest_df_pass_df(mode, d, thresh_long, thresh_short, train_start_date, test_end_date, dur = 5):\n",
    "    pd_datareader_data = d\n",
    "    total = len(pd_datareader_data)\n",
    "    pd_datareader_data['RSI_14'] = RSI(pd_datareader_data['Close'], timeperiod=14)\n",
    "    \n",
    "\n",
    "    if(mode == \"ALL\"):\n",
    "        backtest_company_price_data = data_cleaning_for_all_indicators(pd_datareader_data, 5)\n",
    "        model = load_model('ALL_model.h5')\n",
    "    elif(mode == \"RSI\"):\n",
    "        backtest_company_price_data = data_cleaning_RSI(pd_datareader_data, 5)\n",
    "        model = load_model('RSI_Model.h5')\n",
    "    \n",
    "    \n",
    "    #Changing price data to drop rows after creating df\n",
    "    pd_datareader_data = pd_datareader_data[15 :] # Row mismatch fix requires logic for dropping rows\n",
    "    # Model prediction on test data\n",
    "    y_pred_backtest = model.predict(backtest_company_price_data.drop(columns = ['long_or_short']))\n",
    "    backtest_df = backtest_company_price_data.copy()\n",
    "    backtest_df['prob_pred'] = y_pred_backtest\n",
    "    # Creating a column with high confidence outputs for long/short\n",
    "    backtest_df['thresh_signal'] = backtest_df['prob_pred'].apply(lambda x: 1 if x > thresh_long else (0 if x < thresh_short else np.nan))\n",
    "    backtest_df.reset_index(inplace = True, drop = True)\n",
    "    high_conf_stats =  backtest_df[['thresh_signal', 'long_or_short']].dropna()\n",
    "#     print(\"\\nPredictions of high confidence = \", \n",
    "#           len(high_conf_stats), \"\\n\")\n",
    "#     print(\"\\nClassification Report\", classification_report(high_conf_stats['long_or_short'],\n",
    "#                             high_conf_stats['thresh_signal']), \"\\n\")\n",
    "    \n",
    "    \n",
    "    high_conf_colormap = []\n",
    "    buy_sell_signal_from_model = []\n",
    "    for s in backtest_df['thresh_signal'].values:\n",
    "        if (s == 0.0):\n",
    "            for i in range(5):\n",
    "                high_conf_colormap.append('k')\n",
    "                buy_sell_signal_from_model.append(0)\n",
    "        elif(s == 1.0):\n",
    "            for i in range(5):\n",
    "                high_conf_colormap.append('y')\n",
    "                buy_sell_signal_from_model.append(1)\n",
    "        else:\n",
    "            for i in range(5):\n",
    "                high_conf_colormap.append('b')\n",
    "                buy_sell_signal_from_model.append(np.nan)\n",
    "\n",
    "    \n",
    "    # Fix for df shape mismatch\n",
    "    row_mismatch_count = pd_datareader_data.shape[0] - len(buy_sell_signal_from_model)\n",
    "    pd_datareader_data = pd_datareader_data[ : -row_mismatch_count]\n",
    "    pd_datareader_data['buy_sell_signal_from_model'] = buy_sell_signal_from_model\n",
    "    \n",
    "    # Profit/Loss calculation for long signals\n",
    "    profit_long = []\n",
    "    buy_price = []\n",
    "    sell_price = []\n",
    "    \n",
    "    for i in range(len(pd_datareader_data) - dur):\n",
    "        if(pd_datareader_data.iloc[i]['buy_sell_signal_from_model'] == 1.0):\n",
    "            profit = pd_datareader_data.iloc[i + dur]['Close'] - pd_datareader_data.iloc[i + 1]['Close']\n",
    "            profit_long.append(profit)\n",
    "            buy_price.append(pd_datareader_data.iloc[i + 1]['Close'])\n",
    "            sell_price.append(pd_datareader_data.iloc[i + dur]['Close'])\n",
    "        else:\n",
    "            profit_long.append(np.nan)\n",
    "            buy_price.append(np.nan)\n",
    "            sell_price.append(np.nan)\n",
    "            \n",
    "    # For the last \"dur\" days we cannot get a profit since those days dont exist. Hence just adding nans. \n",
    "    for i in range(dur):\n",
    "        profit_long.append(np.nan)\n",
    "        buy_price.append(np.nan)\n",
    "        sell_price.append(np.nan)\n",
    "\n",
    "    pd_datareader_data['profit_long'] = profit_long\n",
    "    pd_datareader_data['buy_price'] = buy_price\n",
    "    pd_datareader_data['sell_price'] = sell_price\n",
    "    pd_datareader_data['profit_percentage_per_trade'] = (pd_datareader_data['sell_price'] - pd_datareader_data['buy_price']) / pd_datareader_data['buy_price'] * 100\n",
    "    return(pd_datareader_data[pd_datareader_data['buy_sell_signal_from_model'] == 1], total)\n",
    "\n",
    "def get_backtest_df_pass_df_abhi(mode, d, thresh_long, thresh_short, train_start_date, test_end_date, dur = 5):\n",
    "    pd_datareader_data = d\n",
    "    total = len(pd_datareader_data)\n",
    "    pd_datareader_data['RSI_14'] = RSI(pd_datareader_data['Close'], timeperiod=14)\n",
    "    \n",
    "\n",
    "    if(mode == \"ALL\"):\n",
    "        backtest_company_price_data = data_cleaning_for_all_indicators(pd_datareader_data, 5)\n",
    "        model = load_model('ALL_model.h5')\n",
    "    elif(mode == \"RSI\"):\n",
    "        backtest_company_price_data = data_cleaning_RSI(pd_datareader_data, 5)\n",
    "        model = load_model('RSI_Model.h5')\n",
    "    \n",
    "    \n",
    "    #Changing price data to drop rows after creating df\n",
    "    pd_datareader_data = pd_datareader_data[15 :] # Row mismatch fix requires logic for dropping rows\n",
    "    # Model prediction on test data\n",
    "    y_pred_backtest = model.predict(backtest_company_price_data.drop(columns = ['long_or_short']))\n",
    "    backtest_df = backtest_company_price_data.copy()\n",
    "    backtest_df['prob_pred'] = y_pred_backtest\n",
    "    # Creating a column with high confidence outputs for long/short\n",
    "    backtest_df['thresh_signal'] = backtest_df['prob_pred'].apply(lambda x: 1 if x > thresh_long else (0 if x < thresh_short else np.nan))\n",
    "    backtest_df.reset_index(inplace = True, drop = True)\n",
    "    high_conf_stats =  backtest_df[['thresh_signal', 'long_or_short']].dropna()\n",
    "#     print(\"\\nPredictions of high confidence = \", \n",
    "#           len(high_conf_stats), \"\\n\")\n",
    "#     print(\"\\nClassification Report\", classification_report(high_conf_stats['long_or_short'],\n",
    "#                             high_conf_stats['thresh_signal']), \"\\n\")\n",
    "    \n",
    "    \n",
    "    high_conf_colormap = []\n",
    "    buy_sell_signal_from_model = []\n",
    "    for s in backtest_df['thresh_signal'].values:\n",
    "        if (s == 0.0):\n",
    "            for i in range(1):\n",
    "                high_conf_colormap.append('k')\n",
    "                buy_sell_signal_from_model.append(0)\n",
    "        elif(s == 1.0):\n",
    "            for i in range(1):\n",
    "                high_conf_colormap.append('y')\n",
    "                buy_sell_signal_from_model.append(1)\n",
    "        else:\n",
    "            for i in range(1):\n",
    "                high_conf_colormap.append('b')\n",
    "                buy_sell_signal_from_model.append(np.nan)\n",
    "\n",
    "    \n",
    "    # Fix for df shape mismatch\n",
    "    row_mismatch_count = pd_datareader_data.shape[0] - len(buy_sell_signal_from_model)\n",
    "    pd_datareader_data = pd_datareader_data[ : -row_mismatch_count]\n",
    "    pd_datareader_data['buy_sell_signal_from_model'] = buy_sell_signal_from_model\n",
    "    \n",
    "    # Profit/Loss calculation for long signals\n",
    "    profit_long = []\n",
    "    buy_price = []\n",
    "    sell_price = []\n",
    "    \n",
    "    for i in range(len(pd_datareader_data) - dur):\n",
    "        if(pd_datareader_data.iloc[i]['buy_sell_signal_from_model'] == 1.0):\n",
    "            profit = pd_datareader_data.iloc[i + dur]['Close'] - pd_datareader_data.iloc[i + 1]['Close']\n",
    "            profit_long.append(profit)\n",
    "            buy_price.append(pd_datareader_data.iloc[i + 1]['Close'])\n",
    "            sell_price.append(pd_datareader_data.iloc[i + dur]['Close'])\n",
    "        else:\n",
    "            profit_long.append(np.nan)\n",
    "            buy_price.append(np.nan)\n",
    "            sell_price.append(np.nan)\n",
    "            \n",
    "    # For the last \"dur\" days we cannot get a profit since those days dont exist. Hence just adding nans. \n",
    "    for i in range(dur):\n",
    "        profit_long.append(np.nan)\n",
    "        buy_price.append(np.nan)\n",
    "        sell_price.append(np.nan)\n",
    "\n",
    "    pd_datareader_data['profit_long'] = profit_long\n",
    "    pd_datareader_data['buy_price'] = buy_price\n",
    "    pd_datareader_data['sell_price'] = sell_price\n",
    "    pd_datareader_data['profit_percentage_per_trade'] = (pd_datareader_data['sell_price'] - pd_datareader_data['buy_price']) / pd_datareader_data['buy_price'] * 100\n",
    "    return(pd_datareader_data[pd_datareader_data['buy_sell_signal_from_model'] == 1], total)\n",
    "\n",
    "\n",
    "\n",
    "def get_backtest_df(mode, ticker_for_backtest, thresh_long, thresh_short, train_start_date, test_end_date, dur = 5):\n",
    "    pd_datareader_data = data.DataReader(ticker_for_backtest, 'yahoo', train_start_date, test_end_date)\n",
    "    total = len(pd_datareader_data)\n",
    "    pd_datareader_data['RSI_14'] = RSI(pd_datareader_data['Close'], timeperiod=14)\n",
    "    \n",
    "\n",
    "    if(mode == \"ALL\"):\n",
    "        backtest_company_price_data = data_cleaning_for_all_indicators_abhi(pd_datareader_data, 5)\n",
    "        model = load_model('ALL_model.h5')\n",
    "    elif(mode == \"RSI\"):\n",
    "        backtest_company_price_data = data_cleaning_RSI(pd_datareader_data, 5)\n",
    "        model = load_model('RSI_Model.h5')\n",
    "    \n",
    "    \n",
    "    #Changing price data to drop rows after creating df\n",
    "    pd_datareader_data = pd_datareader_data[15 :] # Row mismatch fix requires logic for dropping rows\n",
    "    # Model prediction on test data\n",
    "    y_pred_backtest = model.predict(backtest_company_price_data.drop(columns = ['long_or_short']))\n",
    "    backtest_df = backtest_company_price_data.copy()\n",
    "    backtest_df['prob_pred'] = y_pred_backtest\n",
    "    # Creating a column with high confidence outputs for long/short\n",
    "    backtest_df['thresh_signal'] = backtest_df['prob_pred'].apply(lambda x: 1 if x > thresh_long else (0 if x < thresh_short else np.nan))\n",
    "    backtest_df.reset_index(inplace = True, drop = True)\n",
    "    high_conf_stats =  backtest_df[['thresh_signal', 'long_or_short']].dropna()\n",
    "#     print(\"\\nPredictions of high confidence = \", \n",
    "#           len(high_conf_stats), \"\\n\")\n",
    "#     print(\"\\nClassification Report\", classification_report(high_conf_stats['long_or_short'],\n",
    "#                             high_conf_stats['thresh_signal']), \"\\n\")\n",
    "    \n",
    "    \n",
    "    high_conf_colormap = []\n",
    "    buy_sell_signal_from_model = []\n",
    "    for s in backtest_df['thresh_signal'].values:\n",
    "        if (s == 0.0):\n",
    "            for i in range(5):\n",
    "                high_conf_colormap.append('k')\n",
    "                buy_sell_signal_from_model.append(0)\n",
    "        elif(s == 1.0):\n",
    "            for i in range(5):\n",
    "                high_conf_colormap.append('y')\n",
    "                buy_sell_signal_from_model.append(1)\n",
    "        else:\n",
    "            for i in range(5):\n",
    "                high_conf_colormap.append('b')\n",
    "                buy_sell_signal_from_model.append(np.nan)\n",
    "\n",
    "    \n",
    "    # Fix for df shape mismatch\n",
    "    row_mismatch_count = pd_datareader_data.shape[0] - len(buy_sell_signal_from_model)\n",
    "    pd_datareader_data = pd_datareader_data[ : -row_mismatch_count]\n",
    "    pd_datareader_data['buy_sell_signal_from_model'] = buy_sell_signal_from_model\n",
    "    \n",
    "    # Profit/Loss calculation for long signals\n",
    "    profit_long = []\n",
    "    buy_price = []\n",
    "    sell_price = []\n",
    "    \n",
    "    for i in range(len(pd_datareader_data) - dur):\n",
    "        if(pd_datareader_data.iloc[i]['buy_sell_signal_from_model'] == 1.0):\n",
    "            profit = pd_datareader_data.iloc[i + dur]['Close'] - pd_datareader_data.iloc[i + 1]['Close']\n",
    "            profit_long.append(profit)\n",
    "            buy_price.append(pd_datareader_data.iloc[i + 1]['Close'])\n",
    "            sell_price.append(pd_datareader_data.iloc[i + dur]['Close'])\n",
    "        else:\n",
    "            profit_long.append(np.nan)\n",
    "            buy_price.append(np.nan)\n",
    "            sell_price.append(np.nan)\n",
    "            \n",
    "    # For the last \"dur\" days we cannot get a profit since those days dont exist. Hence just adding nans. \n",
    "    for i in range(dur):\n",
    "        profit_long.append(np.nan)\n",
    "        buy_price.append(np.nan)\n",
    "        sell_price.append(np.nan)\n",
    "\n",
    "    pd_datareader_data['profit_long'] = profit_long\n",
    "    pd_datareader_data['buy_price'] = buy_price\n",
    "    pd_datareader_data['sell_price'] = sell_price\n",
    "    pd_datareader_data['profit_percentage_per_trade'] = (pd_datareader_data['sell_price'] - pd_datareader_data['buy_price']) / pd_datareader_data['buy_price'] * 100\n",
    "    return(pd_datareader_data[pd_datareader_data['buy_sell_signal_from_model'] == 1], total)\n",
    "\n",
    "def get_backtest_df_abhi(mode, ticker_for_backtest, thresh_long, thresh_short, train_start_date, test_end_date, dur = 5):\n",
    "    pd_datareader_data = data.DataReader(ticker_for_backtest, 'yahoo', train_start_date, test_end_date)\n",
    "    total = len(pd_datareader_data)\n",
    "    pd_datareader_data['RSI_14'] = RSI(pd_datareader_data['Close'], timeperiod=14)\n",
    "    \n",
    "\n",
    "    if(mode == \"ALL\"):\n",
    "        backtest_company_price_data = data_cleaning_for_all_indicators_abhi(pd_datareader_data, 5)\n",
    "        model = load_model('ALL_model.h5')\n",
    "    elif(mode == \"RSI\"):\n",
    "        backtest_company_price_data = data_cleaning_RSI(pd_datareader_data, 5)\n",
    "        model = load_model('RSI_Model.h5')\n",
    "    \n",
    "    \n",
    "    #Changing price data to drop rows after creating df\n",
    "    pd_datareader_data = pd_datareader_data[15 :] # Row mismatch fix requires logic for dropping rows\n",
    "    # Model prediction on test data\n",
    "    y_pred_backtest = model.predict(backtest_company_price_data.drop(columns = ['long_or_short']))\n",
    "    backtest_df = backtest_company_price_data.copy()\n",
    "    backtest_df['prob_pred'] = y_pred_backtest\n",
    "    # Creating a column with high confidence outputs for long/short\n",
    "    backtest_df['thresh_signal'] = backtest_df['prob_pred'].apply(lambda x: 1 if x > thresh_long else (0 if x < thresh_short else np.nan))\n",
    "    backtest_df.reset_index(inplace = True, drop = True)\n",
    "    high_conf_stats =  backtest_df[['thresh_signal', 'long_or_short']].dropna()\n",
    "#     print(\"\\nPredictions of high confidence = \", \n",
    "#           len(high_conf_stats), \"\\n\")\n",
    "#     print(\"\\nClassification Report\", classification_report(high_conf_stats['long_or_short'],\n",
    "#                             high_conf_stats['thresh_signal']), \"\\n\")\n",
    "    \n",
    "    \n",
    "    high_conf_colormap = []\n",
    "    buy_sell_signal_from_model = []\n",
    "    for s in backtest_df['thresh_signal'].values:\n",
    "        if (s == 0.0):\n",
    "            for i in range(1):\n",
    "                high_conf_colormap.append('k')\n",
    "                buy_sell_signal_from_model.append(0)\n",
    "        elif(s == 1.0):\n",
    "            for i in range(1):\n",
    "                high_conf_colormap.append('y')\n",
    "                buy_sell_signal_from_model.append(1)\n",
    "        else:\n",
    "            for i in range(1):\n",
    "                high_conf_colormap.append('b')\n",
    "                buy_sell_signal_from_model.append(np.nan)\n",
    "\n",
    "    \n",
    "    # Fix for df shape mismatch\n",
    "    row_mismatch_count = pd_datareader_data.shape[0] - len(buy_sell_signal_from_model)\n",
    "    pd_datareader_data = pd_datareader_data[ : -row_mismatch_count]\n",
    "    pd_datareader_data['buy_sell_signal_from_model'] = buy_sell_signal_from_model\n",
    "    \n",
    "    # Profit/Loss calculation for long signals\n",
    "    profit_long = []\n",
    "    buy_price = []\n",
    "    sell_price = []\n",
    "    \n",
    "    for i in range(len(pd_datareader_data) - dur):\n",
    "        if(pd_datareader_data.iloc[i]['buy_sell_signal_from_model'] == 1.0):\n",
    "            profit = pd_datareader_data.iloc[i + dur]['Close'] - pd_datareader_data.iloc[i + 1]['Close']\n",
    "            profit_long.append(profit)\n",
    "            buy_price.append(pd_datareader_data.iloc[i + 1]['Close'])\n",
    "            sell_price.append(pd_datareader_data.iloc[i + dur]['Close'])\n",
    "        else:\n",
    "            profit_long.append(np.nan)\n",
    "            buy_price.append(np.nan)\n",
    "            sell_price.append(np.nan)\n",
    "            \n",
    "    # For the last \"dur\" days we cannot get a profit since those days dont exist. Hence just adding nans. \n",
    "    for i in range(dur):\n",
    "        profit_long.append(np.nan)\n",
    "        buy_price.append(np.nan)\n",
    "        sell_price.append(np.nan)\n",
    "\n",
    "    pd_datareader_data['profit_long'] = profit_long\n",
    "    pd_datareader_data['buy_price'] = buy_price\n",
    "    pd_datareader_data['sell_price'] = sell_price\n",
    "    pd_datareader_data['profit_percentage_per_trade'] = (pd_datareader_data['sell_price'] - pd_datareader_data['buy_price']) / pd_datareader_data['buy_price'] * 100\n",
    "    return(pd_datareader_data[pd_datareader_data['buy_sell_signal_from_model'] == 1], total)\n",
    "\n",
    "\n",
    "def get_backtest_stats(pd_datareader_data, principal = 100):\n",
    "    return_dict = {}\n",
    "    # Total length of data\n",
    "#     return_dict['total_length_of_data'] =  pd_datareader_data.shape[0]\n",
    "    return_dict['total_length_of_data'] =  pd_datareader_data[pd_datareader_data['buy_sell_signal_from_model'] == 1].shape[0]\n",
    "    # Length of data with decisions above confidence values\n",
    "    return_dict['length_of_high_confidence_data'] = pd_datareader_data['profit_percentage_per_trade'].notna().sum()\n",
    "    # Mean profit per trade\n",
    "    return_dict['mean_profit_per_trade'] = pd_datareader_data['profit_percentage_per_trade'].mean()\n",
    "    # Profit from holding stock from start date to end date\n",
    "    return_dict['layman_profit'] = (pd_datareader_data.iloc[-1]['Close'] - pd_datareader_data.iloc[0]['Close'])/pd_datareader_data.iloc[0]['Close'] * 100\n",
    "    # Calculating the returns based on an initial investment\n",
    "    for pp in pd_datareader_data['profit_percentage_per_trade'].dropna().values:\n",
    "#         print((100 + pp) / 100)\n",
    "        principal = principal * ((100 + pp) / 100)\n",
    "#         print(principal,\"\\n\\n\")\n",
    "    return_dict['returns'] = principal\n",
    "    return(return_dict)\n",
    "\n",
    "def predict_given_date_ticker(model, pred_date, ticker):\n",
    "    pred_start_date = pred_date - datetime.timedelta(50)\n",
    "#     print(pred_start_date)\n",
    "    pred_df = data.DataReader(ticker, 'yahoo', pred_start_date, pred_date)\n",
    "    RSI_14 = RSI(pred_df['Close'], timeperiod=14)\n",
    "#     print(RSI_14.index[-1])\n",
    "    closePrice = pred_df['Close'].to_numpy()\n",
    "    dateTimePrice = pred_df.index.values\n",
    "    highList = pred_df['High'].to_numpy()\n",
    "    lowList = pred_df['Low'].to_numpy()\n",
    "    shiftDateTime = dateTimePrice[:-lookAheadPeriod]\n",
    "    shiftClosePrice = closePrice[lookAheadPeriod:]\n",
    "    STOCH14K, STOCH14D = STOCH(highList, lowList, pred_df['Close'], fastk_period=14, slowk_period=3, slowd_period=3)\n",
    "    temp = pd.concat([pd.DataFrame(RSI_14.tail(5)).T, \n",
    "           pd.DataFrame(STOCH14K.tail(5)).T, \n",
    "           pd.DataFrame(STOCH14D.tail(5)).T],axis=1)\n",
    "#     print(RSI_14.tail(5).index)\n",
    "    temp.columns = pred_cols\n",
    "#     print(model.predict(temp)[0][0])\n",
    "#     print(temp)\n",
    "    return(model.predict(temp)[0][0])\n",
    "\n",
    "def realistic_back_test(model, no_of_days_back, ticker_selector, thresh):\n",
    "    # Backtest from \"no_of_days\" back to today\n",
    "    pred_start_date = datetime.date.today() - datetime.timedelta(no_of_days_back)\n",
    "    stock_ticker_list = get_tickers(ticker_selector)\n",
    "    # df for all tickers combined\n",
    "    pred_df = pd.DataFrame([])\n",
    "    for t in stock_ticker_list:\n",
    "        try:\n",
    "            temp_df = data.DataReader(t, 'yahoo', pred_start_date, \n",
    "                                      datetime.date.today())\n",
    "            temp_df['RSI'] = RSI(temp_df['Close'], timeperiod=14)\n",
    "            temp_df['STOCH14K'], temp_df['STOCH14D'] = STOCH(temp_df['High'].to_numpy(), \n",
    "                                       temp_df['Low'].to_numpy(), \n",
    "                                       temp_df['Close'], \n",
    "                                       fastk_period=14, \n",
    "                                       slowk_period=3, \n",
    "                                       slowd_period=3)\n",
    "            # Adding ticker to df for groupby and identification\n",
    "            temp_df['Ticker'] = t\n",
    "            # First 5 will be nan since need atleast 5 days to predict\n",
    "            temp_pred_list = [np.nan] * 5\n",
    "            for i in range(len(temp_df) - 5):\n",
    "                slice_df = pd.concat([temp_df['RSI'].iloc[i : i + 5].T,\n",
    "                          temp_df['STOCH14K'].iloc[i : i + 5].T,\n",
    "                          temp_df['STOCH14D'].iloc[i : i + 5].T], axis = 0)\n",
    "                temp_pred_list.append(model.predict(pd.DataFrame(slice_df).T)[0][0])\n",
    "            temp_df['Prediction Probability'] = temp_pred_list\n",
    "            temp_df['Buy Signal'] = temp_df['Prediction Probability'].apply(lambda x: 1 if x >= thresh else 0)\n",
    "            # Buy and sell price lists will be cut short by 5 on each side\n",
    "            temp_buy = [np.nan] * 5\n",
    "            temp_sell = [np.nan] * 5\n",
    "            for i in range(5, len(temp_df) - 5):\n",
    "                if(temp_df.iloc[i]['Buy Signal'] == 1):\n",
    "                    temp_buy.append(temp_df.iloc[i + 1]['Close'])\n",
    "                    temp_sell.append(temp_df.iloc[i + 5]['Close'])\n",
    "                else:\n",
    "                    temp_buy.append(np.nan)\n",
    "                    temp_sell.append(np.nan)\n",
    "            # Adding the extra 5 nans \n",
    "            for i in range(5):\n",
    "                temp_buy.append(np.nan)\n",
    "                temp_sell.append(np.nan)\n",
    "            temp_df['Buy Price'] = temp_buy\n",
    "            temp_df['Sell Price'] = temp_sell\n",
    "            temp_df['Profit'] = temp_df['Sell Price'] - temp_df['Buy Price']\n",
    "            temp_df['Profit Percentage'] = (temp_df['Profit'] / temp_df['Buy Price']) * 100\n",
    "        except Exception as e:\n",
    "            print(\"Error with \", t)\n",
    "            continue\n",
    "        pred_df = pd.concat([pred_df, temp_df])\n",
    "        print(t)\n",
    "    return(pred_df)\n",
    "\n",
    "def get_tickers(n):\n",
    "    nifty_csv = ['n500.csv', 'n200.csv', 'n50.csv', 'NASDAQ 100 Tickers.csv']\n",
    "    n_csv = pd.read_csv(nifty_csv[n]) # Change index for corresponding list of stocks\n",
    "    nifty_tickers = []\n",
    "    if(n != 3):\n",
    "        for t in n_csv['Symbol'].values:\n",
    "            nifty_tickers.append(t+'.NS')\n",
    "    else:\n",
    "        for t in n_csv['Symbol'].values:\n",
    "            nifty_tickers.append(t)\n",
    "    return (nifty_tickers)\n",
    "# asdf = get_backtest_df(\"TATAMOTORS.BO\", 0.5, 0.5, train_start_date, test_end_date, 5)\n",
    "# get_backtest_stats(asdf)     high_conf_stats['thresh_signal']), \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Large Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ACC.NS\n",
      "2 AUBANK.NS\n",
      "3 AARTIIND.NS\n",
      "4 ABBOTINDIA.NS\n",
      "5 ADANIENT.NS\n",
      "6 ADANIGREEN.NS\n",
      "7 ADANIPORTS.NS\n",
      "8 ATGL.NS\n",
      "9 ADANITRANS.NS\n",
      "10 ABCAPITAL.NS\n",
      "11 ABFRL.NS\n",
      "12 AJANTPHARM.NS\n",
      "13 APLLTD.NS\n",
      "14 ALKEM.NS\n",
      "15 AMARAJABAT.NS\n",
      "16 AMBUJACEM.NS\n",
      "17 APOLLOHOSP.NS\n",
      "18 APOLLOTYRE.NS\n",
      "19 ASHOKLEY.NS\n",
      "20 ASIANPAINT.NS\n",
      "21 AUROPHARMA.NS\n",
      "22 DMART.NS\n",
      "23 AXISBANK.NS\n",
      "24 BAJAJ-AUTO.NS\n",
      "25 BAJFINANCE.NS\n",
      "26 BAJAJFINSV.NS\n",
      "27 BAJAJHLDNG.NS\n",
      "28 BALKRISIND.NS\n",
      "29 BANDHANBNK.NS\n",
      "30 BANKBARODA.NS\n",
      "31 BANKINDIA.NS\n",
      "32 BATAINDIA.NS\n",
      "33 BERGEPAINT.NS\n",
      "34 BEL.NS\n",
      "35 BHARATFORG.NS\n",
      "36 BHEL.NS\n",
      "37 BPCL.NS\n",
      "38 BHARTIARTL.NS\n",
      "39 BIOCON.NS\n",
      "40 BBTC.NS\n",
      "41 BOSCHLTD.NS\n",
      "42 BRITANNIA.NS\n",
      "43 CESC.NS\n",
      "44 CADILAHC.NS\n",
      "CADILAHC.NS No data fetched for symbol CADILAHC.NS using YahooDailyReader\n",
      "44 CANBK.NS\n",
      "45 CASTROLIND.NS\n",
      "46 CHOLAFIN.NS\n",
      "47 CIPLA.NS\n",
      "48 CUB.NS\n",
      "49 COALINDIA.NS\n",
      "50 COFORGE.NS\n",
      "51 COLPAL.NS\n",
      "52 CONCOR.NS\n",
      "53 COROMANDEL.NS\n",
      "54 CROMPTON.NS\n",
      "55 CUMMINSIND.NS\n",
      "56 DLF.NS\n",
      "57 DABUR.NS\n",
      "58 DALBHARAT.NS\n",
      "59 DEEPAKNTR.NS\n",
      "60 DHANI.NS\n",
      "61 DIVISLAB.NS\n",
      "62 DIXON.NS\n",
      "63 LALPATHLAB.NS\n",
      "64 DRREDDY.NS\n",
      "65 EICHERMOT.NS\n",
      "66 EMAMILTD.NS\n",
      "67 ENDURANCE.NS\n",
      "68 ESCORTS.NS\n",
      "69 EXIDEIND.NS\n",
      "70 FEDERALBNK.NS\n",
      "71 FORTIS.NS\n",
      "72 GAIL.NS\n",
      "73 GMRINFRA.NS\n",
      "74 GLENMARK.NS\n",
      "75 GODREJAGRO.NS\n",
      "76 GODREJCP.NS\n",
      "77 GODREJIND.NS\n",
      "78 GODREJPROP.NS\n",
      "79 GRASIM.NS\n",
      "80 GUJGASLTD.NS\n",
      "81 GSPL.NS\n",
      "82 HCLTECH.NS\n",
      "83 HDFCAMC.NS\n",
      "84 HDFCBANK.NS\n",
      "85 HDFCLIFE.NS\n",
      "86 HAVELLS.NS\n",
      "87 HEROMOTOCO.NS\n",
      "88 HINDALCO.NS\n",
      "89 HAL.NS\n",
      "90 HINDPETRO.NS\n",
      "91 HINDUNILVR.NS\n",
      "92 HINDZINC.NS\n",
      "93 HDFC.NS\n",
      "94 ICICIBANK.NS\n",
      "95 ICICIGI.NS\n",
      "96 ICICIPRULI.NS\n",
      "97 ISEC.NS\n",
      "98 IDFCFIRSTB.NS\n",
      "99 ITC.NS\n",
      "100 IBULHSGFIN.NS\n",
      "101 INDIAMART.NS\n",
      "102 INDHOTEL.NS\n",
      "103 IOC.NS\n",
      "104 IRCTC.NS\n",
      "105 IGL.NS\n",
      "106 INDUSTOWER.NS\n",
      "107 INDUSINDBK.NS\n",
      "108 NAUKRI.NS\n",
      "109 INFY.NS\n",
      "110 INDIGO.NS\n",
      "111 IPCALAB.NS\n",
      "112 JSWENERGY.NS\n",
      "113 JSWSTEEL.NS\n",
      "114 JINDALSTEL.NS\n",
      "115 JUBLFOOD.NS\n",
      "116 KOTAKBANK.NS\n",
      "117 L&TFH.NS\n",
      "118 LTTS.NS\n",
      "119 LICHSGFIN.NS\n",
      "120 LTI.NS\n",
      "121 LT.NS\n",
      "122 LAURUSLABS.NS\n",
      "123 LUPIN.NS\n",
      "124 MRF.NS\n",
      "125 MGL.NS\n",
      "126 M&MFIN.NS\n",
      "127 M&M.NS\n",
      "128 MANAPPURAM.NS\n",
      "129 MARICO.NS\n",
      "130 MARUTI.NS\n",
      "131 MFSL.NS\n",
      "132 MINDTREE.NS\n",
      "133 MOTHERSUMI.NS\n",
      "134 MPHASIS.NS\n",
      "135 MUTHOOTFIN.NS\n",
      "136 NATCOPHARM.NS\n",
      "137 NMDC.NS\n",
      "138 NTPC.NS\n",
      "139 NAVINFLUOR.NS\n",
      "140 NESTLEIND.NS\n",
      "141 NAM-INDIA.NS\n",
      "142 OBEROIRLTY.NS\n",
      "143 ONGC.NS\n",
      "144 OIL.NS\n",
      "145 PIIND.NS\n",
      "146 PAGEIND.NS\n",
      "147 PETRONET.NS\n",
      "148 PFIZER.NS\n",
      "149 PIDILITIND.NS\n",
      "150 PEL.NS\n",
      "151 POLYCAB.NS\n",
      "152 PFC.NS\n",
      "153 POWERGRID.NS\n",
      "154 PRESTIGE.NS\n",
      "155 PGHH.NS\n",
      "156 PNB.NS\n",
      "157 RBLBANK.NS\n",
      "158 RECLTD.NS\n",
      "159 RELIANCE.NS\n",
      "160 SBICARD.NS\n",
      "161 SBILIFE.NS\n",
      "162 SRF.NS\n",
      "163 SANOFI.NS\n",
      "164 SHREECEM.NS\n",
      "165 SRTRANSFIN.NS\n",
      "166 SIEMENS.NS\n",
      "167 SBIN.NS\n",
      "168 SAIL.NS\n",
      "169 SUNPHARMA.NS\n",
      "170 SUNTV.NS\n",
      "171 SYNGENE.NS\n",
      "172 TVSMOTOR.NS\n",
      "173 TATACHEM.NS\n",
      "174 TCS.NS\n",
      "175 TATACONSUM.NS\n",
      "176 TATAELXSI.NS\n",
      "177 TATAMOTORS.NS\n",
      "178 TATAPOWER.NS\n",
      "179 TATASTEEL.NS\n",
      "180 TECHM.NS\n",
      "181 RAMCOCEM.NS\n",
      "182 TITAN.NS\n",
      "183 TORNTPHARM.NS\n",
      "184 TORNTPOWER.NS\n",
      "185 TRENT.NS\n",
      "186 UPL.NS\n",
      "187 ULTRACEMCO.NS\n",
      "188 UNIONBANK.NS\n",
      "189 UBL.NS\n",
      "190 MCDOWELL-N.NS\n",
      "191 VGUARD.NS\n",
      "192 VBL.NS\n",
      "193 VEDL.NS\n",
      "194 IDEA.NS\n",
      "195 VOLTAS.NS\n",
      "196 WHIRLPOOL.NS\n",
      "197 WIPRO.NS\n",
      "198 YESBANK.NS\n",
      "199 ZEEL.NS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rsi0</th>\n",
       "      <th>rsi1</th>\n",
       "      <th>rsi2</th>\n",
       "      <th>rsi3</th>\n",
       "      <th>rsi4</th>\n",
       "      <th>stochk0</th>\n",
       "      <th>stochk1</th>\n",
       "      <th>stochk2</th>\n",
       "      <th>stochk3</th>\n",
       "      <th>stochk4</th>\n",
       "      <th>...</th>\n",
       "      <th>stochd1</th>\n",
       "      <th>stochd2</th>\n",
       "      <th>stochd3</th>\n",
       "      <th>stochd4</th>\n",
       "      <th>close0</th>\n",
       "      <th>close1</th>\n",
       "      <th>close2</th>\n",
       "      <th>close3</th>\n",
       "      <th>close4</th>\n",
       "      <th>long_or_short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.421932</td>\n",
       "      <td>16.771613</td>\n",
       "      <td>18.201264</td>\n",
       "      <td>17.214107</td>\n",
       "      <td>16.351469</td>\n",
       "      <td>5.558989</td>\n",
       "      <td>6.092141</td>\n",
       "      <td>6.940934</td>\n",
       "      <td>8.339270</td>\n",
       "      <td>8.745083</td>\n",
       "      <td>...</td>\n",
       "      <td>6.289554</td>\n",
       "      <td>6.197355</td>\n",
       "      <td>7.124115</td>\n",
       "      <td>8.008429</td>\n",
       "      <td>137.550003</td>\n",
       "      <td>136.399994</td>\n",
       "      <td>136.899994</td>\n",
       "      <td>135.350006</td>\n",
       "      <td>133.949997</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.771613</td>\n",
       "      <td>18.201264</td>\n",
       "      <td>17.214107</td>\n",
       "      <td>16.351469</td>\n",
       "      <td>15.628539</td>\n",
       "      <td>6.092141</td>\n",
       "      <td>6.940934</td>\n",
       "      <td>8.339270</td>\n",
       "      <td>8.745083</td>\n",
       "      <td>9.117975</td>\n",
       "      <td>...</td>\n",
       "      <td>6.197355</td>\n",
       "      <td>7.124115</td>\n",
       "      <td>8.008429</td>\n",
       "      <td>8.734109</td>\n",
       "      <td>136.399994</td>\n",
       "      <td>136.899994</td>\n",
       "      <td>135.350006</td>\n",
       "      <td>133.949997</td>\n",
       "      <td>132.750000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.201264</td>\n",
       "      <td>17.214107</td>\n",
       "      <td>16.351469</td>\n",
       "      <td>15.628539</td>\n",
       "      <td>14.532944</td>\n",
       "      <td>6.940934</td>\n",
       "      <td>8.339270</td>\n",
       "      <td>8.745083</td>\n",
       "      <td>9.117975</td>\n",
       "      <td>7.417553</td>\n",
       "      <td>...</td>\n",
       "      <td>7.124115</td>\n",
       "      <td>8.008429</td>\n",
       "      <td>8.734109</td>\n",
       "      <td>8.426870</td>\n",
       "      <td>136.899994</td>\n",
       "      <td>135.350006</td>\n",
       "      <td>133.949997</td>\n",
       "      <td>132.750000</td>\n",
       "      <td>130.850006</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.214107</td>\n",
       "      <td>16.351469</td>\n",
       "      <td>15.628539</td>\n",
       "      <td>14.532944</td>\n",
       "      <td>27.861515</td>\n",
       "      <td>8.339270</td>\n",
       "      <td>8.745083</td>\n",
       "      <td>9.117975</td>\n",
       "      <td>7.417553</td>\n",
       "      <td>13.342194</td>\n",
       "      <td>...</td>\n",
       "      <td>8.008429</td>\n",
       "      <td>8.734109</td>\n",
       "      <td>8.426870</td>\n",
       "      <td>9.959241</td>\n",
       "      <td>135.350006</td>\n",
       "      <td>133.949997</td>\n",
       "      <td>132.750000</td>\n",
       "      <td>130.850006</td>\n",
       "      <td>135.500000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.351469</td>\n",
       "      <td>15.628539</td>\n",
       "      <td>14.532944</td>\n",
       "      <td>27.861515</td>\n",
       "      <td>35.543891</td>\n",
       "      <td>8.745083</td>\n",
       "      <td>9.117975</td>\n",
       "      <td>7.417553</td>\n",
       "      <td>13.342194</td>\n",
       "      <td>23.906231</td>\n",
       "      <td>...</td>\n",
       "      <td>8.734109</td>\n",
       "      <td>8.426870</td>\n",
       "      <td>9.959241</td>\n",
       "      <td>14.888660</td>\n",
       "      <td>133.949997</td>\n",
       "      <td>132.750000</td>\n",
       "      <td>130.850006</td>\n",
       "      <td>135.500000</td>\n",
       "      <td>138.800003</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4907</th>\n",
       "      <td>38.164341</td>\n",
       "      <td>39.586595</td>\n",
       "      <td>35.905573</td>\n",
       "      <td>41.597000</td>\n",
       "      <td>36.077038</td>\n",
       "      <td>5.682236</td>\n",
       "      <td>9.267877</td>\n",
       "      <td>7.202170</td>\n",
       "      <td>14.493482</td>\n",
       "      <td>9.880550</td>\n",
       "      <td>...</td>\n",
       "      <td>8.489467</td>\n",
       "      <td>7.384095</td>\n",
       "      <td>10.321177</td>\n",
       "      <td>10.525401</td>\n",
       "      <td>253.800003</td>\n",
       "      <td>255.600006</td>\n",
       "      <td>248.149994</td>\n",
       "      <td>255.399994</td>\n",
       "      <td>243.800003</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4908</th>\n",
       "      <td>39.586595</td>\n",
       "      <td>35.905573</td>\n",
       "      <td>41.597000</td>\n",
       "      <td>36.077038</td>\n",
       "      <td>36.507258</td>\n",
       "      <td>9.267877</td>\n",
       "      <td>7.202170</td>\n",
       "      <td>14.493482</td>\n",
       "      <td>9.880550</td>\n",
       "      <td>10.581206</td>\n",
       "      <td>...</td>\n",
       "      <td>7.384095</td>\n",
       "      <td>10.321177</td>\n",
       "      <td>10.525401</td>\n",
       "      <td>11.651746</td>\n",
       "      <td>255.600006</td>\n",
       "      <td>248.149994</td>\n",
       "      <td>255.399994</td>\n",
       "      <td>243.800003</td>\n",
       "      <td>244.350006</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4909</th>\n",
       "      <td>35.905573</td>\n",
       "      <td>41.597000</td>\n",
       "      <td>36.077038</td>\n",
       "      <td>36.507258</td>\n",
       "      <td>39.610926</td>\n",
       "      <td>7.202170</td>\n",
       "      <td>14.493482</td>\n",
       "      <td>9.880550</td>\n",
       "      <td>10.581206</td>\n",
       "      <td>10.528262</td>\n",
       "      <td>...</td>\n",
       "      <td>10.321177</td>\n",
       "      <td>10.525401</td>\n",
       "      <td>11.651746</td>\n",
       "      <td>10.330006</td>\n",
       "      <td>248.149994</td>\n",
       "      <td>255.399994</td>\n",
       "      <td>243.800003</td>\n",
       "      <td>244.350006</td>\n",
       "      <td>248.250000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4910</th>\n",
       "      <td>41.597000</td>\n",
       "      <td>36.077038</td>\n",
       "      <td>36.507258</td>\n",
       "      <td>39.610926</td>\n",
       "      <td>33.543731</td>\n",
       "      <td>14.493482</td>\n",
       "      <td>9.880550</td>\n",
       "      <td>10.581206</td>\n",
       "      <td>10.528262</td>\n",
       "      <td>11.560986</td>\n",
       "      <td>...</td>\n",
       "      <td>10.525401</td>\n",
       "      <td>11.651746</td>\n",
       "      <td>10.330006</td>\n",
       "      <td>10.890152</td>\n",
       "      <td>255.399994</td>\n",
       "      <td>243.800003</td>\n",
       "      <td>244.350006</td>\n",
       "      <td>248.250000</td>\n",
       "      <td>234.850006</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4911</th>\n",
       "      <td>36.077038</td>\n",
       "      <td>36.507258</td>\n",
       "      <td>39.610926</td>\n",
       "      <td>33.543731</td>\n",
       "      <td>32.659258</td>\n",
       "      <td>9.880550</td>\n",
       "      <td>10.581206</td>\n",
       "      <td>10.528262</td>\n",
       "      <td>11.560986</td>\n",
       "      <td>11.012659</td>\n",
       "      <td>...</td>\n",
       "      <td>11.651746</td>\n",
       "      <td>10.330006</td>\n",
       "      <td>10.890152</td>\n",
       "      <td>11.033969</td>\n",
       "      <td>243.800003</td>\n",
       "      <td>244.350006</td>\n",
       "      <td>248.250000</td>\n",
       "      <td>234.850006</td>\n",
       "      <td>232.649994</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>843206 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           rsi0       rsi1       rsi2       rsi3       rsi4    stochk0  \\\n",
       "0     17.421932  16.771613  18.201264  17.214107  16.351469   5.558989   \n",
       "1     16.771613  18.201264  17.214107  16.351469  15.628539   6.092141   \n",
       "2     18.201264  17.214107  16.351469  15.628539  14.532944   6.940934   \n",
       "3     17.214107  16.351469  15.628539  14.532944  27.861515   8.339270   \n",
       "4     16.351469  15.628539  14.532944  27.861515  35.543891   8.745083   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "4907  38.164341  39.586595  35.905573  41.597000  36.077038   5.682236   \n",
       "4908  39.586595  35.905573  41.597000  36.077038  36.507258   9.267877   \n",
       "4909  35.905573  41.597000  36.077038  36.507258  39.610926   7.202170   \n",
       "4910  41.597000  36.077038  36.507258  39.610926  33.543731  14.493482   \n",
       "4911  36.077038  36.507258  39.610926  33.543731  32.659258   9.880550   \n",
       "\n",
       "        stochk1    stochk2    stochk3    stochk4  ...    stochd1    stochd2  \\\n",
       "0      6.092141   6.940934   8.339270   8.745083  ...   6.289554   6.197355   \n",
       "1      6.940934   8.339270   8.745083   9.117975  ...   6.197355   7.124115   \n",
       "2      8.339270   8.745083   9.117975   7.417553  ...   7.124115   8.008429   \n",
       "3      8.745083   9.117975   7.417553  13.342194  ...   8.008429   8.734109   \n",
       "4      9.117975   7.417553  13.342194  23.906231  ...   8.734109   8.426870   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "4907   9.267877   7.202170  14.493482   9.880550  ...   8.489467   7.384095   \n",
       "4908   7.202170  14.493482   9.880550  10.581206  ...   7.384095  10.321177   \n",
       "4909  14.493482   9.880550  10.581206  10.528262  ...  10.321177  10.525401   \n",
       "4910   9.880550  10.581206  10.528262  11.560986  ...  10.525401  11.651746   \n",
       "4911  10.581206  10.528262  11.560986  11.012659  ...  11.651746  10.330006   \n",
       "\n",
       "        stochd3    stochd4      close0      close1      close2      close3  \\\n",
       "0      7.124115   8.008429  137.550003  136.399994  136.899994  135.350006   \n",
       "1      8.008429   8.734109  136.399994  136.899994  135.350006  133.949997   \n",
       "2      8.734109   8.426870  136.899994  135.350006  133.949997  132.750000   \n",
       "3      8.426870   9.959241  135.350006  133.949997  132.750000  130.850006   \n",
       "4      9.959241  14.888660  133.949997  132.750000  130.850006  135.500000   \n",
       "...         ...        ...         ...         ...         ...         ...   \n",
       "4907  10.321177  10.525401  253.800003  255.600006  248.149994  255.399994   \n",
       "4908  10.525401  11.651746  255.600006  248.149994  255.399994  243.800003   \n",
       "4909  11.651746  10.330006  248.149994  255.399994  243.800003  244.350006   \n",
       "4910  10.330006  10.890152  255.399994  243.800003  244.350006  248.250000   \n",
       "4911  10.890152  11.033969  243.800003  244.350006  248.250000  234.850006   \n",
       "\n",
       "          close4  long_or_short  \n",
       "0     133.949997            1.0  \n",
       "1     132.750000            1.0  \n",
       "2     130.850006            0.0  \n",
       "3     135.500000            0.0  \n",
       "4     138.800003            0.0  \n",
       "...          ...            ...  \n",
       "4907  243.800003            NaN  \n",
       "4908  244.350006            NaN  \n",
       "4909  248.250000            NaN  \n",
       "4910  234.850006            NaN  \n",
       "4911  232.649994            NaN  \n",
       "\n",
       "[843206 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates required dataset from choosen input which denotes which set of \n",
    "# companies to use. \n",
    "\n",
    "nifty_tickers = get_tickers(1)\n",
    "nifty_stats_dict = {}    \n",
    "i = 1\n",
    "nifty200_train_data = pd.DataFrame()\n",
    "for t in nifty_tickers:\n",
    "    try:\n",
    "        print(i, t)\n",
    "        temp_df = data.DataReader(t, 'yahoo', train_start_date, test_end_date)\n",
    "        temp_single_df = data_cleaning_for_all_indicators_abhi(temp_df, 5)\n",
    "        nifty200_train_data = pd.concat([nifty200_train_data, temp_single_df])\n",
    "        i += 1\n",
    "    except Exception as e:\n",
    "        print(t, e)\n",
    "\n",
    "nifty200_train_data.to_csv(\"nifty200_train_data_with_incorrected_target_with_0.csv\")\n",
    "nifty200_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134795, 15) (33699, 15) (134795,) (33699,)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 600)               9600      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 500)               300500    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 15)                465       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 547,761\n",
      "Trainable params: 547,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.6166 - accuracy: 0.6630 - val_loss: 0.5775 - val_accuracy: 0.6973\n",
      "Epoch 2/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5831 - accuracy: 0.6889 - val_loss: 0.5713 - val_accuracy: 0.7001\n",
      "Epoch 3/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5788 - accuracy: 0.6924 - val_loss: 0.5790 - val_accuracy: 0.6891\n",
      "Epoch 4/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5779 - accuracy: 0.6920 - val_loss: 0.5902 - val_accuracy: 0.6867\n",
      "Epoch 5/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5762 - accuracy: 0.6957 - val_loss: 0.5685 - val_accuracy: 0.7038\n",
      "Epoch 6/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5768 - accuracy: 0.6953 - val_loss: 0.5709 - val_accuracy: 0.6973\n",
      "Epoch 7/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5757 - accuracy: 0.6942 - val_loss: 0.5913 - val_accuracy: 0.6801\n",
      "Epoch 8/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5765 - accuracy: 0.6931 - val_loss: 0.5842 - val_accuracy: 0.6825\n",
      "Epoch 9/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5730 - accuracy: 0.6955 - val_loss: 0.5701 - val_accuracy: 0.6977\n",
      "Epoch 10/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5745 - accuracy: 0.6948 - val_loss: 0.5677 - val_accuracy: 0.7036\n",
      "Epoch 11/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5748 - accuracy: 0.6977 - val_loss: 0.5743 - val_accuracy: 0.7020\n",
      "Epoch 12/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5744 - accuracy: 0.6951 - val_loss: 0.5717 - val_accuracy: 0.6995\n",
      "Epoch 13/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5729 - accuracy: 0.6972 - val_loss: 0.5676 - val_accuracy: 0.7034\n",
      "Epoch 14/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5726 - accuracy: 0.6966 - val_loss: 0.5683 - val_accuracy: 0.7051\n",
      "Epoch 15/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5727 - accuracy: 0.6964 - val_loss: 0.5691 - val_accuracy: 0.7012\n",
      "Epoch 16/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5723 - accuracy: 0.6972 - val_loss: 0.5696 - val_accuracy: 0.7020\n",
      "Epoch 17/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5745 - accuracy: 0.6937 - val_loss: 0.5706 - val_accuracy: 0.6995\n",
      "Epoch 18/150\n",
      "3900/3900 [==============================] - 23s 6ms/step - loss: 0.5722 - accuracy: 0.6973 - val_loss: 0.5738 - val_accuracy: 0.6953\n",
      "Epoch 19/150\n",
      "3900/3900 [==============================] - 25s 6ms/step - loss: 0.5748 - accuracy: 0.6927 - val_loss: 0.5711 - val_accuracy: 0.6977\n",
      "Epoch 20/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5722 - accuracy: 0.6983 - val_loss: 0.5680 - val_accuracy: 0.7028\n",
      "Epoch 21/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5735 - accuracy: 0.6950 - val_loss: 0.5679 - val_accuracy: 0.7020\n",
      "Epoch 22/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5726 - accuracy: 0.6963 - val_loss: 0.5770 - val_accuracy: 0.6938\n",
      "Epoch 23/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5728 - accuracy: 0.6978 - val_loss: 0.5703 - val_accuracy: 0.6998\n",
      "Epoch 24/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5726 - accuracy: 0.6979 - val_loss: 0.5673 - val_accuracy: 0.7033\n",
      "Epoch 25/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5709 - accuracy: 0.6984 - val_loss: 0.5672 - val_accuracy: 0.7060\n",
      "Epoch 26/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5747 - accuracy: 0.6955 - val_loss: 0.5667 - val_accuracy: 0.7030\n",
      "Epoch 27/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5724 - accuracy: 0.6970 - val_loss: 0.5676 - val_accuracy: 0.7036\n",
      "Epoch 28/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5737 - accuracy: 0.6968 - val_loss: 0.5664 - val_accuracy: 0.7029\n",
      "Epoch 29/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5722 - accuracy: 0.6979 - val_loss: 0.5672 - val_accuracy: 0.7019\n",
      "Epoch 30/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5735 - accuracy: 0.6957 - val_loss: 0.5735 - val_accuracy: 0.6965\n",
      "Epoch 31/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5721 - accuracy: 0.6948 - val_loss: 0.5671 - val_accuracy: 0.7043\n",
      "Epoch 32/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5731 - accuracy: 0.6964 - val_loss: 0.5670 - val_accuracy: 0.7048\n",
      "Epoch 33/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5703 - accuracy: 0.6979 - val_loss: 0.5687 - val_accuracy: 0.7002\n",
      "Epoch 34/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5736 - accuracy: 0.6947 - val_loss: 0.5737 - val_accuracy: 0.6967\n",
      "Epoch 35/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5728 - accuracy: 0.6968 - val_loss: 0.5739 - val_accuracy: 0.6920\n",
      "Epoch 36/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5696 - accuracy: 0.6997 - val_loss: 0.5673 - val_accuracy: 0.7052\n",
      "Epoch 37/150\n",
      "3900/3900 [==============================] - 25s 6ms/step - loss: 0.5721 - accuracy: 0.6979 - val_loss: 0.5663 - val_accuracy: 0.7040\n",
      "Epoch 38/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5711 - accuracy: 0.6981 - val_loss: 0.5670 - val_accuracy: 0.7002\n",
      "Epoch 39/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5732 - accuracy: 0.6983 - val_loss: 0.5687 - val_accuracy: 0.7041\n",
      "Epoch 40/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5710 - accuracy: 0.6978 - val_loss: 0.5775 - val_accuracy: 0.6872\n",
      "Epoch 41/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5727 - accuracy: 0.6970 - val_loss: 0.5683 - val_accuracy: 0.7020\n",
      "Epoch 42/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5722 - accuracy: 0.6966 - val_loss: 0.5677 - val_accuracy: 0.7047\n",
      "Epoch 43/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5716 - accuracy: 0.6974 - val_loss: 0.5701 - val_accuracy: 0.6998\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 44/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5671 - accuracy: 0.6984 - val_loss: 0.5665 - val_accuracy: 0.7010\n",
      "Epoch 45/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5686 - accuracy: 0.7005 - val_loss: 0.5658 - val_accuracy: 0.7057\n",
      "Epoch 46/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5657 - accuracy: 0.7024 - val_loss: 0.5661 - val_accuracy: 0.7051\n",
      "Epoch 47/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5688 - accuracy: 0.6989 - val_loss: 0.5656 - val_accuracy: 0.7038\n",
      "Epoch 48/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5685 - accuracy: 0.6993 - val_loss: 0.5658 - val_accuracy: 0.7051\n",
      "Epoch 49/150\n",
      "3900/3900 [==============================] - 23s 6ms/step - loss: 0.5687 - accuracy: 0.6995 - val_loss: 0.5657 - val_accuracy: 0.7055\n",
      "Epoch 50/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5665 - accuracy: 0.6992 - val_loss: 0.5657 - val_accuracy: 0.7054\n",
      "Epoch 51/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5671 - accuracy: 0.7010 - val_loss: 0.5658 - val_accuracy: 0.7027\n",
      "Epoch 52/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5681 - accuracy: 0.6987 - val_loss: 0.5666 - val_accuracy: 0.7033\n",
      "Epoch 53/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5675 - accuracy: 0.7003 - val_loss: 0.5656 - val_accuracy: 0.7048\n",
      "Epoch 54/150\n",
      "3900/3900 [==============================] - 23s 6ms/step - loss: 0.5687 - accuracy: 0.6981 - val_loss: 0.5660 - val_accuracy: 0.7030\n",
      "Epoch 55/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5682 - accuracy: 0.6990 - val_loss: 0.5657 - val_accuracy: 0.7053\n",
      "Epoch 56/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5678 - accuracy: 0.6991 - val_loss: 0.5654 - val_accuracy: 0.7040\n",
      "Epoch 57/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5663 - accuracy: 0.7011 - val_loss: 0.5656 - val_accuracy: 0.7058\n",
      "Epoch 58/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5676 - accuracy: 0.7003 - val_loss: 0.5664 - val_accuracy: 0.7018\n",
      "Epoch 59/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5692 - accuracy: 0.6982 - val_loss: 0.5657 - val_accuracy: 0.7030\n",
      "Epoch 60/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5690 - accuracy: 0.6977 - val_loss: 0.5658 - val_accuracy: 0.7032\n",
      "Epoch 61/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5678 - accuracy: 0.6986 - val_loss: 0.5678 - val_accuracy: 0.7025\n",
      "Epoch 62/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5677 - accuracy: 0.7000 - val_loss: 0.5665 - val_accuracy: 0.7031\n",
      "Epoch 63/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5673 - accuracy: 0.6999 - val_loss: 0.5655 - val_accuracy: 0.7024\n",
      "Epoch 64/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5668 - accuracy: 0.6999 - val_loss: 0.5661 - val_accuracy: 0.7054\n",
      "Epoch 65/150\n",
      "3900/3900 [==============================] - 23s 6ms/step - loss: 0.5685 - accuracy: 0.6981 - val_loss: 0.5658 - val_accuracy: 0.7052\n",
      "Epoch 66/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5706 - accuracy: 0.6972 - val_loss: 0.5662 - val_accuracy: 0.7050\n",
      "Epoch 67/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5670 - accuracy: 0.7000 - val_loss: 0.5665 - val_accuracy: 0.7046\n",
      "Epoch 68/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5676 - accuracy: 0.7001 - val_loss: 0.5662 - val_accuracy: 0.7045\n",
      "Epoch 69/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5668 - accuracy: 0.7014 - val_loss: 0.5713 - val_accuracy: 0.6995\n",
      "Epoch 70/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5671 - accuracy: 0.7005 - val_loss: 0.5659 - val_accuracy: 0.7021\n",
      "Epoch 71/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5689 - accuracy: 0.6984 - val_loss: 0.5661 - val_accuracy: 0.7063\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "Epoch 72/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5666 - accuracy: 0.7006 - val_loss: 0.5660 - val_accuracy: 0.7054\n",
      "Epoch 73/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5680 - accuracy: 0.6999 - val_loss: 0.5658 - val_accuracy: 0.7049\n",
      "Epoch 74/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5681 - accuracy: 0.7006 - val_loss: 0.5657 - val_accuracy: 0.7036\n",
      "Epoch 75/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5691 - accuracy: 0.6981 - val_loss: 0.5667 - val_accuracy: 0.7026\n",
      "Epoch 76/150\n",
      "3900/3900 [==============================] - 24s 6ms/step - loss: 0.5664 - accuracy: 0.7006 - val_loss: 0.5658 - val_accuracy: 0.7066\n",
      "Epoch 00076: early stopping\n"
     ]
    }
   ],
   "source": [
    "# FB = data.DataReader(\"FB\", 'yahoo', train_start_date, test_end_date)\n",
    "# AAPL = data.DataReader(\"AAPL\", 'yahoo', train_start_date, test_end_date)\n",
    "# AMZN = data.DataReader(\"AMZN\", 'yahoo', train_start_date, test_end_date)\n",
    "# NFLX = data.DataReader(\"NFLX\", 'yahoo', train_start_date, test_end_date)\n",
    "# GOOG = data.DataReader(\"GOOG\", 'yahoo', train_start_date, test_end_date)\n",
    "# MSFT = data.DataReader(\"MSFT\", 'yahoo', train_start_date, test_end_date)\n",
    "# QQQ = data.DataReader(\"QQQ\", 'yahoo', train_start_date, test_end_date)\n",
    "# XLK = data.DataReader(\"XLK\", 'yahoo', train_start_date, test_end_date)\n",
    "\n",
    "# ticker_symbols = [FB,AAPL,AMZN,NFLX,GOOG,MSFT,QQQ,XLK]\n",
    "# df_all_tickers_ind = pd.DataFrame()\n",
    "# for ticker in ticker_symbols:\n",
    "#     single_ticker_df = data_cleaning_for_all_indicators(ticker, 5)\n",
    "#     df_all_tickers_ind = pd.concat([df_all_tickers_ind,single_ticker_df])\n",
    "# df_all_tickers_ind.dropna(inplace = True)\n",
    "    \n",
    "# X, y = df_all_tickers_ind.values[:,:-1],df_all_tickers_ind.values[:,-1]\n",
    "nifty200_train_data = pd.read_csv(\"nifty200_train_data.csv\", index_col = 0)\n",
    "# nifty200_train_data.drop(columns = ['close0', \n",
    "#                                    'close1',\n",
    "#                                    'close2',\n",
    "#                                    'close3',\n",
    "#                                    'close4',], inplace = True)\n",
    "nifty200_train_data.dropna(inplace = True)\n",
    "# nifty200_train_data = nifty200_train_data.sample(frac = 0.05)\n",
    "X, y = nifty200_train_data.values[:,:-1],nifty200_train_data.values[:,-1]\n",
    "\n",
    "# sample_size = X.shape[0]          # number of samples in train set\n",
    "# time_steps  = X.shape[1]          # number of features in train set\n",
    "# input_dimension = 1               # each feature is represented by 1 number\n",
    "# X = X.reshape(sample_size,time_steps,input_dimension)\n",
    "\n",
    "\n",
    "X = X.astype('float32')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Input(shape=(X_train.shape[1],X_train.shape[2])))\n",
    "model.add(Dense(600, activation='relu', input_shape=(n_features,)))\n",
    "# model.add(Conv1D(64, 2, activation='relu'))\n",
    "# model.add(Dropout(0.15))\n",
    "# model.add(Conv1D(16, 2, activation='relu'))\n",
    "# model.add(MaxPool1D(pool_size=2))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=20)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=15, min_lr=0.0001, verbose = 1)\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=1, validation_data=(X_val,y_val),callbacks=[early_stop, reduce_lr])\n",
    "model.save(\"ALL_model_with_incorrected_target.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054/1054 [==============================] - 3s 3ms/step - loss: 0.5693 - accuracy: 0.7012\n",
      "\n",
      "\n",
      "Test Accuracy: 0.701\n",
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.67      0.67     15458\n",
      "         1.0       0.72      0.73      0.73     18241\n",
      "\n",
      "    accuracy                           0.70     33699\n",
      "   macro avg       0.70      0.70      0.70     33699\n",
      "weighted avg       0.70      0.70      0.70     33699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('\\n\\nTest Accuracy: %.3f' % acc)\n",
    "\n",
    "y_pred = np.where(model.predict(X_test) > threshold, 1,0)\n",
    "print(\"\\n\\n\", classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FB = data.DataReader(\"FB\", 'yahoo', train_start_date, test_end_date)\n",
    "# AAPL = data.DataReader(\"AAPL\", 'yahoo', train_start_date, test_end_date)\n",
    "# AMZN = data.DataReader(\"AMZN\", 'yahoo', train_start_date, test_end_date)\n",
    "# NFLX = data.DataReader(\"NFLX\", 'yahoo', train_start_date, test_end_date)\n",
    "# GOOG = data.DataReader(\"GOOG\", 'yahoo', train_start_date, test_end_date)\n",
    "# MSFT = data.DataReader(\"MSFT\", 'yahoo', train_start_date, test_end_date)\n",
    "# QQQ = data.DataReader(\"QQQ\", 'yahoo', train_start_date, test_end_date)\n",
    "# XLK = data.DataReader(\"XLK\", 'yahoo', train_start_date, test_end_date)\n",
    "\n",
    "# ticker_symbols = [FB,AAPL,AMZN,NFLX,GOOG,MSFT,QQQ,XLK]\n",
    "# df_all_tickers_ind = pd.DataFrame()\n",
    "# for ticker in ticker_symbols:\n",
    "#     single_ticker_df = data_cleaning_for_all_indicators(ticker, 5)\n",
    "#     df_all_tickers_ind = pd.concat([df_all_tickers_ind,single_ticker_df])\n",
    "# df_all_tickers_ind.dropna(inplace = True)\n",
    "    \n",
    "# X, y = df_all_tickers_ind.values[:,:-1],df_all_tickers_ind.values[:,-1]\n",
    "nifty200_train_data = pd.read_csv(\"nifty200_train_data_with_corrected_target.csv\", index_col = 0)\n",
    "nifty200_train_data.drop(columns = ['close0', \n",
    "                                   'close1',\n",
    "                                   'close2',\n",
    "                                   'close3',\n",
    "                                   'close4',], inplace = True)\n",
    "nifty200_train_data.dropna(inplace = True)\n",
    "# nifty200_train_data = nifty200_train_data.sample(frac = 0.05)\n",
    "X, y = nifty200_train_data.values[:,:-1],nifty200_train_data.values[:,-1]\n",
    "\n",
    "sample_size = X.shape[0]          # number of samples in train set\n",
    "time_steps  = X.shape[1]          # number of features in train set\n",
    "input_dimension = 1               # each feature is represented by 1 number\n",
    "X = X.reshape(sample_size,time_steps,input_dimension)\n",
    "\n",
    "\n",
    "X = X.astype('float32')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train.shape[1],X_train.shape[2])))\n",
    "model.add(Conv1D(64, 2, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Conv1D(16, 2, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=20)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=15, min_lr=0.0001, verbose = 1)\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=1, validation_data=(X_val,y_val),callbacks=[early_stop, reduce_lr])\n",
    "model.save(\"ALL_model_with_corrected_target.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuned Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134795, 15) (33699, 15) (134795,) (33699,)\n",
      "|   iter    |  target   | activa... | batch_... |  dropout  | dropou... |  epochs   |  layers1  |  layers2  | learni... |  neurons  | normal... | optimizer |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.7158 - accuracy: 0.5574\n",
      "Epoch 2/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6811 - accuracy: 0.5461\n",
      "Epoch 3/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6859 - accuracy: 0.5355\n",
      "Epoch 4/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6806 - accuracy: 0.5490\n",
      "Epoch 5/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6820 - accuracy: 0.5472\n",
      "Epoch 6/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6863 - accuracy: 0.5425\n",
      "Epoch 7/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6873 - accuracy: 0.5330\n",
      "Epoch 8/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6754 - accuracy: 0.5535\n",
      "Epoch 9/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6861 - accuracy: 0.5363\n",
      "Epoch 10/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6747 - accuracy: 0.5540\n",
      "Epoch 11/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6852 - accuracy: 0.5441\n",
      "Epoch 12/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6789 - accuracy: 0.5522\n",
      "Epoch 13/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6827 - accuracy: 0.5549\n",
      "Epoch 14/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6857 - accuracy: 0.5435\n",
      "Epoch 15/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6877 - accuracy: 0.5358\n",
      "Epoch 16/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6876 - accuracy: 0.5387\n",
      "Epoch 17/49\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 0.6873 - accuracy: 0.5366\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.17326199392583208.\n",
      "Epoch 18/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6873 - accuracy: 0.5398\n",
      "Epoch 19/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6863 - accuracy: 0.5430\n",
      "Epoch 20/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6860 - accuracy: 0.5467\n",
      "Epoch 00020: early stopping\n",
      "  1/403 [..............................] - ETA: 50s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403/403 [==============================] - 1s 1ms/step\n",
      "Epoch 1/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6649 - accuracy: 0.5987\n",
      "Epoch 2/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6597 - accuracy: 0.5925\n",
      "Epoch 3/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6568 - accuracy: 0.6006\n",
      "Epoch 4/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6494 - accuracy: 0.6083\n",
      "Epoch 5/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6371 - accuracy: 0.6356\n",
      "Epoch 6/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6473 - accuracy: 0.6114\n",
      "Epoch 7/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6390 - accuracy: 0.6279\n",
      "Epoch 8/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6389 - accuracy: 0.6304\n",
      "Epoch 9/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6374 - accuracy: 0.6404\n",
      "Epoch 10/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6301 - accuracy: 0.6430\n",
      "Epoch 11/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6253 - accuracy: 0.6558\n",
      "Epoch 12/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6342 - accuracy: 0.6285\n",
      "Epoch 13/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6246 - accuracy: 0.6470\n",
      "Epoch 14/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6206 - accuracy: 0.6556\n",
      "Epoch 15/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6167 - accuracy: 0.6632\n",
      "Epoch 16/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6228 - accuracy: 0.6573\n",
      "Epoch 17/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6117 - accuracy: 0.6735\n",
      "Epoch 18/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6160 - accuracy: 0.6627\n",
      "Epoch 19/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6152 - accuracy: 0.6643\n",
      "Epoch 20/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6177 - accuracy: 0.6605\n",
      "Epoch 21/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6143 - accuracy: 0.6579\n",
      "Epoch 22/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6127 - accuracy: 0.6630\n",
      "Epoch 23/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6119 - accuracy: 0.6637\n",
      "Epoch 24/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6121 - accuracy: 0.6661\n",
      "Epoch 25/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6214 - accuracy: 0.6515\n",
      "Epoch 26/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6202 - accuracy: 0.6568\n",
      "Epoch 27/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6181 - accuracy: 0.6550\n",
      "Epoch 28/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6115 - accuracy: 0.6706\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.03465239878516642.\n",
      "Epoch 29/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6071 - accuracy: 0.6770\n",
      "Epoch 30/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6009 - accuracy: 0.6827\n",
      "Epoch 31/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5997 - accuracy: 0.6729\n",
      "Epoch 32/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5987 - accuracy: 0.6780\n",
      "Epoch 33/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5958 - accuracy: 0.6833\n",
      "Epoch 34/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5938 - accuracy: 0.6813\n",
      "Epoch 35/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5947 - accuracy: 0.6856\n",
      "Epoch 36/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5964 - accuracy: 0.6794\n",
      "Epoch 37/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5956 - accuracy: 0.6866\n",
      "Epoch 38/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5965 - accuracy: 0.6759\n",
      "Epoch 39/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5956 - accuracy: 0.6772\n",
      "Epoch 40/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5989 - accuracy: 0.6821\n",
      "Epoch 41/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5980 - accuracy: 0.6794\n",
      "Epoch 42/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5958 - accuracy: 0.6861\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.006930479757033284.\n",
      "Epoch 43/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5922 - accuracy: 0.6819\n",
      "Epoch 44/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5898 - accuracy: 0.6822\n",
      "Epoch 45/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5898 - accuracy: 0.6869\n",
      "Epoch 46/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5895 - accuracy: 0.6886\n",
      "Epoch 47/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5898 - accuracy: 0.6869\n",
      "Epoch 48/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5900 - accuracy: 0.6864\n",
      "Epoch 49/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5920 - accuracy: 0.6873\n",
      " 82/403 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403/403 [==============================] - 0s 1ms/step\n",
      "Epoch 1/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6810 - accuracy: 0.5613\n",
      "Epoch 2/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6300 - accuracy: 0.6648\n",
      "Epoch 3/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5976 - accuracy: 0.6845\n",
      "Epoch 4/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5893 - accuracy: 0.6872\n",
      "Epoch 5/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5883 - accuracy: 0.6853\n",
      "Epoch 6/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5880 - accuracy: 0.6840\n",
      "Epoch 7/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5887 - accuracy: 0.6880\n",
      "Epoch 8/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5859 - accuracy: 0.6867\n",
      "Epoch 9/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5830 - accuracy: 0.6921\n",
      "Epoch 10/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5857 - accuracy: 0.6866\n",
      "Epoch 11/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5819 - accuracy: 0.6917\n",
      "Epoch 12/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5854 - accuracy: 0.6877\n",
      "Epoch 13/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5821 - accuracy: 0.6903\n",
      "Epoch 14/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5850 - accuracy: 0.6860\n",
      "Epoch 15/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5833 - accuracy: 0.6894\n",
      "Epoch 16/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5847 - accuracy: 0.6915\n",
      "Epoch 17/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5827 - accuracy: 0.6878\n",
      "Epoch 18/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5831 - accuracy: 0.6899\n",
      "Epoch 19/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5831 - accuracy: 0.6883\n",
      "Epoch 20/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5838 - accuracy: 0.6904\n",
      "Epoch 21/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5819 - accuracy: 0.6905\n",
      "Epoch 22/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5811 - accuracy: 0.6914\n",
      "Epoch 23/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5817 - accuracy: 0.6901\n",
      "Epoch 24/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5810 - accuracy: 0.6933\n",
      "Epoch 25/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5812 - accuracy: 0.6912\n",
      "Epoch 26/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5843 - accuracy: 0.6872\n",
      "Epoch 27/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5809 - accuracy: 0.6905\n",
      "Epoch 28/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5821 - accuracy: 0.6909\n",
      "Epoch 29/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5838 - accuracy: 0.6867\n",
      "Epoch 30/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5815 - accuracy: 0.6905\n",
      "Epoch 31/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5810 - accuracy: 0.6903\n",
      "Epoch 32/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5826 - accuracy: 0.6880\n",
      "Epoch 33/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5814 - accuracy: 0.6914\n",
      "Epoch 34/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5826 - accuracy: 0.6892\n",
      "Epoch 35/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5813 - accuracy: 0.6914\n",
      "Epoch 36/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5822 - accuracy: 0.6898\n",
      "Epoch 37/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5797 - accuracy: 0.6920\n",
      "Epoch 38/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5805 - accuracy: 0.6925\n",
      "Epoch 39/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5787 - accuracy: 0.6940\n",
      "Epoch 40/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5803 - accuracy: 0.6929\n",
      "Epoch 41/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5810 - accuracy: 0.6909\n",
      "Epoch 42/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5808 - accuracy: 0.6930\n",
      "Epoch 43/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5833 - accuracy: 0.6887\n",
      "Epoch 44/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5804 - accuracy: 0.6922\n",
      "Epoch 45/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5798 - accuracy: 0.6927\n",
      "Epoch 46/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5814 - accuracy: 0.6893\n",
      "Epoch 47/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5811 - accuracy: 0.6912\n",
      "Epoch 48/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5835 - accuracy: 0.6897\n",
      "Epoch 49/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5818 - accuracy: 0.6909\n",
      " 92/403 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403/403 [==============================] - 0s 1ms/step\n",
      "Epoch 1/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6807 - accuracy: 0.5605\n",
      "Epoch 2/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6262 - accuracy: 0.6653\n",
      "Epoch 3/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5970 - accuracy: 0.6854\n",
      "Epoch 4/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5900 - accuracy: 0.6859\n",
      "Epoch 5/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5866 - accuracy: 0.6907\n",
      "Epoch 6/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5847 - accuracy: 0.6908\n",
      "Epoch 7/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5852 - accuracy: 0.6885\n",
      "Epoch 8/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5857 - accuracy: 0.6894\n",
      "Epoch 9/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5849 - accuracy: 0.6889\n",
      "Epoch 10/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5816 - accuracy: 0.6934\n",
      "Epoch 11/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5849 - accuracy: 0.6897\n",
      "Epoch 12/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5841 - accuracy: 0.6891\n",
      "Epoch 13/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5831 - accuracy: 0.6919\n",
      "Epoch 14/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5847 - accuracy: 0.6884\n",
      "Epoch 15/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5824 - accuracy: 0.6925\n",
      "Epoch 16/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5844 - accuracy: 0.6912\n",
      "Epoch 17/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5842 - accuracy: 0.6890\n",
      "Epoch 18/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5829 - accuracy: 0.6904\n",
      "Epoch 19/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5827 - accuracy: 0.6915\n",
      "Epoch 20/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5804 - accuracy: 0.6927\n",
      "Epoch 21/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5846 - accuracy: 0.6908\n",
      "Epoch 22/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5836 - accuracy: 0.6885\n",
      "Epoch 23/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5836 - accuracy: 0.6912\n",
      "Epoch 24/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5836 - accuracy: 0.6886\n",
      "Epoch 25/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5841 - accuracy: 0.6904\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.001386095951406657.\n",
      "Epoch 26/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5817 - accuracy: 0.6914\n",
      "Epoch 27/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5790 - accuracy: 0.6918\n",
      "Epoch 28/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5811 - accuracy: 0.6910\n",
      "Epoch 29/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5812 - accuracy: 0.6888\n",
      "Epoch 30/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5797 - accuracy: 0.6910\n",
      "Epoch 31/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5830 - accuracy: 0.6905\n",
      "Epoch 32/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5812 - accuracy: 0.6898\n",
      "Epoch 33/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5787 - accuracy: 0.6953\n",
      "Epoch 34/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5758 - accuracy: 0.6944\n",
      "Epoch 35/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5790 - accuracy: 0.6927\n",
      "Epoch 36/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5779 - accuracy: 0.6947\n",
      "Epoch 37/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5773 - accuracy: 0.6921\n",
      "Epoch 38/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5766 - accuracy: 0.6956\n",
      "Epoch 39/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5788 - accuracy: 0.6920\n",
      "Epoch 40/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5795 - accuracy: 0.6934\n",
      "Epoch 41/49\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 0.5780 - accuracy: 0.6954\n",
      "Epoch 42/49\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5772 - accuracy: 0.6949\n",
      "Epoch 43/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5781 - accuracy: 0.6924\n",
      "Epoch 44/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5788 - accuracy: 0.6936\n",
      "Epoch 45/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5795 - accuracy: 0.6899\n",
      "Epoch 46/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5788 - accuracy: 0.6931\n",
      "Epoch 47/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5793 - accuracy: 0.6914\n",
      "Epoch 48/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5812 - accuracy: 0.6902\n",
      "Epoch 49/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5779 - accuracy: 0.6938\n",
      " 85/403 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403/403 [==============================] - 1s 1ms/step\n",
      "Epoch 1/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.7171 - accuracy: 0.5451\n",
      "Epoch 2/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6786 - accuracy: 0.5718\n",
      "Epoch 3/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6698 - accuracy: 0.6006\n",
      "Epoch 4/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6605 - accuracy: 0.6259\n",
      "Epoch 5/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6505 - accuracy: 0.6425\n",
      "Epoch 6/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6409 - accuracy: 0.6540\n",
      "Epoch 7/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6317 - accuracy: 0.6613\n",
      "Epoch 8/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6213 - accuracy: 0.6701\n",
      "Epoch 9/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6142 - accuracy: 0.6734\n",
      "Epoch 10/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6106 - accuracy: 0.6748\n",
      "Epoch 11/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6056 - accuracy: 0.6771\n",
      "Epoch 12/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.6012 - accuracy: 0.6818\n",
      "Epoch 13/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5966 - accuracy: 0.6846\n",
      "Epoch 14/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5941 - accuracy: 0.6879\n",
      "Epoch 15/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5924 - accuracy: 0.6872\n",
      "Epoch 16/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5891 - accuracy: 0.6874\n",
      "Epoch 17/49\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 0.5900 - accuracy: 0.6875\n",
      "Epoch 18/49\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 0.5872 - accuracy: 0.6893\n",
      "Epoch 19/49\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 0.5866 - accuracy: 0.6900\n",
      "Epoch 20/49\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 0.5835 - accuracy: 0.6917\n",
      "Epoch 21/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5864 - accuracy: 0.6899\n",
      "Epoch 22/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5824 - accuracy: 0.6926\n",
      "Epoch 23/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5822 - accuracy: 0.6926\n",
      "Epoch 24/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5834 - accuracy: 0.6928\n",
      "Epoch 25/49\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 0.5847 - accuracy: 0.6901\n",
      "Epoch 26/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5806 - accuracy: 0.6954\n",
      "Epoch 27/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5802 - accuracy: 0.6952\n",
      "Epoch 28/49\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 0.5823 - accuracy: 0.6919\n",
      "Epoch 29/49\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 0.5830 - accuracy: 0.6888\n",
      "Epoch 30/49\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 0.5821 - accuracy: 0.6922\n",
      "Epoch 31/49\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 0.5829 - accuracy: 0.6934\n",
      "Epoch 32/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5828 - accuracy: 0.6907\n",
      "Epoch 33/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5801 - accuracy: 0.6925\n",
      "Epoch 34/49\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 0.5827 - accuracy: 0.6917\n",
      "Epoch 35/49\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 0.5804 - accuracy: 0.6932\n",
      "Epoch 36/49\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 0.5796 - accuracy: 0.6931\n",
      "Epoch 37/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5801 - accuracy: 0.6938\n",
      "Epoch 38/49\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 0.5801 - accuracy: 0.6923\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 39/49\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 0.5794 - accuracy: 0.6928\n",
      "Epoch 40/49\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 0.5779 - accuracy: 0.6959\n",
      "Epoch 41/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5798 - accuracy: 0.6935\n",
      "Epoch 42/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5809 - accuracy: 0.6928\n",
      "Epoch 43/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5841 - accuracy: 0.6913\n",
      "Epoch 44/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5796 - accuracy: 0.6924\n",
      "Epoch 45/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5775 - accuracy: 0.6928\n",
      "Epoch 46/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5840 - accuracy: 0.6893\n",
      "Epoch 47/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5793 - accuracy: 0.6925\n",
      "Epoch 48/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5799 - accuracy: 0.6913\n",
      "Epoch 49/49\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 0.5809 - accuracy: 0.6927\n",
      " 84/403 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403/403 [==============================] - 1s 1ms/step\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6634  \u001b[0m | \u001b[0m 1.124   \u001b[0m | \u001b[0m 61.63   \u001b[0m | \u001b[0m 0.732   \u001b[0m | \u001b[0m 0.1796  \u001b[0m | \u001b[0m 48.72   \u001b[0m | \u001b[0m 1.312   \u001b[0m | \u001b[0m 1.116   \u001b[0m | \u001b[0m 0.8663  \u001b[0m | \u001b[0m 124.2   \u001b[0m | \u001b[0m 0.7081  \u001b[0m | \u001b[0m 0.08234 \u001b[0m |\n",
      "Epoch 1/52\n",
      "1783/1783 [==============================] - 12s 7ms/step - loss: 2725.2889 - accuracy: 0.5619\n",
      "Epoch 2/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 12268214.6125 - accuracy: 0.6300\n",
      "Epoch 3/52\n",
      "1783/1783 [==============================] - 12s 7ms/step - loss: 1312520.6861 - accuracy: 0.5568\n",
      "Epoch 4/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 1402623.0253 - accuracy: 0.5547\n",
      "Epoch 5/52\n",
      "1783/1783 [==============================] - 10s 6ms/step - loss: 141102993.0095 - accuracy: 0.5346\n",
      "Epoch 6/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 30132322.7749 - accuracy: 0.5340\n",
      "Epoch 7/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 2541451.3655 - accuracy: 0.5585\n",
      "Epoch 8/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 21598990.3511 - accuracy: 0.5803\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.08650261472469473.\n",
      "Epoch 9/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 133998.5225 - accuracy: 0.6083\n",
      "Epoch 10/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 131046.1700 - accuracy: 0.5793\n",
      "Epoch 11/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 134760.7080 - accuracy: 0.5735\n",
      "Epoch 00011: early stopping\n",
      " 73/446 [===>..........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446/446 [==============================] - 1s 1ms/step\n",
      "Epoch 1/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 25443.6222 - accuracy: 0.5084\n",
      "Epoch 2/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 17.5831 - accuracy: 0.5455\n",
      "Epoch 3/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 168.1276 - accuracy: 0.5444\n",
      "Epoch 4/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 1.1634 - accuracy: 0.5853\n",
      "Epoch 5/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 6065.4169 - accuracy: 0.5828\n",
      "Epoch 6/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 48.7386 - accuracy: 0.5807\n",
      "Epoch 7/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 18.1753 - accuracy: 0.5809\n",
      "Epoch 8/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 3785.9598 - accuracy: 0.5358\n",
      "Epoch 9/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 36.2363 - accuracy: 0.5663\n",
      "Epoch 10/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 1.9869 - accuracy: 0.5871\n",
      "Epoch 11/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 31867.2194 - accuracy: 0.5347\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.017300522944938945.\n",
      "Epoch 12/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 20.3977 - accuracy: 0.5893\n",
      "Epoch 13/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 29.4664 - accuracy: 0.5691\n",
      "Epoch 14/52\n",
      "1783/1783 [==============================] - 10s 6ms/step - loss: 29.4945 - accuracy: 0.5689\n",
      "Epoch 00014: early stopping\n",
      " 76/446 [====>.........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446/446 [==============================] - 1s 1ms/step\n",
      "Epoch 1/52\n",
      "1783/1783 [==============================] - 12s 6ms/step - loss: 1.1577 - accuracy: 0.6163\n",
      "Epoch 2/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5890 - accuracy: 0.6861\n",
      "Epoch 3/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5862 - accuracy: 0.6905\n",
      "Epoch 4/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.7023 - accuracy: 0.6735\n",
      "Epoch 5/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5931 - accuracy: 0.6825\n",
      "Epoch 6/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5883 - accuracy: 0.6884\n",
      "Epoch 7/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.6704 - accuracy: 0.6804\n",
      "Epoch 8/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.6009 - accuracy: 0.6776\n",
      "Epoch 9/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5896 - accuracy: 0.6853\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0034601045889877894.\n",
      "Epoch 10/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5792 - accuracy: 0.6922\n",
      "Epoch 11/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5800 - accuracy: 0.6915\n",
      "Epoch 12/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5774 - accuracy: 0.6937\n",
      "Epoch 13/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5788 - accuracy: 0.6926\n",
      "Epoch 14/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5758 - accuracy: 0.6954\n",
      "Epoch 15/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5772 - accuracy: 0.6950\n",
      "Epoch 16/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5770 - accuracy: 0.6940\n",
      "Epoch 17/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5775 - accuracy: 0.6947\n",
      "Epoch 18/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5760 - accuracy: 0.6936\n",
      "Epoch 19/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5761 - accuracy: 0.6941\n",
      "Epoch 20/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5775 - accuracy: 0.6919\n",
      "Epoch 21/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5750 - accuracy: 0.6963\n",
      "Epoch 22/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5765 - accuracy: 0.6950\n",
      "Epoch 23/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5766 - accuracy: 0.6934\n",
      "Epoch 24/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5790 - accuracy: 0.6922\n",
      "Epoch 25/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5766 - accuracy: 0.6948\n",
      "Epoch 26/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5757 - accuracy: 0.6938\n",
      "Epoch 27/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5762 - accuracy: 0.6940\n",
      "Epoch 28/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5773 - accuracy: 0.6958\n",
      "Epoch 29/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5772 - accuracy: 0.6933\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 30/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5752 - accuracy: 0.6925\n",
      "Epoch 31/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5717 - accuracy: 0.6959\n",
      "Epoch 32/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5743 - accuracy: 0.6938\n",
      "Epoch 33/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5731 - accuracy: 0.6952\n",
      "Epoch 34/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5747 - accuracy: 0.6969\n",
      "Epoch 35/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5725 - accuracy: 0.6960\n",
      "Epoch 36/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5699 - accuracy: 0.6987\n",
      "Epoch 37/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5732 - accuracy: 0.6965\n",
      "Epoch 38/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5727 - accuracy: 0.6971\n",
      "Epoch 39/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5741 - accuracy: 0.6966\n",
      "Epoch 40/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5740 - accuracy: 0.6951\n",
      "Epoch 41/52\n",
      "1783/1783 [==============================] - 12s 7ms/step - loss: 0.5715 - accuracy: 0.6968\n",
      "Epoch 42/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5725 - accuracy: 0.6961\n",
      "Epoch 43/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5735 - accuracy: 0.6950\n",
      "Epoch 44/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5741 - accuracy: 0.6940\n",
      "Epoch 45/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5736 - accuracy: 0.6955\n",
      "Epoch 46/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5734 - accuracy: 0.6940\n",
      "Epoch 47/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5736 - accuracy: 0.6943\n",
      "Epoch 48/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5728 - accuracy: 0.6951\n",
      "Epoch 49/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5735 - accuracy: 0.6939\n",
      "Epoch 50/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5727 - accuracy: 0.6961\n",
      "Epoch 51/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5733 - accuracy: 0.6946\n",
      "Epoch 52/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5725 - accuracy: 0.6975\n",
      " 75/446 [====>.........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446/446 [==============================] - 1s 1ms/step\n",
      "Epoch 1/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5940 - accuracy: 0.6798\n",
      "Epoch 2/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5769 - accuracy: 0.6934\n",
      "Epoch 3/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5756 - accuracy: 0.6963\n",
      "Epoch 4/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5778 - accuracy: 0.6936\n",
      "Epoch 5/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5778 - accuracy: 0.6911\n",
      "Epoch 6/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5743 - accuracy: 0.6956\n",
      "Epoch 7/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5755 - accuracy: 0.6936\n",
      "Epoch 8/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5771 - accuracy: 0.6942\n",
      "Epoch 9/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5733 - accuracy: 0.6942\n",
      "Epoch 10/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5736 - accuracy: 0.6948\n",
      "Epoch 11/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5744 - accuracy: 0.6940\n",
      "Epoch 12/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5742 - accuracy: 0.6955\n",
      "Epoch 13/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5739 - accuracy: 0.6934\n",
      "Epoch 14/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5732 - accuracy: 0.6952\n",
      "Epoch 15/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5751 - accuracy: 0.6942\n",
      "Epoch 16/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5745 - accuracy: 0.6952\n",
      "Epoch 17/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5780 - accuracy: 0.6936\n",
      "Epoch 18/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5719 - accuracy: 0.6964\n",
      "Epoch 19/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5734 - accuracy: 0.6971\n",
      "Epoch 20/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5740 - accuracy: 0.6948\n",
      "Epoch 21/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5727 - accuracy: 0.6968\n",
      "Epoch 22/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5729 - accuracy: 0.6957\n",
      "Epoch 23/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5725 - accuracy: 0.6944\n",
      "Epoch 24/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5709 - accuracy: 0.6977\n",
      "Epoch 25/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5740 - accuracy: 0.6944\n",
      "Epoch 26/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5744 - accuracy: 0.6931\n",
      "Epoch 27/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5753 - accuracy: 0.6962\n",
      "Epoch 28/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5761 - accuracy: 0.6932\n",
      "Epoch 29/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5740 - accuracy: 0.6958\n",
      "Epoch 30/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5734 - accuracy: 0.6968\n",
      "Epoch 31/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5749 - accuracy: 0.6939\n",
      "Epoch 32/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5734 - accuracy: 0.6961\n",
      "Epoch 33/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5745 - accuracy: 0.6930\n",
      "Epoch 34/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5738 - accuracy: 0.6967\n",
      "Epoch 35/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5727 - accuracy: 0.6962\n",
      "Epoch 36/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5732 - accuracy: 0.6951\n",
      "Epoch 37/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5734 - accuracy: 0.6952\n",
      "Epoch 38/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5743 - accuracy: 0.6950\n",
      "Epoch 39/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5714 - accuracy: 0.6963\n",
      "Epoch 40/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5739 - accuracy: 0.6960\n",
      "Epoch 41/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5706 - accuracy: 0.6957\n",
      "Epoch 42/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5703 - accuracy: 0.6991\n",
      "Epoch 43/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5726 - accuracy: 0.6960\n",
      "Epoch 44/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5707 - accuracy: 0.6979\n",
      "Epoch 45/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5719 - accuracy: 0.6962\n",
      "Epoch 46/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5733 - accuracy: 0.6964\n",
      "Epoch 47/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5720 - accuracy: 0.6954\n",
      "Epoch 48/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5728 - accuracy: 0.6928\n",
      "Epoch 49/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5718 - accuracy: 0.6959\n",
      "Epoch 50/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5734 - accuracy: 0.6961\n",
      "Epoch 51/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5701 - accuracy: 0.6994\n",
      "Epoch 52/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5738 - accuracy: 0.6954\n",
      " 73/446 [===>..........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446/446 [==============================] - 1s 1ms/step\n",
      "Epoch 1/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5952 - accuracy: 0.6784\n",
      "Epoch 2/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5778 - accuracy: 0.6939\n",
      "Epoch 3/52\n",
      "1783/1783 [==============================] - 10s 6ms/step - loss: 0.5768 - accuracy: 0.6963\n",
      "Epoch 4/52\n",
      "1783/1783 [==============================] - 10s 6ms/step - loss: 0.5750 - accuracy: 0.6943\n",
      "Epoch 5/52\n",
      "1783/1783 [==============================] - 10s 6ms/step - loss: 0.5758 - accuracy: 0.6937\n",
      "Epoch 6/52\n",
      "1783/1783 [==============================] - 10s 6ms/step - loss: 0.5705 - accuracy: 0.6985\n",
      "Epoch 7/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5750 - accuracy: 0.6945\n",
      "Epoch 8/52\n",
      "1783/1783 [==============================] - 10s 6ms/step - loss: 0.5766 - accuracy: 0.6943\n",
      "Epoch 9/52\n",
      "1783/1783 [==============================] - 10s 6ms/step - loss: 0.5759 - accuracy: 0.6948\n",
      "Epoch 10/52\n",
      "1783/1783 [==============================] - 10s 6ms/step - loss: 0.5738 - accuracy: 0.6961\n",
      "Epoch 11/52\n",
      "1783/1783 [==============================] - 10s 6ms/step - loss: 0.5732 - accuracy: 0.6950\n",
      "Epoch 12/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5752 - accuracy: 0.6944\n",
      "Epoch 13/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5721 - accuracy: 0.6970\n",
      "Epoch 14/52\n",
      "1783/1783 [==============================] - 10s 6ms/step - loss: 0.5721 - accuracy: 0.6968\n",
      "Epoch 15/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5733 - accuracy: 0.6962\n",
      "Epoch 16/52\n",
      "1783/1783 [==============================] - 10s 6ms/step - loss: 0.5725 - accuracy: 0.6978\n",
      "Epoch 17/52\n",
      "1783/1783 [==============================] - 10s 6ms/step - loss: 0.5736 - accuracy: 0.6962\n",
      "Epoch 18/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5730 - accuracy: 0.6974\n",
      "Epoch 19/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5740 - accuracy: 0.6974\n",
      "Epoch 20/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5752 - accuracy: 0.6937\n",
      "Epoch 21/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5760 - accuracy: 0.6943\n",
      "Epoch 22/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5725 - accuracy: 0.6961\n",
      "Epoch 23/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5718 - accuracy: 0.6992\n",
      "Epoch 24/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5730 - accuracy: 0.6964\n",
      "Epoch 25/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5762 - accuracy: 0.6933\n",
      "Epoch 26/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5726 - accuracy: 0.6980\n",
      "Epoch 27/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5721 - accuracy: 0.6981\n",
      "Epoch 28/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5720 - accuracy: 0.6961\n",
      "Epoch 29/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5719 - accuracy: 0.6984\n",
      "Epoch 30/52\n",
      "1783/1783 [==============================] - 10s 6ms/step - loss: 0.5736 - accuracy: 0.6947\n",
      "Epoch 31/52\n",
      "1783/1783 [==============================] - 10s 6ms/step - loss: 0.5710 - accuracy: 0.6956\n",
      "Epoch 32/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5705 - accuracy: 0.6965\n",
      "Epoch 33/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5713 - accuracy: 0.6972\n",
      "Epoch 34/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5735 - accuracy: 0.6956\n",
      "Epoch 35/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5721 - accuracy: 0.6972\n",
      "Epoch 36/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5713 - accuracy: 0.6978\n",
      "Epoch 37/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5738 - accuracy: 0.6944\n",
      "Epoch 38/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5702 - accuracy: 0.6968\n",
      "Epoch 39/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5740 - accuracy: 0.6932\n",
      "Epoch 40/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5738 - accuracy: 0.6967\n",
      "Epoch 41/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5700 - accuracy: 0.7006\n",
      "Epoch 42/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5711 - accuracy: 0.6961\n",
      "Epoch 43/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5734 - accuracy: 0.6950\n",
      "Epoch 44/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5747 - accuracy: 0.6934\n",
      "Epoch 45/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5739 - accuracy: 0.6946\n",
      "Epoch 46/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5730 - accuracy: 0.6972\n",
      "Epoch 47/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5714 - accuracy: 0.6977\n",
      "Epoch 48/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5733 - accuracy: 0.6955\n",
      "Epoch 49/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5742 - accuracy: 0.6942\n",
      "Epoch 50/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5683 - accuracy: 0.7019\n",
      "Epoch 51/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5704 - accuracy: 0.6967\n",
      "Epoch 52/52\n",
      "1783/1783 [==============================] - 11s 6ms/step - loss: 0.5744 - accuracy: 0.6945\n",
      " 73/446 [===>..........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446/446 [==============================] - 1s 1ms/step\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6168  \u001b[0m | \u001b[0m 2.91    \u001b[0m | \u001b[0m 55.96   \u001b[0m | \u001b[0m 0.2123  \u001b[0m | \u001b[0m 0.05455 \u001b[0m | \u001b[0m 52.01   \u001b[0m | \u001b[0m 1.608   \u001b[0m | \u001b[0m 2.05    \u001b[0m | \u001b[0m 0.4325  \u001b[0m | \u001b[0m 65.33   \u001b[0m | \u001b[0m 0.6119  \u001b[0m | \u001b[0m 0.558   \u001b[0m |\n",
      "Epoch 1/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6936 - accuracy: 0.5291\n",
      "Epoch 2/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6926 - accuracy: 0.5296\n",
      "Epoch 3/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6887 - accuracy: 0.5420\n",
      "Epoch 4/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6514 - accuracy: 0.6207\n",
      "Epoch 5/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6166 - accuracy: 0.6613\n",
      "Epoch 6/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6138 - accuracy: 0.6630\n",
      "Epoch 7/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6105 - accuracy: 0.6653\n",
      "Epoch 8/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6154 - accuracy: 0.6604\n",
      "Epoch 9/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6133 - accuracy: 0.6644\n",
      "Epoch 10/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6107 - accuracy: 0.6676\n",
      "Epoch 11/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6100 - accuracy: 0.6689\n",
      "Epoch 12/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6136 - accuracy: 0.6664\n",
      "Epoch 13/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6072 - accuracy: 0.6692\n",
      "Epoch 14/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6049 - accuracy: 0.6745\n",
      "Epoch 15/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6084 - accuracy: 0.6708\n",
      "Epoch 16/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6081 - accuracy: 0.6693\n",
      "Epoch 17/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6013 - accuracy: 0.6769\n",
      "Epoch 18/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6067 - accuracy: 0.6715\n",
      "Epoch 19/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6034 - accuracy: 0.6755\n",
      "Epoch 20/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6042 - accuracy: 0.6730\n",
      "Epoch 21/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6019 - accuracy: 0.6727\n",
      "Epoch 22/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6010 - accuracy: 0.6795\n",
      "Epoch 23/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6050 - accuracy: 0.6707\n",
      "Epoch 24/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6012 - accuracy: 0.6761\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.009480792461455546.\n",
      "Epoch 25/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5932 - accuracy: 0.6826\n",
      "Epoch 26/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5910 - accuracy: 0.6865\n",
      "Epoch 27/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5875 - accuracy: 0.6843\n",
      "Epoch 28/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5867 - accuracy: 0.6891\n",
      "Epoch 29/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5873 - accuracy: 0.6882\n",
      "Epoch 30/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5886 - accuracy: 0.6881\n",
      "Epoch 31/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5890 - accuracy: 0.6841\n",
      "Epoch 32/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5845 - accuracy: 0.6902\n",
      "Epoch 33/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5866 - accuracy: 0.6872\n",
      "Epoch 34/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5863 - accuracy: 0.6871\n",
      "Epoch 35/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5848 - accuracy: 0.6875\n",
      "Epoch 36/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5872 - accuracy: 0.6872\n",
      "Epoch 37/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5841 - accuracy: 0.6903\n",
      "Epoch 38/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5869 - accuracy: 0.6859\n",
      "Epoch 39/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5879 - accuracy: 0.6876\n",
      "Epoch 40/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5893 - accuracy: 0.6845\n",
      "Epoch 41/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5854 - accuracy: 0.6892\n",
      "Epoch 42/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5853 - accuracy: 0.6902\n",
      "Epoch 43/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5824 - accuracy: 0.6926\n",
      "Epoch 44/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5839 - accuracy: 0.6911\n",
      "Epoch 45/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5842 - accuracy: 0.6863\n",
      "Epoch 46/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5842 - accuracy: 0.6903\n",
      "Epoch 47/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5836 - accuracy: 0.6905\n",
      "Epoch 48/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5836 - accuracy: 0.6927\n",
      "Epoch 49/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5831 - accuracy: 0.6922\n",
      "Epoch 50/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5858 - accuracy: 0.6885\n",
      "Epoch 51/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5846 - accuracy: 0.6902\n",
      "Epoch 52/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5850 - accuracy: 0.6893\n",
      "Epoch 53/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5834 - accuracy: 0.6937\n",
      "Epoch 54/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.5857 - accuracy: 0.6879\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0018961584922911091.\n",
      " 85/735 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735/735 [==============================] - 1s 1ms/step\n",
      "Epoch 1/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6968 - accuracy: 0.5285\n",
      "Epoch 2/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6900 - accuracy: 0.5411\n",
      "Epoch 3/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6900 - accuracy: 0.5407\n",
      "Epoch 4/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6898 - accuracy: 0.5416\n",
      "Epoch 5/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5440\n",
      "Epoch 6/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5436\n",
      "Epoch 7/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6899 - accuracy: 0.5409\n",
      "Epoch 8/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6899 - accuracy: 0.5409\n",
      "Epoch 9/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5425\n",
      "Epoch 10/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6896 - accuracy: 0.5423\n",
      "Epoch 11/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6899 - accuracy: 0.5407\n",
      "Epoch 12/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6897 - accuracy: 0.5413\n",
      "Epoch 13/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6892 - accuracy: 0.5444\n",
      "Epoch 14/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5428\n",
      "Epoch 15/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5426\n",
      "Epoch 16/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6898 - accuracy: 0.5403\n",
      "Epoch 17/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5433\n",
      "Epoch 18/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5431\n",
      "Epoch 19/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6892 - accuracy: 0.5437\n",
      "Epoch 20/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5423\n",
      "Epoch 21/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6896 - accuracy: 0.5415\n",
      "Epoch 22/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5418\n",
      "Epoch 23/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5426\n",
      "Epoch 24/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6888 - accuracy: 0.5456\n",
      "Epoch 25/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5423\n",
      "Epoch 26/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5427\n",
      "Epoch 27/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5437\n",
      "Epoch 28/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6885 - accuracy: 0.5467\n",
      "Epoch 29/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6884 - accuracy: 0.5467\n",
      "Epoch 30/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6890 - accuracy: 0.5434\n",
      "Epoch 31/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6892 - accuracy: 0.5415\n",
      "Epoch 32/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5424\n",
      "Epoch 33/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5413\n",
      "Epoch 34/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6890 - accuracy: 0.5422\n",
      "Epoch 35/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6887 - accuracy: 0.5437\n",
      "Epoch 36/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6890 - accuracy: 0.5413\n",
      "Epoch 37/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6888 - accuracy: 0.5426\n",
      "Epoch 38/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6892 - accuracy: 0.5397\n",
      "Epoch 39/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6890 - accuracy: 0.5410\n",
      "Epoch 40/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6884 - accuracy: 0.5441\n",
      "Epoch 41/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6889 - accuracy: 0.5414\n",
      "Epoch 42/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6888 - accuracy: 0.5415\n",
      "Epoch 43/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6883 - accuracy: 0.5439\n",
      "Epoch 44/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6883 - accuracy: 0.5438\n",
      "Epoch 45/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6881 - accuracy: 0.5438\n",
      "Epoch 46/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6885 - accuracy: 0.5418\n",
      "Epoch 47/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6887 - accuracy: 0.5396\n",
      "Epoch 48/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6882 - accuracy: 0.5416\n",
      "Epoch 49/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6878 - accuracy: 0.5441\n",
      "Epoch 50/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6881 - accuracy: 0.5407\n",
      "Epoch 51/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6876 - accuracy: 0.5438\n",
      "Epoch 52/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6877 - accuracy: 0.5421\n",
      "Epoch 53/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6873 - accuracy: 0.5438\n",
      "Epoch 54/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6873 - accuracy: 0.5431\n",
      " 86/735 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735/735 [==============================] - 1s 1ms/step\n",
      "Epoch 1/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.7074 - accuracy: 0.5282\n",
      "Epoch 2/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5445\n",
      "Epoch 3/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5436\n",
      "Epoch 4/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6892 - accuracy: 0.5450\n",
      "Epoch 5/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5450\n",
      "Epoch 6/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5436\n",
      "Epoch 7/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5437\n",
      "Epoch 8/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6896 - accuracy: 0.5426\n",
      "Epoch 9/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5424\n",
      "Epoch 10/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5438\n",
      "Epoch 11/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5436\n",
      "Epoch 12/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6896 - accuracy: 0.5417\n",
      "Epoch 13/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5423\n",
      "Epoch 14/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6896 - accuracy: 0.5420\n",
      "Epoch 15/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5439\n",
      "Epoch 16/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5420\n",
      "Epoch 17/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6896 - accuracy: 0.5411\n",
      "Epoch 18/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5419\n",
      "Epoch 19/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6888 - accuracy: 0.5453\n",
      "Epoch 20/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6889 - accuracy: 0.5449\n",
      "Epoch 21/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5409\n",
      "Epoch 22/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6896 - accuracy: 0.5403\n",
      "Epoch 23/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5421\n",
      "Epoch 24/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6892 - accuracy: 0.5427\n",
      "Epoch 25/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5414\n",
      "Epoch 26/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6888 - accuracy: 0.5442\n",
      "Epoch 27/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6890 - accuracy: 0.5427\n",
      "Epoch 28/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6888 - accuracy: 0.5438\n",
      "Epoch 29/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6888 - accuracy: 0.5436\n",
      "Epoch 30/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6884 - accuracy: 0.5453\n",
      "Epoch 31/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6888 - accuracy: 0.5435\n",
      "Epoch 32/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6886 - accuracy: 0.5440\n",
      "Epoch 33/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6888 - accuracy: 0.5431\n",
      "Epoch 34/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5409\n",
      "Epoch 35/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6886 - accuracy: 0.5434\n",
      "Epoch 36/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6887 - accuracy: 0.5427\n",
      "Epoch 37/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6887 - accuracy: 0.5417\n",
      "Epoch 38/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6888 - accuracy: 0.5416\n",
      "Epoch 39/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6884 - accuracy: 0.5433\n",
      "Epoch 40/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6879 - accuracy: 0.5454\n",
      "Epoch 41/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6880 - accuracy: 0.5444\n",
      "Epoch 42/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6883 - accuracy: 0.5424\n",
      "Epoch 43/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6886 - accuracy: 0.5400\n",
      "Epoch 44/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6882 - accuracy: 0.5419\n",
      "Epoch 45/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6881 - accuracy: 0.5419\n",
      "Epoch 46/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6881 - accuracy: 0.5417\n",
      "Epoch 47/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6877 - accuracy: 0.5432\n",
      "Epoch 48/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6874 - accuracy: 0.5439\n",
      "Epoch 49/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6874 - accuracy: 0.5433\n",
      "Epoch 50/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6875 - accuracy: 0.5418\n",
      "Epoch 51/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6872 - accuracy: 0.5424\n",
      "Epoch 52/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6873 - accuracy: 0.5414\n",
      "Epoch 53/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6870 - accuracy: 0.5420\n",
      "Epoch 54/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6864 - accuracy: 0.5436\n",
      " 83/735 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735/735 [==============================] - 1s 1ms/step\n",
      "Epoch 1/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5442\n",
      "Epoch 2/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6897 - accuracy: 0.5425\n",
      "Epoch 3/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5432\n",
      "Epoch 4/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5435\n",
      "Epoch 5/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5432\n",
      "Epoch 6/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6897 - accuracy: 0.5422\n",
      "Epoch 7/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6896 - accuracy: 0.5424\n",
      "Epoch 8/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5436\n",
      "Epoch 9/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6899 - accuracy: 0.5412\n",
      "Epoch 10/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6896 - accuracy: 0.5427\n",
      "Epoch 11/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6897 - accuracy: 0.5418\n",
      "Epoch 12/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6892 - accuracy: 0.5444\n",
      "Epoch 13/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5436\n",
      "Epoch 14/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6901 - accuracy: 0.5393\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 15/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5428\n",
      "Epoch 16/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6892 - accuracy: 0.5437\n",
      "Epoch 17/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6902 - accuracy: 0.5377\n",
      "Epoch 18/54\n",
      "2937/2937 [==============================] - 13s 5ms/step - loss: 0.6894 - accuracy: 0.5430\n",
      "Epoch 19/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5427\n",
      "Epoch 20/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6896 - accuracy: 0.5417\n",
      "Epoch 21/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6896 - accuracy: 0.5412\n",
      "Epoch 22/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6888 - accuracy: 0.5457\n",
      "Epoch 23/54\n",
      "2937/2937 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.54 - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5433\n",
      "Epoch 24/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6892 - accuracy: 0.5434\n",
      "Epoch 25/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5428\n",
      "Epoch 26/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5426\n",
      "Epoch 27/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5425\n",
      "Epoch 28/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5439\n",
      "Epoch 29/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5425\n",
      "Epoch 30/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6892 - accuracy: 0.5432\n",
      "Epoch 31/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5420\n",
      "Epoch 32/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6897 - accuracy: 0.5404\n",
      "Epoch 33/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5434\n",
      "Epoch 34/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5421\n",
      "Epoch 35/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6884 - accuracy: 0.5471\n",
      "Epoch 36/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5422\n",
      "Epoch 37/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6901 - accuracy: 0.5370\n",
      "Epoch 38/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5418\n",
      "Epoch 39/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6890 - accuracy: 0.5438\n",
      "Epoch 40/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5431\n",
      "Epoch 41/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5405\n",
      "Epoch 42/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5407\n",
      "Epoch 43/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6887 - accuracy: 0.5449\n",
      "Epoch 44/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5410\n",
      "Epoch 45/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5416\n",
      "Epoch 46/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6892 - accuracy: 0.5423\n",
      "Epoch 47/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5428\n",
      "Epoch 48/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5416\n",
      "Epoch 49/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6889 - accuracy: 0.5438\n",
      "Epoch 50/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6887 - accuracy: 0.5447\n",
      "Epoch 51/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5424\n",
      "Epoch 52/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5422\n",
      "Epoch 53/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5422\n",
      "Epoch 54/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6885 - accuracy: 0.5451\n",
      " 90/735 [==>...........................] - ETA: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735/735 [==============================] - 1s 1ms/step\n",
      "Epoch 1/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6902 - accuracy: 0.5420\n",
      "Epoch 2/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6898 - accuracy: 0.5414\n",
      "Epoch 3/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6897 - accuracy: 0.5419\n",
      "Epoch 4/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6897 - accuracy: 0.5418\n",
      "Epoch 5/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6898 - accuracy: 0.5412\n",
      "Epoch 6/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5435\n",
      "Epoch 7/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5425\n",
      "Epoch 8/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5434\n",
      "Epoch 9/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5439\n",
      "Epoch 10/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6898 - accuracy: 0.5407\n",
      "Epoch 11/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6899 - accuracy: 0.5403\n",
      "Epoch 12/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5424\n",
      "Epoch 13/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5424\n",
      "Epoch 14/54\n",
      "2937/2937 [==============================] - 13s 5ms/step - loss: 0.6894 - accuracy: 0.5429\n",
      "Epoch 15/54\n",
      "2937/2937 [==============================] - 13s 5ms/step - loss: 0.6891 - accuracy: 0.5446\n",
      "Epoch 16/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5430\n",
      "Epoch 17/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6888 - accuracy: 0.5462\n",
      "Epoch 18/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5432\n",
      "Epoch 19/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5419\n",
      "Epoch 20/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5424\n",
      "Epoch 21/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6897 - accuracy: 0.5408\n",
      "Epoch 22/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5423\n",
      "Epoch 23/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6897 - accuracy: 0.5409\n",
      "Epoch 24/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5418\n",
      "Epoch 25/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5421\n",
      "Epoch 26/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6898 - accuracy: 0.5393\n",
      "Epoch 27/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5422\n",
      "Epoch 28/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5425\n",
      "Epoch 29/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6887 - accuracy: 0.5460\n",
      "Epoch 30/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5413\n",
      "Epoch 31/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5420\n",
      "Epoch 32/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6897 - accuracy: 0.5400\n",
      "Epoch 33/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6895 - accuracy: 0.5410\n",
      "Epoch 34/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6896 - accuracy: 0.5404\n",
      "Epoch 35/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6886 - accuracy: 0.5458\n",
      "Epoch 36/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5428\n",
      "Epoch 37/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6888 - accuracy: 0.5447\n",
      "Epoch 38/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6897 - accuracy: 0.5396\n",
      "Epoch 39/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5429\n",
      "Epoch 40/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6888 - accuracy: 0.5440\n",
      "Epoch 41/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6890 - accuracy: 0.5427\n",
      "Epoch 42/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5424\n",
      "Epoch 43/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6890 - accuracy: 0.5427\n",
      "Epoch 44/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6892 - accuracy: 0.5416\n",
      "Epoch 45/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6886 - accuracy: 0.5448\n",
      "Epoch 46/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5415\n",
      "Epoch 47/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6891 - accuracy: 0.5418\n",
      "Epoch 48/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6893 - accuracy: 0.5402\n",
      "Epoch 49/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6894 - accuracy: 0.5395\n",
      "Epoch 50/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6892 - accuracy: 0.5407\n",
      "Epoch 51/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6888 - accuracy: 0.5432\n",
      "Epoch 52/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6890 - accuracy: 0.5419\n",
      "Epoch 53/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6887 - accuracy: 0.5429\n",
      "Epoch 54/54\n",
      "2937/2937 [==============================] - 13s 4ms/step - loss: 0.6884 - accuracy: 0.5447\n",
      " 87/735 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735/735 [==============================] - 1s 1ms/step\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.571   \u001b[0m | \u001b[0m 0.8764  \u001b[0m | \u001b[0m 33.59   \u001b[0m | \u001b[0m 0.4561  \u001b[0m | \u001b[0m 0.2356  \u001b[0m | \u001b[0m 53.96   \u001b[0m | \u001b[0m 2.028   \u001b[0m | \u001b[0m 2.185   \u001b[0m | \u001b[0m 0.0474  \u001b[0m | \u001b[0m 125.4   \u001b[0m | \u001b[0m 0.1705  \u001b[0m | \u001b[0m 0.2602  \u001b[0m |\n",
      "Epoch 1/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 777.2856 - accuracy: 0.5635\n",
      "Epoch 2/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6673 - accuracy: 0.6253\n",
      "Epoch 3/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6496 - accuracy: 0.6344\n",
      "Epoch 4/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6365 - accuracy: 0.6467\n",
      "Epoch 5/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6313 - accuracy: 0.6500\n",
      "Epoch 6/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6256 - accuracy: 0.6584\n",
      "Epoch 7/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6213 - accuracy: 0.6572\n",
      "Epoch 8/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6227 - accuracy: 0.6553\n",
      "Epoch 9/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6110 - accuracy: 0.6672\n",
      "Epoch 10/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6105 - accuracy: 0.6684\n",
      "Epoch 11/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6049 - accuracy: 0.6749\n",
      "Epoch 12/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6049 - accuracy: 0.6724\n",
      "Epoch 13/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6021 - accuracy: 0.6758\n",
      "Epoch 14/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6010 - accuracy: 0.6762\n",
      "Epoch 15/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6003 - accuracy: 0.6777\n",
      "Epoch 16/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5980 - accuracy: 0.6798\n",
      "Epoch 17/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5964 - accuracy: 0.6812\n",
      "Epoch 18/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5944 - accuracy: 0.6816\n",
      "Epoch 19/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5942 - accuracy: 0.6833\n",
      "Epoch 20/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5950 - accuracy: 0.6813\n",
      "Epoch 21/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5917 - accuracy: 0.6838\n",
      "Epoch 22/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5927 - accuracy: 0.6814\n",
      "Epoch 23/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5875 - accuracy: 0.6874\n",
      "Epoch 24/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5916 - accuracy: 0.6835\n",
      "Epoch 25/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5857 - accuracy: 0.6892\n",
      "Epoch 26/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5866 - accuracy: 0.6880\n",
      "Epoch 27/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5874 - accuracy: 0.6874\n",
      "Epoch 28/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5856 - accuracy: 0.6892\n",
      "Epoch 29/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5865 - accuracy: 0.6865\n",
      "Epoch 30/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5859 - accuracy: 0.6865\n",
      "Epoch 31/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5822 - accuracy: 0.6922\n",
      "Epoch 32/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5876 - accuracy: 0.6857\n",
      "Epoch 33/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5843 - accuracy: 0.6899\n",
      "Epoch 34/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5829 - accuracy: 0.6917\n",
      "Epoch 35/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5814 - accuracy: 0.6909\n",
      "Epoch 36/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5831 - accuracy: 0.6896\n",
      "Epoch 37/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5812 - accuracy: 0.6909\n",
      "Epoch 38/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5791 - accuracy: 0.6932\n",
      "Epoch 39/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5806 - accuracy: 0.6913\n",
      "Epoch 40/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5802 - accuracy: 0.6921\n",
      "Epoch 41/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5800 - accuracy: 0.6929\n",
      "Epoch 42/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5758 - accuracy: 0.6968\n",
      " 87/403 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403/403 [==============================] - 1s 1ms/step\n",
      "Epoch 1/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 1124.5937 - accuracy: 0.5188\n",
      "Epoch 2/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.7862 - accuracy: 0.5758\n",
      "Epoch 3/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6927 - accuracy: 0.6072\n",
      "Epoch 4/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6708 - accuracy: 0.6192\n",
      "Epoch 5/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6555 - accuracy: 0.6297\n",
      "Epoch 6/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6449 - accuracy: 0.6416\n",
      "Epoch 7/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6340 - accuracy: 0.6464\n",
      "Epoch 8/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6329 - accuracy: 0.6461\n",
      "Epoch 9/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6271 - accuracy: 0.6532\n",
      "Epoch 10/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6211 - accuracy: 0.6580\n",
      "Epoch 11/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6185 - accuracy: 0.6609\n",
      "Epoch 12/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6177 - accuracy: 0.6604\n",
      "Epoch 13/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6135 - accuracy: 0.6629\n",
      "Epoch 14/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6121 - accuracy: 0.6672\n",
      "Epoch 15/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6094 - accuracy: 0.6675\n",
      "Epoch 16/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6104 - accuracy: 0.6680\n",
      "Epoch 17/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6096 - accuracy: 0.6658\n",
      "Epoch 18/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6087 - accuracy: 0.6681\n",
      "Epoch 19/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6078 - accuracy: 0.6695\n",
      "Epoch 20/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6046 - accuracy: 0.6719\n",
      "Epoch 21/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6031 - accuracy: 0.6749\n",
      "Epoch 22/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6033 - accuracy: 0.6721\n",
      "Epoch 23/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6020 - accuracy: 0.6739\n",
      "Epoch 24/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5996 - accuracy: 0.6757\n",
      "Epoch 25/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5981 - accuracy: 0.6789\n",
      "Epoch 26/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5989 - accuracy: 0.6771\n",
      "Epoch 27/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5958 - accuracy: 0.6777\n",
      "Epoch 28/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5965 - accuracy: 0.6777\n",
      "Epoch 29/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5965 - accuracy: 0.6795\n",
      "Epoch 30/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5948 - accuracy: 0.6803\n",
      "Epoch 31/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5924 - accuracy: 0.6837\n",
      "Epoch 32/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5939 - accuracy: 0.6811\n",
      "Epoch 33/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5913 - accuracy: 0.6826\n",
      "Epoch 34/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5938 - accuracy: 0.6795\n",
      "Epoch 35/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5902 - accuracy: 0.6831\n",
      "Epoch 36/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5899 - accuracy: 0.6837\n",
      "Epoch 37/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5864 - accuracy: 0.6859\n",
      "Epoch 38/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5867 - accuracy: 0.6868\n",
      "Epoch 39/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5859 - accuracy: 0.6859\n",
      "Epoch 40/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5834 - accuracy: 0.6876\n",
      "Epoch 41/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5889 - accuracy: 0.6858\n",
      "Epoch 42/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5851 - accuracy: 0.6887\n",
      " 87/403 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403/403 [==============================] - 1s 1ms/step\n",
      "Epoch 1/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 721.5380 - accuracy: 0.5016\n",
      "Epoch 2/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.7779 - accuracy: 0.5216\n",
      "Epoch 3/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.7254 - accuracy: 0.5333\n",
      "Epoch 4/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.7157 - accuracy: 0.5464\n",
      "Epoch 5/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6876 - accuracy: 0.5680\n",
      "Epoch 6/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6701 - accuracy: 0.5954\n",
      "Epoch 7/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6650 - accuracy: 0.6081\n",
      "Epoch 8/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6399 - accuracy: 0.6343\n",
      "Epoch 9/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6274 - accuracy: 0.6463\n",
      "Epoch 10/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6223 - accuracy: 0.6541\n",
      "Epoch 11/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6184 - accuracy: 0.6594\n",
      "Epoch 12/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6158 - accuracy: 0.6616\n",
      "Epoch 13/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6094 - accuracy: 0.6657\n",
      "Epoch 14/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6061 - accuracy: 0.6691\n",
      "Epoch 15/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6067 - accuracy: 0.6709\n",
      "Epoch 16/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6016 - accuracy: 0.6746\n",
      "Epoch 17/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6025 - accuracy: 0.6734\n",
      "Epoch 18/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6012 - accuracy: 0.6723\n",
      "Epoch 19/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5997 - accuracy: 0.6798\n",
      "Epoch 20/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5965 - accuracy: 0.6824\n",
      "Epoch 21/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5925 - accuracy: 0.6851\n",
      "Epoch 22/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5932 - accuracy: 0.6836\n",
      "Epoch 23/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5908 - accuracy: 0.6853\n",
      "Epoch 24/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5907 - accuracy: 0.6857\n",
      "Epoch 25/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5896 - accuracy: 0.6868\n",
      "Epoch 26/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5880 - accuracy: 0.6858\n",
      "Epoch 27/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5892 - accuracy: 0.6862\n",
      "Epoch 28/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5849 - accuracy: 0.6866\n",
      "Epoch 29/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5862 - accuracy: 0.6867\n",
      "Epoch 30/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5861 - accuracy: 0.6873\n",
      "Epoch 31/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5837 - accuracy: 0.6896\n",
      "Epoch 32/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5832 - accuracy: 0.6892\n",
      "Epoch 33/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5852 - accuracy: 0.6865\n",
      "Epoch 34/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5860 - accuracy: 0.6862\n",
      "Epoch 35/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5850 - accuracy: 0.6859\n",
      "Epoch 36/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5835 - accuracy: 0.6887\n",
      "Epoch 37/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5840 - accuracy: 0.6880\n",
      "Epoch 38/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5818 - accuracy: 0.6910\n",
      "Epoch 39/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5844 - accuracy: 0.6900\n",
      "Epoch 40/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5823 - accuracy: 0.6925\n",
      "Epoch 41/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5815 - accuracy: 0.6892\n",
      "Epoch 42/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5832 - accuracy: 0.6890\n",
      " 88/403 [=====>........................] - ETA: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403/403 [==============================] - 1s 1ms/step\n",
      "Epoch 1/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 687.0561 - accuracy: 0.5201\n",
      "Epoch 2/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6872 - accuracy: 0.6094\n",
      "Epoch 3/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6426 - accuracy: 0.6446\n",
      "Epoch 4/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6267 - accuracy: 0.6552\n",
      "Epoch 5/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6215 - accuracy: 0.6577\n",
      "Epoch 6/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6136 - accuracy: 0.6616\n",
      "Epoch 7/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6138 - accuracy: 0.6611\n",
      "Epoch 8/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6069 - accuracy: 0.6680\n",
      "Epoch 9/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6051 - accuracy: 0.6702\n",
      "Epoch 10/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6036 - accuracy: 0.6742\n",
      "Epoch 11/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6046 - accuracy: 0.6731\n",
      "Epoch 12/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.6021 - accuracy: 0.6747\n",
      "Epoch 13/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5997 - accuracy: 0.6785\n",
      "Epoch 14/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5975 - accuracy: 0.6816\n",
      "Epoch 15/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5960 - accuracy: 0.6822\n",
      "Epoch 16/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5960 - accuracy: 0.6822\n",
      "Epoch 17/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5974 - accuracy: 0.6820\n",
      "Epoch 18/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5937 - accuracy: 0.6849\n",
      "Epoch 19/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5911 - accuracy: 0.6869\n",
      "Epoch 20/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5918 - accuracy: 0.6844\n",
      "Epoch 21/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5919 - accuracy: 0.6854\n",
      "Epoch 22/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5902 - accuracy: 0.6877\n",
      "Epoch 23/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5905 - accuracy: 0.6840\n",
      "Epoch 24/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5882 - accuracy: 0.6881\n",
      "Epoch 25/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5860 - accuracy: 0.6877\n",
      "Epoch 26/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5880 - accuracy: 0.6885\n",
      "Epoch 27/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5865 - accuracy: 0.6888\n",
      "Epoch 28/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5861 - accuracy: 0.6903\n",
      "Epoch 29/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5835 - accuracy: 0.6903\n",
      "Epoch 30/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5859 - accuracy: 0.6870\n",
      "Epoch 31/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5835 - accuracy: 0.6901\n",
      "Epoch 32/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5833 - accuracy: 0.6924\n",
      "Epoch 33/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5853 - accuracy: 0.6876\n",
      "Epoch 34/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5848 - accuracy: 0.6888\n",
      "Epoch 35/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5820 - accuracy: 0.6897\n",
      "Epoch 36/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5827 - accuracy: 0.6907\n",
      "Epoch 37/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5810 - accuracy: 0.6912\n",
      "Epoch 38/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5817 - accuracy: 0.6918\n",
      "Epoch 39/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5817 - accuracy: 0.6894\n",
      "Epoch 40/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5802 - accuracy: 0.6922\n",
      "Epoch 41/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5803 - accuracy: 0.6914\n",
      "Epoch 42/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5818 - accuracy: 0.6888\n",
      " 92/403 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403/403 [==============================] - 0s 1ms/step\n",
      "Epoch 1/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 1031.6427 - accuracy: 0.5392\n",
      "Epoch 2/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.7821 - accuracy: 0.5919\n",
      "Epoch 3/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6724 - accuracy: 0.6291\n",
      "Epoch 4/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6436 - accuracy: 0.6487\n",
      "Epoch 5/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6263 - accuracy: 0.6611\n",
      "Epoch 6/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6191 - accuracy: 0.6655\n",
      "Epoch 7/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6135 - accuracy: 0.6665\n",
      "Epoch 8/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6078 - accuracy: 0.6710\n",
      "Epoch 9/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6042 - accuracy: 0.6737\n",
      "Epoch 10/42\n",
      "1611/1611 [==============================] - 7s 4ms/step - loss: 0.5997 - accuracy: 0.6773\n",
      "Epoch 11/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5987 - accuracy: 0.6781\n",
      "Epoch 12/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6020 - accuracy: 0.6763\n",
      "Epoch 13/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.6012 - accuracy: 0.6780\n",
      "Epoch 14/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5979 - accuracy: 0.6776\n",
      "Epoch 15/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5928 - accuracy: 0.6807\n",
      "Epoch 16/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5937 - accuracy: 0.6797\n",
      "Epoch 17/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5926 - accuracy: 0.6815\n",
      "Epoch 18/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5936 - accuracy: 0.6828\n",
      "Epoch 19/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5919 - accuracy: 0.6818\n",
      "Epoch 20/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5956 - accuracy: 0.6815\n",
      "Epoch 21/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5919 - accuracy: 0.6832\n",
      "Epoch 22/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5913 - accuracy: 0.6828\n",
      "Epoch 23/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5890 - accuracy: 0.6854\n",
      "Epoch 24/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5888 - accuracy: 0.6851\n",
      "Epoch 25/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5866 - accuracy: 0.6843\n",
      "Epoch 26/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5881 - accuracy: 0.6835\n",
      "Epoch 27/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5882 - accuracy: 0.6845\n",
      "Epoch 28/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5879 - accuracy: 0.6863\n",
      "Epoch 29/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5879 - accuracy: 0.6843\n",
      "Epoch 30/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5864 - accuracy: 0.6855\n",
      "Epoch 31/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5854 - accuracy: 0.6849\n",
      "Epoch 32/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5825 - accuracy: 0.6895\n",
      "Epoch 33/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5846 - accuracy: 0.6876\n",
      "Epoch 34/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5874 - accuracy: 0.6864\n",
      "Epoch 35/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5842 - accuracy: 0.6885\n",
      "Epoch 36/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5856 - accuracy: 0.6885\n",
      "Epoch 37/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5873 - accuracy: 0.6854\n",
      "Epoch 38/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5831 - accuracy: 0.6877\n",
      "Epoch 39/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5826 - accuracy: 0.6906\n",
      "Epoch 40/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5843 - accuracy: 0.6881\n",
      "Epoch 41/42\n",
      "1611/1611 [==============================] - 7s 5ms/step - loss: 0.5846 - accuracy: 0.6878\n",
      "Epoch 42/42\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 0.5789 - accuracy: 0.6933\n",
      " 85/403 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403/403 [==============================] - 1s 1ms/step\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.6868  \u001b[0m | \u001b[95m 2.847   \u001b[0m | \u001b[95m 62.35   \u001b[0m | \u001b[95m 0.8084  \u001b[0m | \u001b[95m 0.09138 \u001b[0m | \u001b[95m 41.72   \u001b[0m | \u001b[95m 2.368   \u001b[0m | \u001b[95m 1.88    \u001b[0m | \u001b[95m 0.1229  \u001b[0m | \u001b[95m 104.1   \u001b[0m | \u001b[95m 0.03439 \u001b[0m | \u001b[95m 3.637   \u001b[0m |\n",
      "Epoch 1/96\n",
      "2080/2080 [==============================] - 21s 10ms/step - loss: 17.7214 - accuracy: 0.5058\n",
      "Epoch 2/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 17.1124 - accuracy: 0.5067\n",
      "Epoch 3/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 17.2682 - accuracy: 0.5034\n",
      "Epoch 4/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 17.3403 - accuracy: 0.5031\n",
      "Epoch 5/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 17.1981 - accuracy: 0.5054\n",
      "Epoch 6/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 17.3202 - accuracy: 0.5020\n",
      "Epoch 7/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 17.2690 - accuracy: 0.5049\n",
      "Epoch 8/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 17.1567 - accuracy: 0.5059\n",
      "Epoch 9/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 17.1778 - accuracy: 0.5065\n",
      "Epoch 10/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 17.1654 - accuracy: 0.5050\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.1550715381075507.\n",
      "Epoch 11/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3524 - accuracy: 0.5047\n",
      "Epoch 12/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3812 - accuracy: 0.5005\n",
      "Epoch 13/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3519 - accuracy: 0.5037\n",
      "Epoch 14/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3597 - accuracy: 0.5031\n",
      "Epoch 15/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3636 - accuracy: 0.5040\n",
      "Epoch 16/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3532 - accuracy: 0.5060\n",
      "Epoch 17/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3938 - accuracy: 0.4998\n",
      "Epoch 18/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3594 - accuracy: 0.5025\n",
      "Epoch 19/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3368 - accuracy: 0.5070\n",
      "Epoch 20/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3102 - accuracy: 0.5082\n",
      "Epoch 21/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3802 - accuracy: 0.5021\n",
      "Epoch 22/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3643 - accuracy: 0.5018\n",
      "Epoch 23/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3395 - accuracy: 0.5075\n",
      "Epoch 24/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3727 - accuracy: 0.5022\n",
      "Epoch 25/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3998 - accuracy: 0.4983\n",
      "Epoch 26/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3538 - accuracy: 0.5029\n",
      "Epoch 27/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3813 - accuracy: 0.4993\n",
      "Epoch 28/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3560 - accuracy: 0.5037\n",
      "Epoch 29/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3587 - accuracy: 0.5045\n",
      "Epoch 30/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 3.3768 - accuracy: 0.5014\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.03101430762151014.\n",
      "Epoch 31/96\n",
      "2080/2080 [==============================] - 21s 10ms/step - loss: 0.9058 - accuracy: 0.5031\n",
      "Epoch 32/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.9133 - accuracy: 0.5027\n",
      "Epoch 33/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.9160 - accuracy: 0.5007\n",
      "Epoch 34/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.9148 - accuracy: 0.5018\n",
      "Epoch 35/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.9084 - accuracy: 0.5052\n",
      "Epoch 36/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.9152 - accuracy: 0.5003\n",
      "Epoch 37/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.9115 - accuracy: 0.5001\n",
      "Epoch 38/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.9129 - accuracy: 0.5025\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0062028615243020285.\n",
      "Epoch 39/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.7052 - accuracy: 0.5172\n",
      "Epoch 40/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.7039 - accuracy: 0.5166\n",
      "Epoch 41/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.7049 - accuracy: 0.5134\n",
      "Epoch 42/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.7055 - accuracy: 0.5166\n",
      "Epoch 43/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.7044 - accuracy: 0.5153\n",
      "Epoch 44/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.7047 - accuracy: 0.5170\n",
      "Epoch 45/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.7038 - accuracy: 0.5192\n",
      "Epoch 46/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.7045 - accuracy: 0.5166\n",
      "Epoch 47/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.7053 - accuracy: 0.5157\n",
      "Epoch 48/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.7043 - accuracy: 0.5186\n",
      "Epoch 49/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.7044 - accuracy: 0.5167\n",
      "Epoch 50/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.7053 - accuracy: 0.5137\n",
      "Epoch 51/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.7058 - accuracy: 0.5122\n",
      "Epoch 52/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.7043 - accuracy: 0.5166\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0012405723048604057.\n",
      "Epoch 53/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6918 - accuracy: 0.5346\n",
      "Epoch 54/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6916 - accuracy: 0.5358\n",
      "Epoch 55/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6915 - accuracy: 0.5356\n",
      "Epoch 56/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6916 - accuracy: 0.5334\n",
      "Epoch 57/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6915 - accuracy: 0.5364\n",
      "Epoch 58/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6915 - accuracy: 0.5358\n",
      "Epoch 59/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6911 - accuracy: 0.5365\n",
      "Epoch 60/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6919 - accuracy: 0.5339\n",
      "Epoch 61/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6918 - accuracy: 0.5333\n",
      "Epoch 62/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6920 - accuracy: 0.5352\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 63/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6914 - accuracy: 0.5344\n",
      "Epoch 64/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6909 - accuracy: 0.5369\n",
      "Epoch 65/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6910 - accuracy: 0.5371\n",
      "Epoch 66/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6914 - accuracy: 0.5352\n",
      "Epoch 67/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6914 - accuracy: 0.5355\n",
      "Epoch 68/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6908 - accuracy: 0.5399\n",
      "Epoch 69/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6912 - accuracy: 0.5369\n",
      "Epoch 70/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6912 - accuracy: 0.5366\n",
      "Epoch 71/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6908 - accuracy: 0.5376\n",
      "Epoch 72/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6912 - accuracy: 0.5375\n",
      "Epoch 73/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6908 - accuracy: 0.5401\n",
      "Epoch 74/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.6914 - accuracy: 0.5338\n",
      "Epoch 00074: early stopping\n",
      " 74/520 [===>..........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520/520 [==============================] - 1s 1ms/step\n",
      "Epoch 1/96\n",
      "2080/2080 [==============================] - 21s 10ms/step - loss: 0.6049 - accuracy: 0.6714\n",
      "Epoch 2/96\n",
      "2080/2080 [==============================] - 21s 10ms/step - loss: 0.5879 - accuracy: 0.6875\n",
      "Epoch 3/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5832 - accuracy: 0.6911\n",
      "Epoch 4/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5836 - accuracy: 0.6893\n",
      "Epoch 5/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5834 - accuracy: 0.6885\n",
      "Epoch 6/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5828 - accuracy: 0.6904\n",
      "Epoch 7/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5829 - accuracy: 0.6894\n",
      "Epoch 8/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5822 - accuracy: 0.6924\n",
      "Epoch 9/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5820 - accuracy: 0.6908\n",
      "Epoch 10/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5807 - accuracy: 0.6927\n",
      "Epoch 11/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5811 - accuracy: 0.6906\n",
      "Epoch 12/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5824 - accuracy: 0.6919\n",
      "Epoch 13/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5830 - accuracy: 0.6907\n",
      "Epoch 14/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5803 - accuracy: 0.6937\n",
      "Epoch 15/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5807 - accuracy: 0.6914\n",
      "Epoch 16/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5828 - accuracy: 0.6909\n",
      "Epoch 17/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5815 - accuracy: 0.6915\n",
      "Epoch 18/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5821 - accuracy: 0.6914\n",
      "Epoch 19/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5819 - accuracy: 0.6911\n",
      "Epoch 20/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5825 - accuracy: 0.6905\n",
      "Epoch 21/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5839 - accuracy: 0.6883\n",
      "Epoch 22/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5809 - accuracy: 0.6897\n",
      "Epoch 23/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5803 - accuracy: 0.6926\n",
      "Epoch 24/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5804 - accuracy: 0.6931\n",
      "Epoch 00024: early stopping\n",
      " 78/520 [===>..........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520/520 [==============================] - 1s 1ms/step\n",
      "Epoch 1/96\n",
      "2080/2080 [==============================] - 21s 10ms/step - loss: 0.6089 - accuracy: 0.6662\n",
      "Epoch 2/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5862 - accuracy: 0.6896\n",
      "Epoch 3/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5857 - accuracy: 0.6880\n",
      "Epoch 4/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5851 - accuracy: 0.6886\n",
      "Epoch 5/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5840 - accuracy: 0.6883\n",
      "Epoch 6/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5833 - accuracy: 0.6925\n",
      "Epoch 7/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5843 - accuracy: 0.6894\n",
      "Epoch 8/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5836 - accuracy: 0.6901\n",
      "Epoch 9/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5822 - accuracy: 0.6924\n",
      "Epoch 10/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5827 - accuracy: 0.6893\n",
      "Epoch 11/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5851 - accuracy: 0.6887\n",
      "Epoch 12/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5836 - accuracy: 0.6904\n",
      "Epoch 13/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5851 - accuracy: 0.6876\n",
      "Epoch 14/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5860 - accuracy: 0.6886\n",
      "Epoch 15/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5832 - accuracy: 0.6893\n",
      "Epoch 16/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5816 - accuracy: 0.6940\n",
      "Epoch 17/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5838 - accuracy: 0.6886\n",
      "Epoch 18/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5830 - accuracy: 0.6917\n",
      "Epoch 19/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5807 - accuracy: 0.6922\n",
      "Epoch 00019: early stopping\n",
      " 79/520 [===>..........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520/520 [==============================] - 1s 1ms/step\n",
      "Epoch 1/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.6127 - accuracy: 0.6620\n",
      "Epoch 2/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5860 - accuracy: 0.6889\n",
      "Epoch 3/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5866 - accuracy: 0.6867\n",
      "Epoch 4/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5848 - accuracy: 0.6898\n",
      "Epoch 5/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5857 - accuracy: 0.6880\n",
      "Epoch 6/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5839 - accuracy: 0.6893\n",
      "Epoch 7/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5843 - accuracy: 0.6872\n",
      "Epoch 8/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5819 - accuracy: 0.6896\n",
      "Epoch 9/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5826 - accuracy: 0.6927\n",
      "Epoch 10/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5840 - accuracy: 0.6897\n",
      "Epoch 11/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5822 - accuracy: 0.6922\n",
      "Epoch 12/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5852 - accuracy: 0.6901\n",
      "Epoch 13/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5837 - accuracy: 0.6911\n",
      "Epoch 14/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5845 - accuracy: 0.6881\n",
      "Epoch 15/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5856 - accuracy: 0.6886\n",
      "Epoch 16/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5829 - accuracy: 0.6899\n",
      "Epoch 17/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5842 - accuracy: 0.6871\n",
      "Epoch 18/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5823 - accuracy: 0.6919\n",
      "Epoch 19/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5834 - accuracy: 0.6868\n",
      "Epoch 20/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5843 - accuracy: 0.6868\n",
      "Epoch 21/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5842 - accuracy: 0.6858\n",
      "Epoch 22/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5824 - accuracy: 0.6891\n",
      "Epoch 23/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5835 - accuracy: 0.6912\n",
      "Epoch 24/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5817 - accuracy: 0.6902\n",
      "Epoch 25/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5841 - accuracy: 0.6879\n",
      "Epoch 26/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5838 - accuracy: 0.6886\n",
      "Epoch 27/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5856 - accuracy: 0.6864\n",
      "Epoch 28/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5871 - accuracy: 0.6845\n",
      "Epoch 29/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5845 - accuracy: 0.6891\n",
      "Epoch 30/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5837 - accuracy: 0.6902\n",
      "Epoch 31/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5815 - accuracy: 0.6906\n",
      "Epoch 00031: early stopping\n",
      " 77/520 [===>..........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520/520 [==============================] - 1s 1ms/step\n",
      "Epoch 1/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.6078 - accuracy: 0.6697\n",
      "Epoch 2/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5881 - accuracy: 0.6857\n",
      "Epoch 3/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5844 - accuracy: 0.6918\n",
      "Epoch 4/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5835 - accuracy: 0.6911\n",
      "Epoch 5/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5831 - accuracy: 0.6904\n",
      "Epoch 6/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5825 - accuracy: 0.6897\n",
      "Epoch 7/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5831 - accuracy: 0.6887\n",
      "Epoch 8/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5821 - accuracy: 0.6925\n",
      "Epoch 9/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5821 - accuracy: 0.6910\n",
      "Epoch 10/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5830 - accuracy: 0.6907\n",
      "Epoch 11/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5797 - accuracy: 0.6949\n",
      "Epoch 12/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5850 - accuracy: 0.6901\n",
      "Epoch 13/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5835 - accuracy: 0.6903\n",
      "Epoch 14/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5828 - accuracy: 0.6916\n",
      "Epoch 15/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5805 - accuracy: 0.6947\n",
      "Epoch 16/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5820 - accuracy: 0.6909\n",
      "Epoch 17/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5804 - accuracy: 0.69190s\n",
      "Epoch 18/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5823 - accuracy: 0.6917\n",
      "Epoch 19/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5820 - accuracy: 0.6913\n",
      "Epoch 20/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5845 - accuracy: 0.6902\n",
      "Epoch 21/96\n",
      "2080/2080 [==============================] - 19s 9ms/step - loss: 0.5825 - accuracy: 0.6941\n",
      "Epoch 22/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5834 - accuracy: 0.6899\n",
      "Epoch 23/96\n",
      "2080/2080 [==============================] - 20s 9ms/step - loss: 0.5814 - accuracy: 0.6920\n",
      "Epoch 24/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5833 - accuracy: 0.6897\n",
      "Epoch 25/96\n",
      "2080/2080 [==============================] - 20s 10ms/step - loss: 0.5853 - accuracy: 0.6865\n",
      "Epoch 00025: early stopping\n",
      " 74/520 [===>..........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520/520 [==============================] - 1s 1ms/step\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6536  \u001b[0m | \u001b[0m 0.7763  \u001b[0m | \u001b[0m 47.8    \u001b[0m | \u001b[0m 0.3117  \u001b[0m | \u001b[0m 0.156   \u001b[0m | \u001b[0m 95.61   \u001b[0m | \u001b[0m 1.37    \u001b[0m | \u001b[0m 2.939   \u001b[0m | \u001b[0m 0.7754  \u001b[0m | \u001b[0m 188.5   \u001b[0m | \u001b[0m 0.8948  \u001b[0m | \u001b[0m 2.392   \u001b[0m |\n",
      "Epoch 1/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 313991919119.5805 - accuracy: 0.5028\n",
      "Epoch 2/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 797833778541.1063 - accuracy: 0.5055\n",
      "Epoch 3/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 608852021831.4729 - accuracy: 0.5047\n",
      "Epoch 4/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 639694552325.0759 - accuracy: 0.5074\n",
      "Epoch 5/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 1067314816722.9321 - accuracy: 0.5050\n",
      "Epoch 6/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 465229385763.2750 - accuracy: 0.5036\n",
      "Epoch 7/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 623279464126.2183 - accuracy: 0.5034\n",
      "Epoch 8/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 522291095284.0280 - accuracy: 0.5029\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.1657817543285555.\n",
      "Epoch 9/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 2663242484.5664 - accuracy: 0.5136\n",
      "Epoch 10/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 2379393109.3964 - accuracy: 0.5135\n",
      "Epoch 11/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 1879670485.9716 - accuracy: 0.5095\n",
      "Epoch 12/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 1700766779.3343 - accuracy: 0.5159\n",
      "Epoch 13/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 1429456978.9621 - accuracy: 0.5153\n",
      "Epoch 14/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 982523163.6034 - accuracy: 0.5135\n",
      "Epoch 15/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 705740184.7583 - accuracy: 0.5155\n",
      "Epoch 16/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 602889340.8312 - accuracy: 0.5087\n",
      "Epoch 17/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 449585751.0975 - accuracy: 0.5133\n",
      "Epoch 18/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 557085259.1003 - accuracy: 0.5109\n",
      "Epoch 19/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 562914213.0840 - accuracy: 0.5125\n",
      "Epoch 20/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 412856715.2904 - accuracy: 0.5094\n",
      "Epoch 21/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 358030948.7723 - accuracy: 0.5060\n",
      "Epoch 22/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 347262280.5716 - accuracy: 0.5101\n",
      "Epoch 23/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 511890789.7561 - accuracy: 0.5088\n",
      "Epoch 24/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 266227416.6513 - accuracy: 0.5119\n",
      "Epoch 25/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 236160971.6387 - accuracy: 0.5093\n",
      "Epoch 26/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 245126358.2788 - accuracy: 0.5132\n",
      "Epoch 27/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 264704941.6267 - accuracy: 0.5178\n",
      "Epoch 28/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 247993041.6685 - accuracy: 0.5135\n",
      "Epoch 29/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 212234922.0110 - accuracy: 0.5030\n",
      "Epoch 30/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 270265062.8816 - accuracy: 0.5094\n",
      "Epoch 31/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 184640969.5101 - accuracy: 0.5118\n",
      "Epoch 32/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 188373607.4018 - accuracy: 0.5103\n",
      "Epoch 33/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 137015922.3293 - accuracy: 0.5090\n",
      "Epoch 34/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 105146877.2290 - accuracy: 0.5078\n",
      "Epoch 35/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 134284304.8812 - accuracy: 0.5104\n",
      "Epoch 36/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 148365179.6118 - accuracy: 0.5042\n",
      "Epoch 37/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 131685836.4867 - accuracy: 0.5060\n",
      "Epoch 38/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 109038638.1125 - accuracy: 0.5085\n",
      "Epoch 39/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 105001887.8659 - accuracy: 0.5065\n",
      "Epoch 40/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 91445350.2932 - accuracy: 0.5131\n",
      "Epoch 41/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 103806776.7510 - accuracy: 0.5072\n",
      "Epoch 42/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 112332194.8312 - accuracy: 0.5048\n",
      "Epoch 43/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 79913815.2585 - accuracy: 0.5054\n",
      "Epoch 44/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 79521714.2640 - accuracy: 0.5079\n",
      "Epoch 45/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 48817666.0274 - accuracy: 0.5087\n",
      "Epoch 46/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 73450134.4547 - accuracy: 0.5098\n",
      "Epoch 47/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 67354365.0928 - accuracy: 0.5042\n",
      "Epoch 48/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 64647233.4918 - accuracy: 0.5116\n",
      "Epoch 49/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 54671379.3078 - accuracy: 0.5075\n",
      "Epoch 50/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 60412124.3573 - accuracy: 0.5062\n",
      "Epoch 51/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 41611468.9876 - accuracy: 0.5077\n",
      "Epoch 52/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 55818763.5267 - accuracy: 0.5059\n",
      "Epoch 53/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 63321877.4515 - accuracy: 0.5049\n",
      "Epoch 54/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 49160036.8718 - accuracy: 0.5086\n",
      "Epoch 55/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 49791268.0049 - accuracy: 0.5064\n",
      "Epoch 56/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 46604716.7588 - accuracy: 0.5040\n",
      "Epoch 57/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 40329508.6225 - accuracy: 0.5067\n",
      "Epoch 58/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 38675949.1550 - accuracy: 0.5027\n",
      "Epoch 59/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 34750247.8308 - accuracy: 0.5055\n",
      "Epoch 60/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 59287117.2029 - accuracy: 0.5072\n",
      "Epoch 61/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 26004450.8448 - accuracy: 0.5092\n",
      "Epoch 62/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 40470961.6757 - accuracy: 0.5062\n",
      "Epoch 63/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 81714645.8664 - accuracy: 0.5082\n",
      "Epoch 64/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 33628541.8515 - accuracy: 0.5076\n",
      "Epoch 65/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 37480488.1050 - accuracy: 0.5057\n",
      "Epoch 66/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 28660793.9967 - accuracy: 0.5045\n",
      "Epoch 67/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 52150201.4910 - accuracy: 0.5076\n",
      "Epoch 68/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 31159947.3858 - accuracy: 0.5075\n",
      "Epoch 69/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 43318137.2762 - accuracy: 0.5040\n",
      "  93/1248 [=>............................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248/1248 [==============================] - 1s 1ms/step\n",
      "Epoch 1/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 11479561.5256 - accuracy: 0.5053\n",
      "Epoch 2/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 135714379.0892 - accuracy: 0.5047\n",
      "Epoch 3/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 32384701.9820 - accuracy: 0.5064\n",
      "Epoch 4/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 27635040.6565 - accuracy: 0.5062\n",
      "Epoch 5/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 31214996.0067 - accuracy: 0.5012\n",
      "Epoch 6/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 42270381.7392 - accuracy: 0.5039\n",
      "Epoch 7/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 28795754.6444 - accuracy: 0.5042\n",
      "Epoch 8/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 65561719.0270 - accuracy: 0.5010\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0331563508657111.\n",
      "Epoch 9/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 249736.9893 - accuracy: 0.5161\n",
      "Epoch 10/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 188186.3101 - accuracy: 0.5226\n",
      "Epoch 11/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 103449.4058 - accuracy: 0.5143\n",
      "Epoch 12/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 99588.0777 - accuracy: 0.5080\n",
      "Epoch 13/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 110530.3463 - accuracy: 0.5179\n",
      "Epoch 14/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 54526.0181 - accuracy: 0.5125\n",
      "Epoch 15/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 37577.7810 - accuracy: 0.5192\n",
      "Epoch 16/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 31076.1679 - accuracy: 0.5136\n",
      "Epoch 17/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 33836.2311 - accuracy: 0.5125\n",
      "Epoch 18/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 31757.4756 - accuracy: 0.5110\n",
      "Epoch 19/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 27611.0034 - accuracy: 0.5120\n",
      "Epoch 20/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 24425.0948 - accuracy: 0.5150\n",
      "Epoch 21/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 22020.0856 - accuracy: 0.5118\n",
      "Epoch 22/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 16993.4755 - accuracy: 0.5092\n",
      "Epoch 23/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 13130.7845 - accuracy: 0.5040\n",
      "Epoch 24/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 15337.7577 - accuracy: 0.5114\n",
      "Epoch 25/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 13139.9154 - accuracy: 0.5074\n",
      "Epoch 26/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 12277.8159 - accuracy: 0.5087\n",
      "Epoch 27/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 13860.7778 - accuracy: 0.5122\n",
      "Epoch 28/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 12926.2718 - accuracy: 0.5142\n",
      "Epoch 29/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 11195.9532 - accuracy: 0.5135\n",
      "Epoch 30/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 9940.3238 - accuracy: 0.5133\n",
      "Epoch 31/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 8168.9857 - accuracy: 0.5141\n",
      "Epoch 32/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 10359.4040 - accuracy: 0.5102\n",
      "Epoch 33/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 12856.6576 - accuracy: 0.5107\n",
      "Epoch 34/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 7929.4878 - accuracy: 0.5130\n",
      "Epoch 35/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 7431.6088 - accuracy: 0.5126\n",
      "Epoch 36/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 6987.6374 - accuracy: 0.5123\n",
      "Epoch 37/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 6960.6808 - accuracy: 0.5066\n",
      "Epoch 38/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 5730.4181 - accuracy: 0.5096\n",
      "Epoch 39/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 5421.8920 - accuracy: 0.5112\n",
      "Epoch 40/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 7175.1894 - accuracy: 0.5128\n",
      "Epoch 41/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 4590.4864 - accuracy: 0.5125\n",
      "Epoch 42/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 4820.7210 - accuracy: 0.5066\n",
      "Epoch 43/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 4721.7329 - accuracy: 0.5087\n",
      "Epoch 44/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 4385.8308 - accuracy: 0.5076\n",
      "Epoch 45/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 4078.9427 - accuracy: 0.5084\n",
      "Epoch 46/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 4876.1030 - accuracy: 0.5049\n",
      "Epoch 47/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 4219.5707 - accuracy: 0.5058\n",
      "Epoch 48/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 3933.8010 - accuracy: 0.5101\n",
      "Epoch 49/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 4499.1435 - accuracy: 0.5068\n",
      "Epoch 50/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 2978.5271 - accuracy: 0.5106\n",
      "Epoch 51/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 4452.7141 - accuracy: 0.5097\n",
      "Epoch 52/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 2781.2020 - accuracy: 0.5133\n",
      "Epoch 53/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 4861.0636 - accuracy: 0.5059\n",
      "Epoch 54/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 3062.7895 - accuracy: 0.5075\n",
      "Epoch 55/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 2778.2121 - accuracy: 0.5061\n",
      "Epoch 56/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 2140.5691 - accuracy: 0.5076\n",
      "Epoch 57/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 3002.4885 - accuracy: 0.5051\n",
      "Epoch 58/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 2278.0097 - accuracy: 0.5110\n",
      "Epoch 59/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 2553.2848 - accuracy: 0.5075\n",
      "Epoch 60/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 4008.1437 - accuracy: 0.5070\n",
      "Epoch 61/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 3081.8637 - accuracy: 0.5083\n",
      "Epoch 62/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 3623.8260 - accuracy: 0.5047\n",
      "Epoch 63/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 2614.2106 - accuracy: 0.5092\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.006631270173142221.\n",
      "Epoch 64/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 18.5153 - accuracy: 0.5362\n",
      "Epoch 65/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 8.5615 - accuracy: 0.5568\n",
      "Epoch 66/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 7.8657 - accuracy: 0.5594\n",
      "Epoch 67/69\n",
      "4992/4992 [==============================] - 40s 8ms/step - loss: 4.4976 - accuracy: 0.5565\n",
      "Epoch 68/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 4.4917 - accuracy: 0.5545\n",
      "Epoch 69/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 3.4658 - accuracy: 0.5548\n",
      "  90/1248 [=>............................] - ETA: 1 - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248/1248 [==============================] - 1s 1ms/step\n",
      "Epoch 1/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 1.0401 - accuracy: 0.5935\n",
      "Epoch 2/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.6936 - accuracy: 0.6494\n",
      "Epoch 3/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.6884 - accuracy: 0.6548\n",
      "Epoch 4/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.6988 - accuracy: 0.6556\n",
      "Epoch 5/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.7042 - accuracy: 0.6560\n",
      "Epoch 6/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.6839 - accuracy: 0.6544\n",
      "Epoch 7/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.6999 - accuracy: 0.6515\n",
      "Epoch 8/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.7173 - accuracy: 0.6558\n",
      "Epoch 9/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.7057 - accuracy: 0.6554\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0013262540346284442.\n",
      "Epoch 10/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.5880 - accuracy: 0.6866\n",
      "Epoch 11/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.5861 - accuracy: 0.6916\n",
      "Epoch 12/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5880 - accuracy: 0.6917\n",
      "Epoch 13/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.5916 - accuracy: 0.6883\n",
      "Epoch 14/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.5941 - accuracy: 0.6863\n",
      "Epoch 15/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.5909 - accuracy: 0.6874\n",
      "Epoch 16/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.5899 - accuracy: 0.6854\n",
      "Epoch 17/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.5915 - accuracy: 0.6905\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 18/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.5848 - accuracy: 0.6941\n",
      "Epoch 19/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.5865 - accuracy: 0.6905\n",
      "Epoch 20/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.5880 - accuracy: 0.6913\n",
      "Epoch 00020: early stopping\n",
      "  87/1248 [=>............................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248/1248 [==============================] - 1s 1ms/step\n",
      "Epoch 1/69\n",
      "4992/4992 [==============================] - 40s 8ms/step - loss: 0.6499 - accuracy: 0.6494\n",
      "Epoch 2/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5976 - accuracy: 0.6832\n",
      "Epoch 3/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5946 - accuracy: 0.6840\n",
      "Epoch 4/69\n",
      "4992/4992 [==============================] - 38s 8ms/step - loss: 0.5925 - accuracy: 0.6853\n",
      "Epoch 5/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5919 - accuracy: 0.6850\n",
      "Epoch 6/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5901 - accuracy: 0.6864\n",
      "Epoch 7/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5913 - accuracy: 0.6856\n",
      "Epoch 8/69\n",
      "4992/4992 [==============================] - ETA: 0s - loss: 0.5934 - accuracy: 0.68 - 39s 8ms/step - loss: 0.5934 - accuracy: 0.6842\n",
      "Epoch 9/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5888 - accuracy: 0.6903\n",
      "Epoch 10/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5936 - accuracy: 0.6852\n",
      "Epoch 11/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5905 - accuracy: 0.6858\n",
      "Epoch 12/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5915 - accuracy: 0.6861\n",
      "Epoch 13/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5902 - accuracy: 0.6869\n",
      "Epoch 14/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5906 - accuracy: 0.6886\n",
      "Epoch 15/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5897 - accuracy: 0.6857\n",
      "Epoch 16/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5900 - accuracy: 0.6871\n",
      "Epoch 17/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5893 - accuracy: 0.6881\n",
      "Epoch 18/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5926 - accuracy: 0.6836\n",
      "Epoch 19/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5938 - accuracy: 0.6858\n",
      "Epoch 20/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5919 - accuracy: 0.6890\n",
      "Epoch 21/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5894 - accuracy: 0.6869\n",
      "Epoch 22/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5913 - accuracy: 0.6861\n",
      "Epoch 00022: early stopping\n",
      "  85/1248 [=>............................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248/1248 [==============================] - 1s 1ms/step\n",
      "Epoch 1/69\n",
      "4992/4992 [==============================] - 40s 8ms/step - loss: 0.6608 - accuracy: 0.6434\n",
      "Epoch 2/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5929 - accuracy: 0.6828\n",
      "Epoch 3/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5910 - accuracy: 0.6863\n",
      "Epoch 4/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5901 - accuracy: 0.6858\n",
      "Epoch 5/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5918 - accuracy: 0.6855\n",
      "Epoch 6/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5903 - accuracy: 0.6882\n",
      "Epoch 7/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5882 - accuracy: 0.6884\n",
      "Epoch 8/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5885 - accuracy: 0.6884\n",
      "Epoch 9/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5884 - accuracy: 0.6901\n",
      "Epoch 10/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5873 - accuracy: 0.6893\n",
      "Epoch 11/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5904 - accuracy: 0.6873\n",
      "Epoch 12/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5916 - accuracy: 0.6883\n",
      "Epoch 13/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5936 - accuracy: 0.6848\n",
      "Epoch 14/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5897 - accuracy: 0.6869\n",
      "Epoch 15/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5897 - accuracy: 0.6874\n",
      "Epoch 16/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5891 - accuracy: 0.6869\n",
      "Epoch 17/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5892 - accuracy: 0.6876\n",
      "Epoch 18/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5913 - accuracy: 0.6863\n",
      "Epoch 19/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5888 - accuracy: 0.6878\n",
      "Epoch 20/69\n",
      "4992/4992 [==============================] - 39s 8ms/step - loss: 0.5897 - accuracy: 0.6898\n",
      "Epoch 00020: early stopping\n",
      "  89/1248 [=>............................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248/1248 [==============================] - 1s 1ms/step\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6019  \u001b[0m | \u001b[0m 2.766   \u001b[0m | \u001b[0m 20.25   \u001b[0m | \u001b[0m 0.196   \u001b[0m | \u001b[0m 0.01357 \u001b[0m | \u001b[0m 69.04   \u001b[0m | \u001b[0m 1.777   \u001b[0m | \u001b[0m 1.543   \u001b[0m | \u001b[0m 0.8289  \u001b[0m | \u001b[0m 77.78   \u001b[0m | \u001b[0m 0.2809  \u001b[0m | \u001b[0m 2.171   \u001b[0m |\n",
      "Epoch 1/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5925 - accuracy: 0.6797\n",
      "Epoch 2/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5772 - accuracy: 0.6946\n",
      "Epoch 3/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5756 - accuracy: 0.6953\n",
      "Epoch 4/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5752 - accuracy: 0.6956\n",
      "Epoch 5/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5706 - accuracy: 0.6988\n",
      "Epoch 6/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5759 - accuracy: 0.6956\n",
      "Epoch 7/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5728 - accuracy: 0.6970\n",
      "Epoch 8/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5729 - accuracy: 0.6978\n",
      "Epoch 9/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5720 - accuracy: 0.6976\n",
      "Epoch 10/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5712 - accuracy: 0.6979\n",
      "Epoch 11/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5738 - accuracy: 0.6960\n",
      "Epoch 12/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5719 - accuracy: 0.6996\n",
      "Epoch 13/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5721 - accuracy: 0.6975\n",
      "Epoch 14/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5713 - accuracy: 0.6973\n",
      "Epoch 15/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5703 - accuracy: 0.7007\n",
      "Epoch 16/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5728 - accuracy: 0.6981\n",
      "Epoch 17/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5686 - accuracy: 0.6999\n",
      "Epoch 18/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6960\n",
      "Epoch 19/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5716 - accuracy: 0.6985\n",
      "Epoch 20/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5731 - accuracy: 0.6976\n",
      "Epoch 21/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5713 - accuracy: 0.6981\n",
      "Epoch 22/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5736 - accuracy: 0.6967\n",
      "Epoch 23/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5737 - accuracy: 0.6957\n",
      "Epoch 24/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5729 - accuracy: 0.6983\n",
      "Epoch 25/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6967\n",
      "Epoch 26/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5718 - accuracy: 0.6958\n",
      "Epoch 27/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5710 - accuracy: 0.6989\n",
      "Epoch 28/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5708 - accuracy: 0.6991\n",
      "Epoch 29/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5722 - accuracy: 0.6980\n",
      "Epoch 30/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5718 - accuracy: 0.6968\n",
      "Epoch 31/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5729 - accuracy: 0.6987\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.16312919340527587.\n",
      "Epoch 32/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5681 - accuracy: 0.7015\n",
      "Epoch 33/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5703 - accuracy: 0.6982\n",
      "Epoch 34/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5690 - accuracy: 0.6998\n",
      "Epoch 35/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5682 - accuracy: 0.7002\n",
      "Epoch 36/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5683 - accuracy: 0.7012\n",
      "Epoch 37/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5650 - accuracy: 0.7048\n",
      "Epoch 38/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5666 - accuracy: 0.7002\n",
      "Epoch 39/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5672 - accuracy: 0.7013\n",
      "Epoch 40/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5671 - accuracy: 0.7020\n",
      "Epoch 41/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5661 - accuracy: 0.7019\n",
      "Epoch 42/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5685 - accuracy: 0.6988\n",
      "Epoch 43/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5699 - accuracy: 0.6994\n",
      "Epoch 44/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5676 - accuracy: 0.7001\n",
      "Epoch 45/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5655 - accuracy: 0.7031\n",
      "Epoch 46/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5685 - accuracy: 0.7011\n",
      "Epoch 47/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5680 - accuracy: 0.6995\n",
      "Epoch 48/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5685 - accuracy: 0.6987\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.032625838681055175.\n",
      "Epoch 49/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5667 - accuracy: 0.7029\n",
      "Epoch 50/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5684 - accuracy: 0.7002\n",
      "Epoch 51/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5690 - accuracy: 0.6997\n",
      "Epoch 52/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5664 - accuracy: 0.7017\n",
      "Epoch 53/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5675 - accuracy: 0.7007\n",
      "Epoch 54/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5670 - accuracy: 0.6991\n",
      "Epoch 55/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5673 - accuracy: 0.7003\n",
      "Epoch 56/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5657 - accuracy: 0.7011\n",
      "Epoch 57/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5650 - accuracy: 0.7037\n",
      "Epoch 58/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5682 - accuracy: 0.7007\n",
      "Epoch 59/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5663 - accuracy: 0.7023\n",
      "Epoch 60/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5675 - accuracy: 0.7008\n",
      "Epoch 61/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5681 - accuracy: 0.7000\n",
      "Epoch 62/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5657 - accuracy: 0.7024\n",
      "Epoch 63/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5669 - accuracy: 0.7007\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.006525167736211035.\n",
      "Epoch 64/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5677 - accuracy: 0.6993\n",
      "Epoch 65/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5648 - accuracy: 0.7007\n",
      "Epoch 66/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5668 - accuracy: 0.7021\n",
      "Epoch 67/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5673 - accuracy: 0.7003\n",
      "Epoch 68/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5682 - accuracy: 0.6999\n",
      "Epoch 69/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5647 - accuracy: 0.7037\n",
      "Epoch 70/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5642 - accuracy: 0.7046\n",
      "Epoch 71/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5687 - accuracy: 0.6982\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.001305033547242207.\n",
      "Epoch 72/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5682 - accuracy: 0.6988\n",
      "Epoch 73/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5652 - accuracy: 0.7031\n",
      "Epoch 74/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5696 - accuracy: 0.7000\n",
      "Epoch 00074: early stopping\n",
      " 89/454 [====>.........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454/454 [==============================] - 1s 1ms/step\n",
      "Epoch 1/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.6991 - accuracy: 0.5348\n",
      "Epoch 2/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6476 - accuracy: 0.6242\n",
      "Epoch 3/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6289 - accuracy: 0.6466\n",
      "Epoch 4/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6192 - accuracy: 0.6571\n",
      "Epoch 5/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6117 - accuracy: 0.6650\n",
      "Epoch 6/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6086 - accuracy: 0.6666\n",
      "Epoch 7/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6040 - accuracy: 0.6724\n",
      "Epoch 8/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5998 - accuracy: 0.6746\n",
      "Epoch 9/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5967 - accuracy: 0.6803\n",
      "Epoch 10/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5943 - accuracy: 0.6832\n",
      "Epoch 11/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5928 - accuracy: 0.6831\n",
      "Epoch 12/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5913 - accuracy: 0.6826\n",
      "Epoch 13/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5890 - accuracy: 0.6847\n",
      "Epoch 14/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5886 - accuracy: 0.6849\n",
      "Epoch 15/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5886 - accuracy: 0.6852\n",
      "Epoch 16/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5854 - accuracy: 0.6890\n",
      "Epoch 17/123\n",
      "1816/1816 [==============================] - 10s 6ms/step - loss: 0.5864 - accuracy: 0.6867\n",
      "Epoch 18/123\n",
      "1816/1816 [==============================] - 10s 6ms/step - loss: 0.5838 - accuracy: 0.6880\n",
      "Epoch 19/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5837 - accuracy: 0.6886\n",
      "Epoch 20/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5863 - accuracy: 0.6860\n",
      "Epoch 21/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5862 - accuracy: 0.6873\n",
      "Epoch 22/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5866 - accuracy: 0.6861\n",
      "Epoch 23/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5847 - accuracy: 0.6877\n",
      "Epoch 24/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5813 - accuracy: 0.6906\n",
      "Epoch 25/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5823 - accuracy: 0.6898\n",
      "Epoch 26/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5840 - accuracy: 0.6875\n",
      "Epoch 27/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5804 - accuracy: 0.6910\n",
      "Epoch 28/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5814 - accuracy: 0.6891\n",
      "Epoch 29/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5816 - accuracy: 0.6895\n",
      "Epoch 30/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5798 - accuracy: 0.6913\n",
      "Epoch 31/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5819 - accuracy: 0.6884\n",
      "Epoch 32/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5784 - accuracy: 0.6911\n",
      "Epoch 33/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5806 - accuracy: 0.6889\n",
      "Epoch 34/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5795 - accuracy: 0.6909\n",
      "Epoch 35/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5811 - accuracy: 0.6899\n",
      "Epoch 36/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5786 - accuracy: 0.6904\n",
      "Epoch 37/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5801 - accuracy: 0.6889\n",
      "Epoch 38/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5776 - accuracy: 0.6913\n",
      "Epoch 39/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5813 - accuracy: 0.6889\n",
      "Epoch 40/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5778 - accuracy: 0.6932\n",
      "Epoch 41/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5780 - accuracy: 0.6931\n",
      "Epoch 42/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5759 - accuracy: 0.6940\n",
      "Epoch 43/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5787 - accuracy: 0.6909\n",
      "Epoch 44/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5785 - accuracy: 0.6916\n",
      "Epoch 45/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5794 - accuracy: 0.6896\n",
      "Epoch 46/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5760 - accuracy: 0.6910\n",
      "Epoch 47/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5770 - accuracy: 0.6918\n",
      "Epoch 48/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5755 - accuracy: 0.6943\n",
      "Epoch 49/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5748 - accuracy: 0.6953\n",
      "Epoch 50/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5760 - accuracy: 0.6937\n",
      "Epoch 51/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5763 - accuracy: 0.6932\n",
      "Epoch 52/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5796 - accuracy: 0.6905\n",
      "Epoch 53/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5790 - accuracy: 0.6903\n",
      "Epoch 54/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5766 - accuracy: 0.6933\n",
      "Epoch 55/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5742 - accuracy: 0.6954\n",
      "Epoch 56/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5766 - accuracy: 0.6922\n",
      "Epoch 57/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5773 - accuracy: 0.6932\n",
      "Epoch 58/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5756 - accuracy: 0.6941\n",
      "Epoch 59/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5748 - accuracy: 0.6947\n",
      "Epoch 60/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5744 - accuracy: 0.6947\n",
      "Epoch 61/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5763 - accuracy: 0.6933\n",
      "Epoch 62/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5757 - accuracy: 0.6937\n",
      "Epoch 63/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6946\n",
      "Epoch 64/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5757 - accuracy: 0.6939\n",
      "Epoch 65/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5716 - accuracy: 0.6969\n",
      "Epoch 66/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5745 - accuracy: 0.6941\n",
      "Epoch 67/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5745 - accuracy: 0.6962\n",
      "Epoch 68/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5792 - accuracy: 0.6900\n",
      "Epoch 69/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5750 - accuracy: 0.6932\n",
      "Epoch 70/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5765 - accuracy: 0.6942\n",
      "Epoch 71/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5737 - accuracy: 0.6951\n",
      "Epoch 72/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5755 - accuracy: 0.6918\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 73/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5751 - accuracy: 0.6944\n",
      "Epoch 74/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5742 - accuracy: 0.6963\n",
      "Epoch 75/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5719 - accuracy: 0.6992\n",
      "Epoch 76/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5754 - accuracy: 0.6935\n",
      "Epoch 77/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5735 - accuracy: 0.6942\n",
      "Epoch 78/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5739 - accuracy: 0.6960\n",
      "Epoch 79/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5755 - accuracy: 0.6939\n",
      "Epoch 80/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5735 - accuracy: 0.6956\n",
      "Epoch 81/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5746 - accuracy: 0.6962\n",
      "Epoch 82/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5741 - accuracy: 0.6956\n",
      "Epoch 83/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5733 - accuracy: 0.6954\n",
      "Epoch 84/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5724 - accuracy: 0.6963\n",
      "Epoch 85/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5732 - accuracy: 0.6956\n",
      "Epoch 86/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5737 - accuracy: 0.6965\n",
      "Epoch 87/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5745 - accuracy: 0.6953\n",
      "Epoch 88/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5721 - accuracy: 0.6963\n",
      "Epoch 89/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5729 - accuracy: 0.6969\n",
      "Epoch 90/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5743 - accuracy: 0.6952\n",
      "Epoch 91/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5734 - accuracy: 0.6968\n",
      "Epoch 92/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5719 - accuracy: 0.6966\n",
      "Epoch 93/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5741 - accuracy: 0.6936\n",
      "Epoch 94/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5741 - accuracy: 0.6953\n",
      "Epoch 95/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5717 - accuracy: 0.6974\n",
      "Epoch 96/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5741 - accuracy: 0.6936\n",
      "Epoch 97/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5728 - accuracy: 0.6966\n",
      "Epoch 98/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6943\n",
      "Epoch 99/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5739 - accuracy: 0.6939\n",
      "Epoch 100/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5746 - accuracy: 0.6951\n",
      "Epoch 101/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5742 - accuracy: 0.6959\n",
      "Epoch 102/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5738 - accuracy: 0.6938\n",
      "Epoch 103/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5749 - accuracy: 0.6939\n",
      "Epoch 104/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5734 - accuracy: 0.6961\n",
      "Epoch 105/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5734 - accuracy: 0.6945\n",
      "Epoch 106/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5729 - accuracy: 0.6978\n",
      "Epoch 107/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5727 - accuracy: 0.6956\n",
      "Epoch 108/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5738 - accuracy: 0.6951\n",
      "Epoch 109/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5740 - accuracy: 0.6958\n",
      "Epoch 110/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5714 - accuracy: 0.6978\n",
      "Epoch 111/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5728 - accuracy: 0.6966\n",
      "Epoch 112/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5734 - accuracy: 0.6963\n",
      "Epoch 113/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6949\n",
      "Epoch 114/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5749 - accuracy: 0.6929\n",
      "Epoch 115/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5725 - accuracy: 0.6972\n",
      "Epoch 116/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5731 - accuracy: 0.6950\n",
      "Epoch 117/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5720 - accuracy: 0.6964\n",
      "Epoch 118/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5735 - accuracy: 0.6950\n",
      "Epoch 119/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5745 - accuracy: 0.6942\n",
      "Epoch 120/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5725 - accuracy: 0.6965\n",
      "Epoch 121/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5735 - accuracy: 0.6956\n",
      "Epoch 122/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5737 - accuracy: 0.6957\n",
      "Epoch 123/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5740 - accuracy: 0.6936\n",
      " 93/454 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454/454 [==============================] - 1s 1ms/step\n",
      "Epoch 1/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.7119 - accuracy: 0.5256\n",
      "Epoch 2/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.6488 - accuracy: 0.6291\n",
      "Epoch 3/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6320 - accuracy: 0.6489\n",
      "Epoch 4/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6206 - accuracy: 0.6596\n",
      "Epoch 5/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6118 - accuracy: 0.6693\n",
      "Epoch 6/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6083 - accuracy: 0.6725\n",
      "Epoch 7/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6041 - accuracy: 0.6741\n",
      "Epoch 8/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6025 - accuracy: 0.6744\n",
      "Epoch 9/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5996 - accuracy: 0.6761\n",
      "Epoch 10/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5983 - accuracy: 0.6757\n",
      "Epoch 11/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5966 - accuracy: 0.6772\n",
      "Epoch 12/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5958 - accuracy: 0.6780\n",
      "Epoch 13/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5937 - accuracy: 0.6811\n",
      "Epoch 14/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5928 - accuracy: 0.6829\n",
      "Epoch 15/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5928 - accuracy: 0.6819\n",
      "Epoch 16/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5922 - accuracy: 0.6818\n",
      "Epoch 17/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5927 - accuracy: 0.6824\n",
      "Epoch 18/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5913 - accuracy: 0.6816\n",
      "Epoch 19/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5905 - accuracy: 0.6836\n",
      "Epoch 20/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5885 - accuracy: 0.6832\n",
      "Epoch 21/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5879 - accuracy: 0.6846\n",
      "Epoch 22/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5889 - accuracy: 0.6842\n",
      "Epoch 23/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5856 - accuracy: 0.6852\n",
      "Epoch 24/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5876 - accuracy: 0.6854\n",
      "Epoch 25/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5885 - accuracy: 0.6850\n",
      "Epoch 26/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5861 - accuracy: 0.6864\n",
      "Epoch 27/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5861 - accuracy: 0.6863\n",
      "Epoch 28/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5864 - accuracy: 0.6867\n",
      "Epoch 29/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5851 - accuracy: 0.6843\n",
      "Epoch 30/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5850 - accuracy: 0.6883\n",
      "Epoch 31/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5835 - accuracy: 0.6888\n",
      "Epoch 32/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5842 - accuracy: 0.6870\n",
      "Epoch 33/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5831 - accuracy: 0.6886\n",
      "Epoch 34/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5800 - accuracy: 0.6914\n",
      "Epoch 35/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5835 - accuracy: 0.6896\n",
      "Epoch 36/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5825 - accuracy: 0.6887\n",
      "Epoch 37/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5827 - accuracy: 0.6885\n",
      "Epoch 38/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5826 - accuracy: 0.6881\n",
      "Epoch 39/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5794 - accuracy: 0.6918\n",
      "Epoch 40/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5829 - accuracy: 0.6891\n",
      "Epoch 41/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5820 - accuracy: 0.6906\n",
      "Epoch 42/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5807 - accuracy: 0.6912\n",
      "Epoch 43/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5817 - accuracy: 0.6894\n",
      "Epoch 44/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5810 - accuracy: 0.6907\n",
      "Epoch 45/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5799 - accuracy: 0.6910\n",
      "Epoch 46/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5819 - accuracy: 0.6885\n",
      "Epoch 47/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5795 - accuracy: 0.6924\n",
      "Epoch 48/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5813 - accuracy: 0.6901\n",
      "Epoch 49/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5796 - accuracy: 0.6914\n",
      "Epoch 50/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5805 - accuracy: 0.6914\n",
      "Epoch 51/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5808 - accuracy: 0.6908\n",
      "Epoch 52/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5796 - accuracy: 0.6919\n",
      "Epoch 53/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5814 - accuracy: 0.6920\n",
      "Epoch 54/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5794 - accuracy: 0.6918\n",
      "Epoch 55/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5811 - accuracy: 0.6888\n",
      "Epoch 56/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5788 - accuracy: 0.6919\n",
      "Epoch 57/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5819 - accuracy: 0.6890\n",
      "Epoch 58/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5794 - accuracy: 0.6897\n",
      "Epoch 59/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5786 - accuracy: 0.6934\n",
      "Epoch 60/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5787 - accuracy: 0.6914\n",
      "Epoch 61/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5751 - accuracy: 0.6958\n",
      "Epoch 62/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5782 - accuracy: 0.6928\n",
      "Epoch 63/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5763 - accuracy: 0.6936\n",
      "Epoch 64/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5794 - accuracy: 0.6909\n",
      "Epoch 65/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5776 - accuracy: 0.6921\n",
      "Epoch 66/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5773 - accuracy: 0.6900\n",
      "Epoch 67/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5765 - accuracy: 0.6941\n",
      "Epoch 68/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5786 - accuracy: 0.6913\n",
      "Epoch 69/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5798 - accuracy: 0.6903\n",
      "Epoch 70/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5767 - accuracy: 0.6938\n",
      "Epoch 71/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5767 - accuracy: 0.6936\n",
      "Epoch 72/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5776 - accuracy: 0.6926\n",
      "Epoch 73/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5756 - accuracy: 0.6946\n",
      "Epoch 74/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5750 - accuracy: 0.6945\n",
      "Epoch 75/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5771 - accuracy: 0.6906\n",
      "Epoch 76/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5770 - accuracy: 0.6915\n",
      "Epoch 77/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5729 - accuracy: 0.6961\n",
      "Epoch 78/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5758 - accuracy: 0.6932\n",
      "Epoch 79/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5774 - accuracy: 0.6922\n",
      "Epoch 80/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5754 - accuracy: 0.6956\n",
      "Epoch 81/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5753 - accuracy: 0.6945\n",
      "Epoch 82/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5771 - accuracy: 0.6908\n",
      "Epoch 83/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5759 - accuracy: 0.6937\n",
      "Epoch 84/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5759 - accuracy: 0.6941\n",
      "Epoch 85/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5771 - accuracy: 0.6920\n",
      "Epoch 86/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5770 - accuracy: 0.6926\n",
      "Epoch 87/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5755 - accuracy: 0.6941\n",
      "Epoch 88/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5761 - accuracy: 0.6942\n",
      "Epoch 89/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5758 - accuracy: 0.6935\n",
      "Epoch 90/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5738 - accuracy: 0.6943\n",
      "Epoch 91/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5762 - accuracy: 0.6935\n",
      "Epoch 92/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5759 - accuracy: 0.6938\n",
      "Epoch 93/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5741 - accuracy: 0.6951\n",
      "Epoch 94/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5739 - accuracy: 0.6970\n",
      "Epoch 95/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5757 - accuracy: 0.6933\n",
      "Epoch 96/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5761 - accuracy: 0.6921\n",
      "Epoch 97/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5748 - accuracy: 0.6951\n",
      "Epoch 98/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5774 - accuracy: 0.6904\n",
      "Epoch 99/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5758 - accuracy: 0.6935\n",
      "Epoch 100/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5768 - accuracy: 0.6924\n",
      "Epoch 101/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5756 - accuracy: 0.6935\n",
      "Epoch 102/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5757 - accuracy: 0.6927\n",
      "Epoch 103/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5754 - accuracy: 0.6929\n",
      "Epoch 104/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5725 - accuracy: 0.6969\n",
      "Epoch 105/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5750 - accuracy: 0.6950\n",
      "Epoch 106/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5753 - accuracy: 0.6950\n",
      "Epoch 107/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5771 - accuracy: 0.6919\n",
      "Epoch 108/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5736 - accuracy: 0.6946\n",
      "Epoch 109/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5757 - accuracy: 0.6953\n",
      "Epoch 110/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5737 - accuracy: 0.6972\n",
      "Epoch 111/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5750 - accuracy: 0.6925\n",
      "Epoch 112/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5766 - accuracy: 0.6927\n",
      "Epoch 113/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5755 - accuracy: 0.6945\n",
      "Epoch 114/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5733 - accuracy: 0.6960\n",
      "Epoch 115/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5729 - accuracy: 0.6978\n",
      "Epoch 116/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5760 - accuracy: 0.6930\n",
      "Epoch 117/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5742 - accuracy: 0.6943\n",
      "Epoch 118/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5740 - accuracy: 0.6937\n",
      "Epoch 119/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5762 - accuracy: 0.6921\n",
      "Epoch 120/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5758 - accuracy: 0.6939\n",
      "Epoch 121/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5746 - accuracy: 0.6920\n",
      "Epoch 122/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5776 - accuracy: 0.6915\n",
      "Epoch 123/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5752 - accuracy: 0.6942\n",
      " 90/454 [====>.........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454/454 [==============================] - 1s 1ms/step\n",
      "Epoch 1/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.6893 - accuracy: 0.5668\n",
      "Epoch 2/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6427 - accuracy: 0.6433\n",
      "Epoch 3/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.6294 - accuracy: 0.6551\n",
      "Epoch 4/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.6205 - accuracy: 0.6626\n",
      "Epoch 5/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6122 - accuracy: 0.6722\n",
      "Epoch 6/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6091 - accuracy: 0.6722\n",
      "Epoch 7/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6038 - accuracy: 0.6788\n",
      "Epoch 8/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6010 - accuracy: 0.6780\n",
      "Epoch 9/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6015 - accuracy: 0.6788\n",
      "Epoch 10/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5955 - accuracy: 0.6804\n",
      "Epoch 11/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5955 - accuracy: 0.6803\n",
      "Epoch 12/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5947 - accuracy: 0.6812\n",
      "Epoch 13/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5935 - accuracy: 0.6833\n",
      "Epoch 14/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5919 - accuracy: 0.6845\n",
      "Epoch 15/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5896 - accuracy: 0.6862\n",
      "Epoch 16/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5906 - accuracy: 0.6834\n",
      "Epoch 17/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5877 - accuracy: 0.6859\n",
      "Epoch 18/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5883 - accuracy: 0.6867\n",
      "Epoch 19/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5879 - accuracy: 0.6861\n",
      "Epoch 20/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5887 - accuracy: 0.6850\n",
      "Epoch 21/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5852 - accuracy: 0.6890\n",
      "Epoch 22/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5861 - accuracy: 0.6881\n",
      "Epoch 23/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5871 - accuracy: 0.6871\n",
      "Epoch 24/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5848 - accuracy: 0.6894\n",
      "Epoch 25/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5861 - accuracy: 0.6886\n",
      "Epoch 26/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5836 - accuracy: 0.6905\n",
      "Epoch 27/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5823 - accuracy: 0.6909\n",
      "Epoch 28/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5824 - accuracy: 0.6895\n",
      "Epoch 29/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5838 - accuracy: 0.6869\n",
      "Epoch 30/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5829 - accuracy: 0.6903\n",
      "Epoch 31/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5842 - accuracy: 0.6872\n",
      "Epoch 32/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5813 - accuracy: 0.6910\n",
      "Epoch 33/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5821 - accuracy: 0.6896\n",
      "Epoch 34/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5789 - accuracy: 0.6938\n",
      "Epoch 35/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5803 - accuracy: 0.6917\n",
      "Epoch 36/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5811 - accuracy: 0.6903\n",
      "Epoch 37/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5808 - accuracy: 0.6917\n",
      "Epoch 38/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5802 - accuracy: 0.6905\n",
      "Epoch 39/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5797 - accuracy: 0.6918\n",
      "Epoch 40/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5792 - accuracy: 0.6914\n",
      "Epoch 41/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5813 - accuracy: 0.6892\n",
      "Epoch 42/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5788 - accuracy: 0.6929\n",
      "Epoch 43/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5807 - accuracy: 0.6898\n",
      "Epoch 44/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5788 - accuracy: 0.6932\n",
      "Epoch 45/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5769 - accuracy: 0.6934\n",
      "Epoch 46/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5797 - accuracy: 0.6940\n",
      "Epoch 47/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5794 - accuracy: 0.6904\n",
      "Epoch 48/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5770 - accuracy: 0.6923\n",
      "Epoch 49/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5794 - accuracy: 0.6925\n",
      "Epoch 50/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5805 - accuracy: 0.6925\n",
      "Epoch 51/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5806 - accuracy: 0.6911\n",
      "Epoch 52/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5778 - accuracy: 0.6918\n",
      "Epoch 53/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5781 - accuracy: 0.6941\n",
      "Epoch 54/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5788 - accuracy: 0.6933\n",
      "Epoch 55/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5765 - accuracy: 0.6917\n",
      "Epoch 56/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5755 - accuracy: 0.6945\n",
      "Epoch 57/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5756 - accuracy: 0.6953\n",
      "Epoch 58/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5763 - accuracy: 0.6951\n",
      "Epoch 59/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5792 - accuracy: 0.6899\n",
      "Epoch 60/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5797 - accuracy: 0.6908\n",
      "Epoch 61/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5770 - accuracy: 0.6919\n",
      "Epoch 62/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5756 - accuracy: 0.6960\n",
      "Epoch 63/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5759 - accuracy: 0.6944\n",
      "Epoch 64/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5776 - accuracy: 0.6932\n",
      "Epoch 65/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5762 - accuracy: 0.6936\n",
      "Epoch 66/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5766 - accuracy: 0.6925\n",
      "Epoch 67/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5764 - accuracy: 0.6931\n",
      "Epoch 68/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5763 - accuracy: 0.6941\n",
      "Epoch 69/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5765 - accuracy: 0.6944\n",
      "Epoch 70/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5762 - accuracy: 0.6926\n",
      "Epoch 71/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5764 - accuracy: 0.6936\n",
      "Epoch 72/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5757 - accuracy: 0.6919\n",
      "Epoch 73/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5756 - accuracy: 0.6946\n",
      "Epoch 74/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5752 - accuracy: 0.6952\n",
      "Epoch 75/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5753 - accuracy: 0.6929\n",
      "Epoch 76/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6937\n",
      "Epoch 77/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5748 - accuracy: 0.6951\n",
      "Epoch 78/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5724 - accuracy: 0.6959\n",
      "Epoch 79/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5782 - accuracy: 0.6901\n",
      "Epoch 80/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5751 - accuracy: 0.6965\n",
      "Epoch 81/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5754 - accuracy: 0.6923\n",
      "Epoch 82/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5761 - accuracy: 0.6944\n",
      "Epoch 83/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5753 - accuracy: 0.6947\n",
      "Epoch 84/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5736 - accuracy: 0.6950\n",
      "Epoch 85/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5727 - accuracy: 0.6958\n",
      "Epoch 86/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5708 - accuracy: 0.6978\n",
      "Epoch 87/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5729 - accuracy: 0.6950\n",
      "Epoch 88/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5752 - accuracy: 0.6941\n",
      "Epoch 89/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5742 - accuracy: 0.6956\n",
      "Epoch 90/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6947\n",
      "Epoch 91/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5749 - accuracy: 0.6947\n",
      "Epoch 92/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5753 - accuracy: 0.6940\n",
      "Epoch 93/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5766 - accuracy: 0.6933\n",
      "Epoch 94/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5748 - accuracy: 0.6936\n",
      "Epoch 95/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5726 - accuracy: 0.6954\n",
      "Epoch 96/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5736 - accuracy: 0.6945\n",
      "Epoch 97/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5757 - accuracy: 0.6947\n",
      "Epoch 98/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5724 - accuracy: 0.6957\n",
      "Epoch 99/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5734 - accuracy: 0.6959\n",
      "Epoch 100/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5746 - accuracy: 0.6944\n",
      "Epoch 101/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5732 - accuracy: 0.6974\n",
      "Epoch 102/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6950\n",
      "Epoch 00102: early stopping\n",
      "100/454 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454/454 [==============================] - 1s 1ms/step\n",
      "Epoch 1/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.7478 - accuracy: 0.4889\n",
      "Epoch 2/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6562 - accuracy: 0.6240\n",
      "Epoch 3/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6367 - accuracy: 0.6433\n",
      "Epoch 4/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6262 - accuracy: 0.6528\n",
      "Epoch 5/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6185 - accuracy: 0.6591\n",
      "Epoch 6/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.6132 - accuracy: 0.6668\n",
      "Epoch 7/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6072 - accuracy: 0.6719\n",
      "Epoch 8/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6033 - accuracy: 0.6749\n",
      "Epoch 9/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.6005 - accuracy: 0.6754\n",
      "Epoch 10/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5997 - accuracy: 0.6769\n",
      "Epoch 11/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5968 - accuracy: 0.6819\n",
      "Epoch 12/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5930 - accuracy: 0.6845\n",
      "Epoch 13/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5924 - accuracy: 0.6843\n",
      "Epoch 14/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5913 - accuracy: 0.6847\n",
      "Epoch 15/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5907 - accuracy: 0.6854\n",
      "Epoch 16/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5897 - accuracy: 0.6830\n",
      "Epoch 17/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5884 - accuracy: 0.6868\n",
      "Epoch 18/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5895 - accuracy: 0.6858\n",
      "Epoch 19/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5881 - accuracy: 0.6865\n",
      "Epoch 20/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5878 - accuracy: 0.6856\n",
      "Epoch 21/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5873 - accuracy: 0.6864\n",
      "Epoch 22/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5877 - accuracy: 0.6849\n",
      "Epoch 23/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5884 - accuracy: 0.6847\n",
      "Epoch 24/123\n",
      "1816/1816 [==============================] - 10s 6ms/step - loss: 0.5849 - accuracy: 0.6888\n",
      "Epoch 25/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5868 - accuracy: 0.6867\n",
      "Epoch 26/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5858 - accuracy: 0.6870\n",
      "Epoch 27/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5855 - accuracy: 0.6881\n",
      "Epoch 28/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5839 - accuracy: 0.6884\n",
      "Epoch 29/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5840 - accuracy: 0.6894\n",
      "Epoch 30/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5833 - accuracy: 0.6886\n",
      "Epoch 31/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5840 - accuracy: 0.6883\n",
      "Epoch 32/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5830 - accuracy: 0.6885\n",
      "Epoch 33/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5828 - accuracy: 0.6885\n",
      "Epoch 34/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5806 - accuracy: 0.6909\n",
      "Epoch 35/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5804 - accuracy: 0.6923\n",
      "Epoch 36/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5805 - accuracy: 0.6914\n",
      "Epoch 37/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5809 - accuracy: 0.6904\n",
      "Epoch 38/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5819 - accuracy: 0.6907\n",
      "Epoch 39/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5817 - accuracy: 0.6919\n",
      "Epoch 40/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5792 - accuracy: 0.6926\n",
      "Epoch 41/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5788 - accuracy: 0.6920\n",
      "Epoch 42/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5781 - accuracy: 0.6939\n",
      "Epoch 43/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5801 - accuracy: 0.6921\n",
      "Epoch 44/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5814 - accuracy: 0.6899\n",
      "Epoch 45/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5808 - accuracy: 0.6908\n",
      "Epoch 46/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5793 - accuracy: 0.6898\n",
      "Epoch 47/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5806 - accuracy: 0.6909\n",
      "Epoch 48/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5792 - accuracy: 0.6939\n",
      "Epoch 49/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5809 - accuracy: 0.6891\n",
      "Epoch 50/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5783 - accuracy: 0.6924\n",
      "Epoch 51/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5791 - accuracy: 0.6935\n",
      "Epoch 52/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5764 - accuracy: 0.6941\n",
      "Epoch 53/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5759 - accuracy: 0.6941\n",
      "Epoch 54/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5792 - accuracy: 0.6915\n",
      "Epoch 55/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5792 - accuracy: 0.6922\n",
      "Epoch 56/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5769 - accuracy: 0.6943\n",
      "Epoch 57/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5780 - accuracy: 0.6932\n",
      "Epoch 58/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5791 - accuracy: 0.6926\n",
      "Epoch 59/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5782 - accuracy: 0.6915\n",
      "Epoch 60/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5774 - accuracy: 0.6932\n",
      "Epoch 61/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5750 - accuracy: 0.6976\n",
      "Epoch 62/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5766 - accuracy: 0.6952\n",
      "Epoch 63/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5775 - accuracy: 0.6937\n",
      "Epoch 64/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5756 - accuracy: 0.6964\n",
      "Epoch 65/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5777 - accuracy: 0.6923\n",
      "Epoch 66/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5769 - accuracy: 0.6911\n",
      "Epoch 67/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5773 - accuracy: 0.6928\n",
      "Epoch 68/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5765 - accuracy: 0.6916\n",
      "Epoch 69/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5800 - accuracy: 0.6914\n",
      "Epoch 70/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5745 - accuracy: 0.6958\n",
      "Epoch 71/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5768 - accuracy: 0.6928\n",
      "Epoch 72/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5744 - accuracy: 0.6946\n",
      "Epoch 73/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5754 - accuracy: 0.6950\n",
      "Epoch 74/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5756 - accuracy: 0.6948\n",
      "Epoch 75/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5764 - accuracy: 0.6934\n",
      "Epoch 76/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5773 - accuracy: 0.6959\n",
      "Epoch 77/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5773 - accuracy: 0.6923\n",
      "Epoch 78/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5760 - accuracy: 0.6936\n",
      "Epoch 79/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5762 - accuracy: 0.6949\n",
      "Epoch 80/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5724 - accuracy: 0.6960\n",
      "Epoch 81/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5750 - accuracy: 0.6944\n",
      "Epoch 82/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5761 - accuracy: 0.6929\n",
      "Epoch 83/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5753 - accuracy: 0.6945\n",
      "Epoch 84/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5749 - accuracy: 0.6946\n",
      "Epoch 85/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5762 - accuracy: 0.6937\n",
      "Epoch 86/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5749 - accuracy: 0.6945\n",
      "Epoch 87/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5763 - accuracy: 0.6931\n",
      "Epoch 88/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5750 - accuracy: 0.6941\n",
      "Epoch 89/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5747 - accuracy: 0.6956\n",
      "Epoch 90/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5733 - accuracy: 0.6948\n",
      "Epoch 91/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5756 - accuracy: 0.6935\n",
      "Epoch 92/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5748 - accuracy: 0.6953\n",
      "Epoch 93/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5728 - accuracy: 0.6963\n",
      "Epoch 94/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5757 - accuracy: 0.6931\n",
      "Epoch 95/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5716 - accuracy: 0.6969\n",
      "Epoch 96/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5742 - accuracy: 0.6958\n",
      "Epoch 97/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5750 - accuracy: 0.6941\n",
      "Epoch 98/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5734 - accuracy: 0.6951\n",
      "Epoch 99/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5752 - accuracy: 0.6935\n",
      "Epoch 100/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6959\n",
      "Epoch 101/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6963\n",
      "Epoch 102/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5745 - accuracy: 0.6961\n",
      "Epoch 103/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5730 - accuracy: 0.6962\n",
      "Epoch 104/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5722 - accuracy: 0.6993\n",
      "Epoch 105/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5734 - accuracy: 0.6956\n",
      "Epoch 106/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5753 - accuracy: 0.6909\n",
      "Epoch 107/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5736 - accuracy: 0.6954\n",
      "Epoch 108/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5730 - accuracy: 0.6953\n",
      "Epoch 109/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5701 - accuracy: 0.6991\n",
      "Epoch 110/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5716 - accuracy: 0.6975\n",
      "Epoch 111/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5728 - accuracy: 0.6962\n",
      "Epoch 112/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5726 - accuracy: 0.6981\n",
      "Epoch 113/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5717 - accuracy: 0.6958\n",
      "Epoch 114/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5714 - accuracy: 0.6981\n",
      "Epoch 115/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5747 - accuracy: 0.6943\n",
      "Epoch 116/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5728 - accuracy: 0.6969\n",
      "Epoch 117/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5741 - accuracy: 0.6943\n",
      "Epoch 118/123\n",
      "1816/1816 [==============================] - 10s 5ms/step - loss: 0.5698 - accuracy: 0.7015\n",
      "Epoch 119/123\n",
      "1816/1816 [==============================] - 9s 5ms/step - loss: 0.5723 - accuracy: 0.6957\n",
      "Epoch 00119: early stopping\n",
      " 89/454 [====>.........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454/454 [==============================] - 1s 1ms/step\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.6971  \u001b[0m | \u001b[95m 0.4228  \u001b[0m | \u001b[95m 54.51   \u001b[0m | \u001b[95m 0.07455 \u001b[0m | \u001b[95m 0.2961  \u001b[0m | \u001b[95m 122.7   \u001b[0m | \u001b[95m 1.397   \u001b[0m | \u001b[95m 1.011   \u001b[0m | \u001b[95m 0.8156  \u001b[0m | \u001b[95m 144.3   \u001b[0m | \u001b[95m 0.729   \u001b[0m | \u001b[95m 3.085   \u001b[0m |\n",
      "Epoch 1/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6004 - accuracy: 0.6730\n",
      "Epoch 2/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5777 - accuracy: 0.6933\n",
      "Epoch 3/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5764 - accuracy: 0.6941\n",
      "Epoch 4/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5767 - accuracy: 0.6925\n",
      "Epoch 5/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5763 - accuracy: 0.6939\n",
      "Epoch 6/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5738 - accuracy: 0.6965\n",
      "Epoch 7/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5744 - accuracy: 0.6966\n",
      "Epoch 8/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5748 - accuracy: 0.6943\n",
      "Epoch 9/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5718 - accuracy: 0.6970\n",
      "Epoch 10/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5729 - accuracy: 0.6971\n",
      "Epoch 11/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5734 - accuracy: 0.6969\n",
      "Epoch 12/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5752 - accuracy: 0.6951\n",
      "Epoch 13/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5749 - accuracy: 0.6954\n",
      "Epoch 14/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5748 - accuracy: 0.6960\n",
      "Epoch 15/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5719 - accuracy: 0.6988\n",
      "Epoch 16/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5731 - accuracy: 0.6978\n",
      "Epoch 17/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5730 - accuracy: 0.6982\n",
      "Epoch 18/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5752 - accuracy: 0.6954\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.06233426787878931.\n",
      "Epoch 19/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5720 - accuracy: 0.6956\n",
      "Epoch 20/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5723 - accuracy: 0.6960\n",
      "Epoch 21/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5714 - accuracy: 0.6985\n",
      "Epoch 22/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5715 - accuracy: 0.6984\n",
      "Epoch 23/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5697 - accuracy: 0.6995\n",
      "Epoch 24/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5686 - accuracy: 0.7005\n",
      "Epoch 25/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5686 - accuracy: 0.7002\n",
      "Epoch 26/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5697 - accuracy: 0.6995\n",
      "Epoch 27/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5699 - accuracy: 0.7002\n",
      "Epoch 28/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5715 - accuracy: 0.6961\n",
      "Epoch 29/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5708 - accuracy: 0.6996\n",
      "Epoch 30/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5677 - accuracy: 0.6995\n",
      "Epoch 31/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5710 - accuracy: 0.6992\n",
      "Epoch 32/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5725 - accuracy: 0.6985\n",
      "Epoch 33/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5709 - accuracy: 0.6973\n",
      "Epoch 34/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5680 - accuracy: 0.7014\n",
      "Epoch 35/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5731 - accuracy: 0.6967\n",
      "Epoch 36/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5727 - accuracy: 0.6963\n",
      "Epoch 37/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5720 - accuracy: 0.6982\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.012466853575757863.\n",
      "Epoch 38/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5739 - accuracy: 0.6961\n",
      "Epoch 39/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5684 - accuracy: 0.6995\n",
      "Epoch 40/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5688 - accuracy: 0.6986\n",
      "Epoch 41/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5704 - accuracy: 0.7005\n",
      "Epoch 42/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5688 - accuracy: 0.7015\n",
      "Epoch 43/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5710 - accuracy: 0.6991\n",
      "Epoch 44/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5714 - accuracy: 0.6967\n",
      "Epoch 45/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5716 - accuracy: 0.6975\n",
      "Epoch 46/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5689 - accuracy: 0.7007\n",
      "Epoch 47/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5711 - accuracy: 0.6988\n",
      "Epoch 48/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5711 - accuracy: 0.6986\n",
      "Epoch 49/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5700 - accuracy: 0.6985\n",
      "Epoch 50/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5706 - accuracy: 0.6968\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.002493370715151573.\n",
      "Epoch 51/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5719 - accuracy: 0.6996\n",
      "Epoch 52/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5724 - accuracy: 0.6966\n",
      "Epoch 53/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5702 - accuracy: 0.6989\n",
      "Epoch 54/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5724 - accuracy: 0.6958\n",
      "Epoch 55/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5691 - accuracy: 0.7004\n",
      "Epoch 56/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5704 - accuracy: 0.6958\n",
      "Epoch 57/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5704 - accuracy: 0.6986\n",
      "Epoch 58/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5711 - accuracy: 0.7002\n",
      "Epoch 59/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5690 - accuracy: 0.7011\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 60/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5717 - accuracy: 0.6997\n",
      "Epoch 61/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5705 - accuracy: 0.6992\n",
      "Epoch 62/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5702 - accuracy: 0.6997\n",
      "Epoch 00062: early stopping\n",
      " 86/757 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757/757 [==============================] - 1s 1ms/step\n",
      "Epoch 1/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6824 - accuracy: 0.5752\n",
      "Epoch 2/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6633 - accuracy: 0.6157\n",
      "Epoch 3/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6504 - accuracy: 0.6295\n",
      "Epoch 4/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6432 - accuracy: 0.6386\n",
      "Epoch 5/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6361 - accuracy: 0.6456\n",
      "Epoch 6/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6306 - accuracy: 0.6511\n",
      "Epoch 7/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6231 - accuracy: 0.6572\n",
      "Epoch 8/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6216 - accuracy: 0.6588\n",
      "Epoch 9/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6171 - accuracy: 0.6616\n",
      "Epoch 10/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6138 - accuracy: 0.6664\n",
      "Epoch 11/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6123 - accuracy: 0.6646\n",
      "Epoch 12/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6093 - accuracy: 0.6673\n",
      "Epoch 13/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6067 - accuracy: 0.6713\n",
      "Epoch 14/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6025 - accuracy: 0.6765\n",
      "Epoch 15/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6039 - accuracy: 0.6731\n",
      "Epoch 16/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6003 - accuracy: 0.6766\n",
      "Epoch 17/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5983 - accuracy: 0.6773\n",
      "Epoch 18/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5977 - accuracy: 0.6786\n",
      "Epoch 19/105\n",
      "3026/3026 [==============================] - 18s 6ms/step - loss: 0.5955 - accuracy: 0.6801\n",
      "Epoch 20/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5950 - accuracy: 0.6793\n",
      "Epoch 21/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5966 - accuracy: 0.6798\n",
      "Epoch 22/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5941 - accuracy: 0.6791\n",
      "Epoch 23/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5931 - accuracy: 0.6806\n",
      "Epoch 24/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5900 - accuracy: 0.6845\n",
      "Epoch 25/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5917 - accuracy: 0.6819\n",
      "Epoch 26/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5914 - accuracy: 0.6824\n",
      "Epoch 27/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5923 - accuracy: 0.6827\n",
      "Epoch 28/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5910 - accuracy: 0.6835\n",
      "Epoch 29/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5928 - accuracy: 0.6825\n",
      "Epoch 30/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5920 - accuracy: 0.6815\n",
      "Epoch 31/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5891 - accuracy: 0.6852\n",
      "Epoch 32/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5914 - accuracy: 0.6825\n",
      "Epoch 33/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5907 - accuracy: 0.6813\n",
      "Epoch 34/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5877 - accuracy: 0.6865\n",
      "Epoch 35/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5883 - accuracy: 0.6875\n",
      "Epoch 36/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5917 - accuracy: 0.6819\n",
      "Epoch 37/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5898 - accuracy: 0.6845\n",
      "Epoch 38/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5879 - accuracy: 0.6863\n",
      "Epoch 39/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5859 - accuracy: 0.6867\n",
      "Epoch 40/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5876 - accuracy: 0.6846\n",
      "Epoch 41/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5875 - accuracy: 0.6862\n",
      "Epoch 42/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5876 - accuracy: 0.6857\n",
      "Epoch 43/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5857 - accuracy: 0.6868\n",
      "Epoch 44/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5887 - accuracy: 0.6846\n",
      "Epoch 45/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5884 - accuracy: 0.6835\n",
      "Epoch 46/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5854 - accuracy: 0.6857\n",
      "Epoch 47/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5843 - accuracy: 0.6876\n",
      "Epoch 48/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5857 - accuracy: 0.6861\n",
      "Epoch 49/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5873 - accuracy: 0.6837\n",
      "Epoch 50/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5843 - accuracy: 0.6889\n",
      "Epoch 51/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5852 - accuracy: 0.6855\n",
      "Epoch 52/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5836 - accuracy: 0.6890\n",
      "Epoch 53/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5851 - accuracy: 0.6863\n",
      "Epoch 54/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5855 - accuracy: 0.6877\n",
      "Epoch 55/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5870 - accuracy: 0.6847\n",
      "Epoch 56/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5823 - accuracy: 0.6876\n",
      "Epoch 57/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5856 - accuracy: 0.6850\n",
      "Epoch 58/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5877 - accuracy: 0.6842\n",
      "Epoch 59/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5862 - accuracy: 0.6857\n",
      "Epoch 60/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5846 - accuracy: 0.6875\n",
      "Epoch 61/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5827 - accuracy: 0.6888\n",
      "Epoch 62/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5820 - accuracy: 0.6895\n",
      "Epoch 63/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5831 - accuracy: 0.6873\n",
      "Epoch 64/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5850 - accuracy: 0.6831\n",
      "Epoch 65/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5835 - accuracy: 0.6895\n",
      "Epoch 66/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5841 - accuracy: 0.6873\n",
      "Epoch 67/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5811 - accuracy: 0.6870\n",
      "Epoch 68/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5846 - accuracy: 0.6853\n",
      "Epoch 69/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5834 - accuracy: 0.6873\n",
      "Epoch 70/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5830 - accuracy: 0.6886\n",
      "Epoch 71/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5813 - accuracy: 0.6898\n",
      "Epoch 72/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5813 - accuracy: 0.6894\n",
      "Epoch 73/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5805 - accuracy: 0.6868\n",
      "Epoch 74/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5795 - accuracy: 0.6922\n",
      "Epoch 75/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5824 - accuracy: 0.6894\n",
      "Epoch 76/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5809 - accuracy: 0.6883\n",
      "Epoch 77/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5829 - accuracy: 0.6889\n",
      "Epoch 78/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5810 - accuracy: 0.6879\n",
      "Epoch 79/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5814 - accuracy: 0.6884\n",
      "Epoch 80/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5810 - accuracy: 0.6886\n",
      "Epoch 81/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5811 - accuracy: 0.6907\n",
      "Epoch 82/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5810 - accuracy: 0.6887\n",
      "Epoch 83/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5823 - accuracy: 0.6884\n",
      "Epoch 84/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5816 - accuracy: 0.6896\n",
      "Epoch 85/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5823 - accuracy: 0.6870\n",
      "Epoch 86/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5813 - accuracy: 0.6886\n",
      "Epoch 87/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5809 - accuracy: 0.6908\n",
      "Epoch 88/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5812 - accuracy: 0.6900\n",
      "Epoch 89/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5802 - accuracy: 0.6912\n",
      "Epoch 90/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5818 - accuracy: 0.6877\n",
      "Epoch 91/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5805 - accuracy: 0.6900\n",
      "Epoch 92/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5790 - accuracy: 0.6910\n",
      "Epoch 93/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5796 - accuracy: 0.6920\n",
      "Epoch 94/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5809 - accuracy: 0.6887\n",
      "Epoch 95/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5778 - accuracy: 0.6912\n",
      "Epoch 96/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5810 - accuracy: 0.6891\n",
      "Epoch 97/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5795 - accuracy: 0.6901\n",
      "Epoch 98/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5785 - accuracy: 0.6914\n",
      "Epoch 99/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5788 - accuracy: 0.6921\n",
      "Epoch 100/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5833 - accuracy: 0.6869\n",
      "Epoch 101/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5793 - accuracy: 0.6899\n",
      "Epoch 102/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5785 - accuracy: 0.6889\n",
      "Epoch 103/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5785 - accuracy: 0.6921\n",
      "Epoch 104/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5788 - accuracy: 0.6919\n",
      "Epoch 105/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5806 - accuracy: 0.6879\n",
      " 78/757 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757/757 [==============================] - 1s 1ms/step\n",
      "Epoch 1/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6851 - accuracy: 0.5504\n",
      "Epoch 2/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.6612 - accuracy: 0.6095\n",
      "Epoch 3/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6511 - accuracy: 0.6277\n",
      "Epoch 4/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6427 - accuracy: 0.6395\n",
      "Epoch 5/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6357 - accuracy: 0.6458\n",
      "Epoch 6/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6300 - accuracy: 0.6545\n",
      "Epoch 7/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6252 - accuracy: 0.6577\n",
      "Epoch 8/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6225 - accuracy: 0.6581\n",
      "Epoch 9/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6186 - accuracy: 0.6625\n",
      "Epoch 10/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6167 - accuracy: 0.6630\n",
      "Epoch 11/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6140 - accuracy: 0.6637\n",
      "Epoch 12/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6125 - accuracy: 0.6663\n",
      "Epoch 13/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6104 - accuracy: 0.6689\n",
      "Epoch 14/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6100 - accuracy: 0.6688\n",
      "Epoch 15/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6071 - accuracy: 0.6706\n",
      "Epoch 16/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6076 - accuracy: 0.6684\n",
      "Epoch 17/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6051 - accuracy: 0.6726\n",
      "Epoch 18/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6022 - accuracy: 0.6745\n",
      "Epoch 19/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6018 - accuracy: 0.6759\n",
      "Epoch 20/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6011 - accuracy: 0.6762\n",
      "Epoch 21/105\n",
      "3026/3026 [==============================] - 18s 6ms/step - loss: 0.6011 - accuracy: 0.6764\n",
      "Epoch 22/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6006 - accuracy: 0.6755\n",
      "Epoch 23/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5998 - accuracy: 0.6763\n",
      "Epoch 24/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5968 - accuracy: 0.6767\n",
      "Epoch 25/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5958 - accuracy: 0.6806\n",
      "Epoch 26/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5955 - accuracy: 0.6813\n",
      "Epoch 27/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5939 - accuracy: 0.6821\n",
      "Epoch 28/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5953 - accuracy: 0.6790\n",
      "Epoch 29/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5949 - accuracy: 0.6810\n",
      "Epoch 30/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5921 - accuracy: 0.6823\n",
      "Epoch 31/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5925 - accuracy: 0.6824\n",
      "Epoch 32/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5915 - accuracy: 0.6828\n",
      "Epoch 33/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5920 - accuracy: 0.6831\n",
      "Epoch 34/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5916 - accuracy: 0.6826\n",
      "Epoch 35/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5901 - accuracy: 0.6858\n",
      "Epoch 36/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5897 - accuracy: 0.6852\n",
      "Epoch 37/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5874 - accuracy: 0.6869\n",
      "Epoch 38/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5900 - accuracy: 0.6851\n",
      "Epoch 39/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5896 - accuracy: 0.6835\n",
      "Epoch 40/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5883 - accuracy: 0.6846\n",
      "Epoch 41/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5881 - accuracy: 0.6875\n",
      "Epoch 42/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5874 - accuracy: 0.6869\n",
      "Epoch 43/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5903 - accuracy: 0.6844\n",
      "Epoch 44/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5867 - accuracy: 0.6868\n",
      "Epoch 45/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5861 - accuracy: 0.6880\n",
      "Epoch 46/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5856 - accuracy: 0.6884\n",
      "Epoch 47/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5881 - accuracy: 0.6849\n",
      "Epoch 48/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5836 - accuracy: 0.6909\n",
      "Epoch 49/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5874 - accuracy: 0.6851\n",
      "Epoch 50/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5853 - accuracy: 0.6890\n",
      "Epoch 51/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5864 - accuracy: 0.6870\n",
      "Epoch 52/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5844 - accuracy: 0.6884\n",
      "Epoch 53/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5852 - accuracy: 0.6886\n",
      "Epoch 54/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5848 - accuracy: 0.6877\n",
      "Epoch 55/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5863 - accuracy: 0.6883\n",
      "Epoch 56/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5821 - accuracy: 0.6893\n",
      "Epoch 57/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5848 - accuracy: 0.6867\n",
      "Epoch 58/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5843 - accuracy: 0.6876\n",
      "Epoch 59/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5834 - accuracy: 0.6897\n",
      "Epoch 60/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5835 - accuracy: 0.6902\n",
      "Epoch 61/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5860 - accuracy: 0.6875\n",
      "Epoch 62/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5837 - accuracy: 0.6875\n",
      "Epoch 63/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5833 - accuracy: 0.6887\n",
      "Epoch 64/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5849 - accuracy: 0.6880\n",
      "Epoch 65/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5841 - accuracy: 0.6868\n",
      "Epoch 66/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5831 - accuracy: 0.6914\n",
      "Epoch 67/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5813 - accuracy: 0.6909\n",
      "Epoch 68/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5844 - accuracy: 0.6898\n",
      "Epoch 69/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5828 - accuracy: 0.6890\n",
      "Epoch 70/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5813 - accuracy: 0.6905\n",
      "Epoch 71/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5817 - accuracy: 0.6918\n",
      "Epoch 72/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5845 - accuracy: 0.6891\n",
      "Epoch 73/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5833 - accuracy: 0.6887\n",
      "Epoch 74/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5824 - accuracy: 0.6891\n",
      "Epoch 75/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5835 - accuracy: 0.6888\n",
      "Epoch 76/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5819 - accuracy: 0.6903\n",
      "Epoch 77/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5824 - accuracy: 0.6916\n",
      "Epoch 78/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5815 - accuracy: 0.6910\n",
      "Epoch 79/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5817 - accuracy: 0.6904\n",
      "Epoch 80/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5812 - accuracy: 0.6903\n",
      "Epoch 81/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5799 - accuracy: 0.6897\n",
      "Epoch 82/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5804 - accuracy: 0.6903\n",
      "Epoch 83/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5811 - accuracy: 0.6911\n",
      "Epoch 84/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5808 - accuracy: 0.6888\n",
      "Epoch 85/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5806 - accuracy: 0.6926\n",
      "Epoch 86/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5816 - accuracy: 0.6911\n",
      "Epoch 87/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5785 - accuracy: 0.6901\n",
      "Epoch 88/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5841 - accuracy: 0.6879\n",
      "Epoch 89/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5817 - accuracy: 0.6899\n",
      "Epoch 90/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5786 - accuracy: 0.6926\n",
      "Epoch 91/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5816 - accuracy: 0.6897\n",
      "Epoch 92/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5790 - accuracy: 0.6922\n",
      "Epoch 93/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5831 - accuracy: 0.6879\n",
      "Epoch 94/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5810 - accuracy: 0.6901\n",
      "Epoch 95/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5795 - accuracy: 0.6922\n",
      "Epoch 96/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5789 - accuracy: 0.6933\n",
      "Epoch 97/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5786 - accuracy: 0.6903\n",
      "Epoch 98/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5799 - accuracy: 0.6880\n",
      "Epoch 99/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5808 - accuracy: 0.6898\n",
      "Epoch 100/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5791 - accuracy: 0.6912\n",
      "Epoch 101/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5800 - accuracy: 0.6919\n",
      "Epoch 102/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5798 - accuracy: 0.6910\n",
      "Epoch 103/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5791 - accuracy: 0.6920\n",
      "Epoch 104/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5791 - accuracy: 0.6919\n",
      "Epoch 105/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5799 - accuracy: 0.6871\n",
      " 84/757 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757/757 [==============================] - 1s 1ms/step\n",
      "Epoch 1/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6864 - accuracy: 0.5446\n",
      "Epoch 2/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6655 - accuracy: 0.6002\n",
      "Epoch 3/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.6509 - accuracy: 0.6291\n",
      "Epoch 4/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6415 - accuracy: 0.6401\n",
      "Epoch 5/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6344 - accuracy: 0.6479\n",
      "Epoch 6/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6291 - accuracy: 0.6542\n",
      "Epoch 7/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6224 - accuracy: 0.6589\n",
      "Epoch 8/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6185 - accuracy: 0.6630\n",
      "Epoch 9/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6149 - accuracy: 0.6654\n",
      "Epoch 10/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.6130 - accuracy: 0.6660\n",
      "Epoch 11/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6116 - accuracy: 0.6679\n",
      "Epoch 12/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.6104 - accuracy: 0.6678\n",
      "Epoch 13/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6079 - accuracy: 0.6702\n",
      "Epoch 14/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6060 - accuracy: 0.6712\n",
      "Epoch 15/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.6059 - accuracy: 0.6720\n",
      "Epoch 16/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6037 - accuracy: 0.6766\n",
      "Epoch 17/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6031 - accuracy: 0.6765\n",
      "Epoch 18/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6030 - accuracy: 0.6753\n",
      "Epoch 19/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6032 - accuracy: 0.6771\n",
      "Epoch 20/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5991 - accuracy: 0.6789\n",
      "Epoch 21/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5989 - accuracy: 0.6783\n",
      "Epoch 22/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5969 - accuracy: 0.6807\n",
      "Epoch 23/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5971 - accuracy: 0.6799\n",
      "Epoch 24/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5990 - accuracy: 0.6791\n",
      "Epoch 25/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5957 - accuracy: 0.6818\n",
      "Epoch 26/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5964 - accuracy: 0.6800\n",
      "Epoch 27/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5956 - accuracy: 0.6818\n",
      "Epoch 28/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5948 - accuracy: 0.6829\n",
      "Epoch 29/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5934 - accuracy: 0.6812\n",
      "Epoch 30/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5942 - accuracy: 0.6833\n",
      "Epoch 31/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5943 - accuracy: 0.6815\n",
      "Epoch 32/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5909 - accuracy: 0.6866\n",
      "Epoch 33/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5941 - accuracy: 0.6819\n",
      "Epoch 34/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5909 - accuracy: 0.6836\n",
      "Epoch 35/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5933 - accuracy: 0.6838\n",
      "Epoch 36/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5899 - accuracy: 0.6853\n",
      "Epoch 37/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5926 - accuracy: 0.6825\n",
      "Epoch 38/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5904 - accuracy: 0.6847\n",
      "Epoch 39/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5907 - accuracy: 0.6841\n",
      "Epoch 40/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5891 - accuracy: 0.6861\n",
      "Epoch 41/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5902 - accuracy: 0.6844\n",
      "Epoch 42/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5887 - accuracy: 0.6861\n",
      "Epoch 43/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5907 - accuracy: 0.6836\n",
      "Epoch 44/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5891 - accuracy: 0.6847\n",
      "Epoch 45/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5878 - accuracy: 0.6872\n",
      "Epoch 46/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5883 - accuracy: 0.6850\n",
      "Epoch 47/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5889 - accuracy: 0.6859\n",
      "Epoch 48/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5885 - accuracy: 0.6845\n",
      "Epoch 49/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5878 - accuracy: 0.6843\n",
      "Epoch 50/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5852 - accuracy: 0.6882\n",
      "Epoch 51/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5863 - accuracy: 0.6859\n",
      "Epoch 52/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5879 - accuracy: 0.6852\n",
      "Epoch 53/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5857 - accuracy: 0.6879\n",
      "Epoch 54/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5855 - accuracy: 0.6890\n",
      "Epoch 55/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5858 - accuracy: 0.6863\n",
      "Epoch 56/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5860 - accuracy: 0.6877\n",
      "Epoch 57/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5852 - accuracy: 0.6880\n",
      "Epoch 58/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5847 - accuracy: 0.6880\n",
      "Epoch 59/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5849 - accuracy: 0.6879\n",
      "Epoch 60/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5842 - accuracy: 0.6883\n",
      "Epoch 61/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5849 - accuracy: 0.6870\n",
      "Epoch 62/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5840 - accuracy: 0.6875\n",
      "Epoch 63/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5840 - accuracy: 0.6879\n",
      "Epoch 64/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5818 - accuracy: 0.6912\n",
      "Epoch 65/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5830 - accuracy: 0.6877\n",
      "Epoch 66/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5835 - accuracy: 0.6875\n",
      "Epoch 67/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5815 - accuracy: 0.6914\n",
      "Epoch 68/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5829 - accuracy: 0.6897\n",
      "Epoch 69/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5845 - accuracy: 0.6873\n",
      "Epoch 70/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5854 - accuracy: 0.6864\n",
      "Epoch 71/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5834 - accuracy: 0.6869\n",
      "Epoch 72/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5819 - accuracy: 0.6905\n",
      "Epoch 73/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5808 - accuracy: 0.6930\n",
      "Epoch 74/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5833 - accuracy: 0.6887\n",
      "Epoch 75/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5808 - accuracy: 0.6908\n",
      "Epoch 76/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5820 - accuracy: 0.6903\n",
      "Epoch 77/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5782 - accuracy: 0.6928\n",
      "Epoch 78/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5833 - accuracy: 0.6887\n",
      "Epoch 79/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5820 - accuracy: 0.6889\n",
      "Epoch 80/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5801 - accuracy: 0.6907\n",
      "Epoch 81/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5801 - accuracy: 0.6893\n",
      "Epoch 82/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5808 - accuracy: 0.6900\n",
      "Epoch 83/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5825 - accuracy: 0.6890\n",
      "Epoch 84/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5807 - accuracy: 0.6894\n",
      "Epoch 85/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5815 - accuracy: 0.6897\n",
      "Epoch 86/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5773 - accuracy: 0.6945\n",
      "Epoch 87/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5814 - accuracy: 0.6912\n",
      "Epoch 88/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5806 - accuracy: 0.6912\n",
      "Epoch 89/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5798 - accuracy: 0.6906\n",
      "Epoch 90/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5804 - accuracy: 0.6883\n",
      "Epoch 91/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5801 - accuracy: 0.6913\n",
      "Epoch 92/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5801 - accuracy: 0.6909\n",
      "Epoch 93/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5798 - accuracy: 0.6909\n",
      "Epoch 94/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5810 - accuracy: 0.6885\n",
      "Epoch 95/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5824 - accuracy: 0.6869\n",
      "Epoch 96/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5781 - accuracy: 0.6898\n",
      "Epoch 97/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5800 - accuracy: 0.6915\n",
      "Epoch 98/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5806 - accuracy: 0.6897\n",
      "Epoch 99/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5807 - accuracy: 0.6917\n",
      "Epoch 100/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5792 - accuracy: 0.6914\n",
      "Epoch 101/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5772 - accuracy: 0.6932\n",
      "Epoch 102/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5796 - accuracy: 0.6901\n",
      "Epoch 103/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5807 - accuracy: 0.6908\n",
      "Epoch 104/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5794 - accuracy: 0.6928\n",
      "Epoch 105/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5809 - accuracy: 0.6902\n",
      " 83/757 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757/757 [==============================] - 1s 1ms/step\n",
      "Epoch 1/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6959 - accuracy: 0.4836\n",
      "Epoch 2/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.6809 - accuracy: 0.5618\n",
      "Epoch 3/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6708 - accuracy: 0.5831\n",
      "Epoch 4/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.6631 - accuracy: 0.6021\n",
      "Epoch 5/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.6557 - accuracy: 0.6164\n",
      "Epoch 6/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6495 - accuracy: 0.6259\n",
      "Epoch 7/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6425 - accuracy: 0.6351\n",
      "Epoch 8/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.6377 - accuracy: 0.6423\n",
      "Epoch 9/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.6326 - accuracy: 0.6483\n",
      "Epoch 10/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6271 - accuracy: 0.6541\n",
      "Epoch 11/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6233 - accuracy: 0.6557\n",
      "Epoch 12/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.6183 - accuracy: 0.6610\n",
      "Epoch 13/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.6156 - accuracy: 0.6647\n",
      "Epoch 14/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6125 - accuracy: 0.6665\n",
      "Epoch 15/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6094 - accuracy: 0.6685\n",
      "Epoch 16/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6074 - accuracy: 0.6719\n",
      "Epoch 17/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6050 - accuracy: 0.6729\n",
      "Epoch 18/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6027 - accuracy: 0.6769\n",
      "Epoch 19/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6027 - accuracy: 0.6768\n",
      "Epoch 20/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.6010 - accuracy: 0.6772\n",
      "Epoch 21/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5990 - accuracy: 0.6809\n",
      "Epoch 22/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5971 - accuracy: 0.6801\n",
      "Epoch 23/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5956 - accuracy: 0.6805\n",
      "Epoch 24/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5948 - accuracy: 0.6830\n",
      "Epoch 25/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5970 - accuracy: 0.6811\n",
      "Epoch 26/105\n",
      "3026/3026 [==============================] - 18s 6ms/step - loss: 0.5915 - accuracy: 0.6865\n",
      "Epoch 27/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5943 - accuracy: 0.6821\n",
      "Epoch 28/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5912 - accuracy: 0.6853\n",
      "Epoch 29/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5911 - accuracy: 0.6854\n",
      "Epoch 30/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5897 - accuracy: 0.6838\n",
      "Epoch 31/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5893 - accuracy: 0.6854\n",
      "Epoch 32/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5881 - accuracy: 0.6874\n",
      "Epoch 33/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5863 - accuracy: 0.6882\n",
      "Epoch 34/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5896 - accuracy: 0.6828\n",
      "Epoch 35/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5870 - accuracy: 0.6890\n",
      "Epoch 36/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5875 - accuracy: 0.6887\n",
      "Epoch 37/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5887 - accuracy: 0.6861\n",
      "Epoch 38/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5868 - accuracy: 0.6851\n",
      "Epoch 39/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5869 - accuracy: 0.6876\n",
      "Epoch 40/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5859 - accuracy: 0.6880\n",
      "Epoch 41/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5859 - accuracy: 0.6858\n",
      "Epoch 42/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5870 - accuracy: 0.6879\n",
      "Epoch 43/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5868 - accuracy: 0.6880\n",
      "Epoch 44/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5845 - accuracy: 0.6889\n",
      "Epoch 45/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5871 - accuracy: 0.6851\n",
      "Epoch 46/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5841 - accuracy: 0.6897\n",
      "Epoch 47/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5854 - accuracy: 0.6883\n",
      "Epoch 48/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5852 - accuracy: 0.6883\n",
      "Epoch 49/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5838 - accuracy: 0.6875\n",
      "Epoch 50/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5836 - accuracy: 0.6866\n",
      "Epoch 51/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5811 - accuracy: 0.6924\n",
      "Epoch 52/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5843 - accuracy: 0.6903\n",
      "Epoch 53/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5829 - accuracy: 0.6882\n",
      "Epoch 54/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5818 - accuracy: 0.6887\n",
      "Epoch 55/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5821 - accuracy: 0.6905\n",
      "Epoch 56/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5818 - accuracy: 0.6905\n",
      "Epoch 57/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5849 - accuracy: 0.6879\n",
      "Epoch 58/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5813 - accuracy: 0.6898\n",
      "Epoch 59/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5807 - accuracy: 0.6909\n",
      "Epoch 60/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5849 - accuracy: 0.6882\n",
      "Epoch 61/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5805 - accuracy: 0.6894\n",
      "Epoch 62/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5794 - accuracy: 0.6933\n",
      "Epoch 63/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5815 - accuracy: 0.6900\n",
      "Epoch 64/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5784 - accuracy: 0.6922\n",
      "Epoch 65/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5801 - accuracy: 0.6897\n",
      "Epoch 66/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5798 - accuracy: 0.6907\n",
      "Epoch 67/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5820 - accuracy: 0.6896\n",
      "Epoch 68/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5778 - accuracy: 0.6936\n",
      "Epoch 69/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5825 - accuracy: 0.6882\n",
      "Epoch 70/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5794 - accuracy: 0.6928\n",
      "Epoch 71/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5831 - accuracy: 0.6885\n",
      "Epoch 72/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5780 - accuracy: 0.6919\n",
      "Epoch 73/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5790 - accuracy: 0.6937\n",
      "Epoch 74/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5784 - accuracy: 0.6930\n",
      "Epoch 75/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5803 - accuracy: 0.6919\n",
      "Epoch 76/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5814 - accuracy: 0.6898\n",
      "Epoch 77/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5794 - accuracy: 0.6921\n",
      "Epoch 78/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5779 - accuracy: 0.6927\n",
      "Epoch 79/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5779 - accuracy: 0.6931\n",
      "Epoch 80/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5828 - accuracy: 0.6890\n",
      "Epoch 81/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5782 - accuracy: 0.6926\n",
      "Epoch 82/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5795 - accuracy: 0.6921\n",
      "Epoch 83/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5774 - accuracy: 0.6930\n",
      "Epoch 84/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5775 - accuracy: 0.6922\n",
      "Epoch 85/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5785 - accuracy: 0.6917\n",
      "Epoch 86/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5782 - accuracy: 0.6953\n",
      "Epoch 87/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5794 - accuracy: 0.6902\n",
      "Epoch 88/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5782 - accuracy: 0.6918\n",
      "Epoch 89/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5770 - accuracy: 0.6936\n",
      "Epoch 90/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5792 - accuracy: 0.6927\n",
      "Epoch 91/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5771 - accuracy: 0.6945\n",
      "Epoch 92/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5773 - accuracy: 0.6934\n",
      "Epoch 93/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5778 - accuracy: 0.6935\n",
      "Epoch 94/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5781 - accuracy: 0.6917\n",
      "Epoch 95/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5769 - accuracy: 0.6921\n",
      "Epoch 96/105\n",
      "3026/3026 [==============================] - 16s 5ms/step - loss: 0.5785 - accuracy: 0.6909\n",
      "Epoch 97/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5780 - accuracy: 0.6928\n",
      "Epoch 98/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5762 - accuracy: 0.6947\n",
      "Epoch 99/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5773 - accuracy: 0.6921\n",
      "Epoch 100/105\n",
      "3026/3026 [==============================] - 17s 6ms/step - loss: 0.5791 - accuracy: 0.6910\n",
      "Epoch 101/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5776 - accuracy: 0.6926\n",
      "Epoch 102/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5778 - accuracy: 0.6923\n",
      "Epoch 103/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5763 - accuracy: 0.6921\n",
      "Epoch 104/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5796 - accuracy: 0.6914\n",
      "Epoch 105/105\n",
      "3026/3026 [==============================] - 17s 5ms/step - loss: 0.5788 - accuracy: 0.6910\n",
      " 81/757 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757/757 [==============================] - 1s 1ms/step\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6962  \u001b[0m | \u001b[0m 0.2221  \u001b[0m | \u001b[0m 33.21   \u001b[0m | \u001b[0m 0.1159  \u001b[0m | \u001b[0m 0.2589  \u001b[0m | \u001b[0m 104.8   \u001b[0m | \u001b[0m 1.662   \u001b[0m | \u001b[0m 1.127   \u001b[0m | \u001b[0m 0.3117  \u001b[0m | \u001b[0m 71.78   \u001b[0m | \u001b[0m 0.7296  \u001b[0m | \u001b[0m 2.55    \u001b[0m |\n",
      "Epoch 1/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4571\n",
      "Epoch 2/121\n",
      "  35/2560 [..............................] - ETA: 12s - loss: nan - accuracy: 0.4985"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1765: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current - self.min_delta, self.best):\n",
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py:2511: RuntimeWarning: invalid value encountered in less\n",
      "  self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4590\n",
      "Epoch 3/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4553\n",
      "Epoch 4/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4556\n",
      "Epoch 5/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4561\n",
      "Epoch 6/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4567\n",
      "Epoch 7/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4558\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.09886036015360528.\n",
      "Epoch 8/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4590\n",
      "Epoch 9/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4572\n",
      "Epoch 10/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4569\n",
      "Epoch 00010: early stopping\n",
      " 76/640 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/640 [==============================] - 1s 1ms/step\n",
      "Epoch 1/121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:463: RuntimeWarning: invalid value encountered in greater\n",
      "  return (proba > 0.5).astype('int32')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4595\n",
      "Epoch 2/121\n",
      "  34/2560 [..............................] - ETA: 11s - loss: nan - accuracy: 0.4365"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1765: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current - self.min_delta, self.best):\n",
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py:2511: RuntimeWarning: invalid value encountered in less\n",
      "  self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4566\n",
      "Epoch 3/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4576\n",
      "Epoch 4/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4558\n",
      "Epoch 5/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4592\n",
      "Epoch 6/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4588\n",
      "Epoch 7/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4547\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.019772072030721056.\n",
      "Epoch 8/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4563\n",
      "Epoch 9/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4554\n",
      "Epoch 10/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: nan - accuracy: 0.4579\n",
      "Epoch 00010: early stopping\n",
      " 78/640 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/640 [==============================] - 1s 1ms/step\n",
      "Epoch 1/121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:463: RuntimeWarning: invalid value encountered in greater\n",
      "  return (proba > 0.5).astype('int32')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.7039 - accuracy: 0.6436\n",
      "Epoch 2/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5934 - accuracy: 0.6785\n",
      "Epoch 3/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5886 - accuracy: 0.6811\n",
      "Epoch 4/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5860 - accuracy: 0.6852\n",
      "Epoch 5/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5862 - accuracy: 0.6857\n",
      "Epoch 6/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5861 - accuracy: 0.6845\n",
      "Epoch 7/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5831 - accuracy: 0.6865\n",
      "Epoch 8/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5837 - accuracy: 0.6846\n",
      "Epoch 9/121\n",
      "2560/2560 [==============================] - 13s 5ms/step - loss: 0.5806 - accuracy: 0.6877\n",
      "Epoch 10/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5815 - accuracy: 0.6890\n",
      "Epoch 11/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5785 - accuracy: 0.6907\n",
      "Epoch 12/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5785 - accuracy: 0.6892\n",
      "Epoch 13/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5812 - accuracy: 0.6914\n",
      "Epoch 14/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5798 - accuracy: 0.6925\n",
      "Epoch 15/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5773 - accuracy: 0.6935\n",
      "Epoch 16/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5777 - accuracy: 0.6953\n",
      "Epoch 17/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5767 - accuracy: 0.6938\n",
      "Epoch 18/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5775 - accuracy: 0.6926\n",
      "Epoch 19/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5743 - accuracy: 0.6967\n",
      "Epoch 20/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5768 - accuracy: 0.6922\n",
      "Epoch 21/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5755 - accuracy: 0.6953\n",
      "Epoch 22/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5761 - accuracy: 0.6929\n",
      "Epoch 23/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5728 - accuracy: 0.6971\n",
      "Epoch 24/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5741 - accuracy: 0.6957\n",
      "Epoch 25/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5745 - accuracy: 0.6969\n",
      "Epoch 26/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5752 - accuracy: 0.6952\n",
      "Epoch 27/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5731 - accuracy: 0.6975\n",
      "Epoch 28/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5734 - accuracy: 0.6951\n",
      "Epoch 29/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5770 - accuracy: 0.6921\n",
      "Epoch 30/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5729 - accuracy: 0.6948\n",
      "Epoch 31/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5733 - accuracy: 0.6966\n",
      "Epoch 32/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5735 - accuracy: 0.6941\n",
      "Epoch 33/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5707 - accuracy: 0.6981\n",
      "Epoch 34/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5732 - accuracy: 0.6956\n",
      "Epoch 35/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5716 - accuracy: 0.6979\n",
      "Epoch 36/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5713 - accuracy: 0.6957\n",
      "Epoch 37/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5730 - accuracy: 0.6954\n",
      "Epoch 38/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5730 - accuracy: 0.6953\n",
      "Epoch 39/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5713 - accuracy: 0.6955\n",
      "Epoch 40/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5740 - accuracy: 0.6932\n",
      "Epoch 41/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5737 - accuracy: 0.6941\n",
      "Epoch 42/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5700 - accuracy: 0.6977\n",
      "Epoch 43/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5740 - accuracy: 0.6950\n",
      "Epoch 44/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5736 - accuracy: 0.6936\n",
      "Epoch 45/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5713 - accuracy: 0.6977\n",
      "Epoch 46/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5702 - accuracy: 0.6971\n",
      "Epoch 47/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5686 - accuracy: 0.6989\n",
      "Epoch 48/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5741 - accuracy: 0.6943\n",
      "Epoch 49/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5728 - accuracy: 0.6947\n",
      "Epoch 50/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5693 - accuracy: 0.6981\n",
      "Epoch 51/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5704 - accuracy: 0.6963\n",
      "Epoch 52/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5711 - accuracy: 0.6955\n",
      "Epoch 53/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5709 - accuracy: 0.6978\n",
      "Epoch 54/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5708 - accuracy: 0.6984\n",
      "Epoch 55/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5684 - accuracy: 0.6999\n",
      "Epoch 56/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5729 - accuracy: 0.6941\n",
      "Epoch 57/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5721 - accuracy: 0.6956\n",
      "Epoch 58/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5722 - accuracy: 0.6959\n",
      "Epoch 59/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5732 - accuracy: 0.6941\n",
      "Epoch 60/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5710 - accuracy: 0.6984\n",
      "Epoch 61/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5706 - accuracy: 0.6985\n",
      "Epoch 62/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5708 - accuracy: 0.6965\n",
      "Epoch 63/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5692 - accuracy: 0.6999\n",
      "Epoch 64/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5711 - accuracy: 0.6974\n",
      "Epoch 65/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5722 - accuracy: 0.6937\n",
      "Epoch 66/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5699 - accuracy: 0.6991\n",
      "Epoch 67/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5707 - accuracy: 0.6981\n",
      "Epoch 68/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5691 - accuracy: 0.6996\n",
      "Epoch 69/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5721 - accuracy: 0.6958\n",
      "Epoch 70/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5691 - accuracy: 0.6991\n",
      "Epoch 71/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5689 - accuracy: 0.6995\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.003954414406144211.\n",
      "Epoch 72/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5681 - accuracy: 0.7001\n",
      "Epoch 73/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5657 - accuracy: 0.6989\n",
      "Epoch 74/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5688 - accuracy: 0.6979\n",
      "Epoch 75/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5676 - accuracy: 0.6999\n",
      "Epoch 76/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5671 - accuracy: 0.7002\n",
      "Epoch 77/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5652 - accuracy: 0.7009\n",
      "Epoch 78/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5675 - accuracy: 0.7006\n",
      "Epoch 79/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5671 - accuracy: 0.6983\n",
      "Epoch 80/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5647 - accuracy: 0.7005\n",
      "Epoch 81/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5656 - accuracy: 0.6998\n",
      "Epoch 82/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5672 - accuracy: 0.7000\n",
      "Epoch 83/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5663 - accuracy: 0.6991\n",
      "Epoch 84/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5665 - accuracy: 0.6986\n",
      "Epoch 85/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5676 - accuracy: 0.7005\n",
      "Epoch 86/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5662 - accuracy: 0.7007\n",
      "Epoch 87/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5654 - accuracy: 0.7001\n",
      "Epoch 88/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5665 - accuracy: 0.6991\n",
      "Epoch 89/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5641 - accuracy: 0.7005\n",
      "Epoch 90/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5652 - accuracy: 0.7000\n",
      "Epoch 91/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5674 - accuracy: 0.6997\n",
      "Epoch 92/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5658 - accuracy: 0.6988\n",
      "Epoch 93/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5645 - accuracy: 0.7002\n",
      "Epoch 94/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5649 - accuracy: 0.7015\n",
      "Epoch 95/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5649 - accuracy: 0.7018\n",
      "Epoch 96/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5654 - accuracy: 0.7002\n",
      "Epoch 97/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5645 - accuracy: 0.7026\n",
      "Epoch 98/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5636 - accuracy: 0.7012\n",
      "Epoch 99/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5648 - accuracy: 0.7008\n",
      "Epoch 100/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5646 - accuracy: 0.7017\n",
      "Epoch 101/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5672 - accuracy: 0.6983\n",
      "Epoch 102/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5642 - accuracy: 0.7023\n",
      "Epoch 103/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5648 - accuracy: 0.7020\n",
      "Epoch 104/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5667 - accuracy: 0.6984\n",
      "Epoch 105/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5678 - accuracy: 0.6978\n",
      "Epoch 106/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5671 - accuracy: 0.6995\n",
      "Epoch 107/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5664 - accuracy: 0.7002\n",
      "Epoch 108/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5625 - accuracy: 0.7010\n",
      "Epoch 109/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5659 - accuracy: 0.6999\n",
      "Epoch 110/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5628 - accuracy: 0.7020\n",
      "Epoch 111/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5626 - accuracy: 0.7037\n",
      "Epoch 112/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5631 - accuracy: 0.7023\n",
      "Epoch 113/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5656 - accuracy: 0.7006\n",
      "Epoch 114/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5652 - accuracy: 0.6990\n",
      "Epoch 115/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5667 - accuracy: 0.6981\n",
      "Epoch 116/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5668 - accuracy: 0.6996\n",
      "Epoch 117/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5627 - accuracy: 0.7027\n",
      "Epoch 118/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5633 - accuracy: 0.7021\n",
      "Epoch 119/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5640 - accuracy: 0.7007\n",
      "Epoch 120/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5652 - accuracy: 0.6991\n",
      "Epoch 121/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5617 - accuracy: 0.7050\n",
      " 79/640 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/640 [==============================] - 1s 1ms/step\n",
      "Epoch 1/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.6548 - accuracy: 0.6461\n",
      "Epoch 2/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5973 - accuracy: 0.6746\n",
      "Epoch 3/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5895 - accuracy: 0.6806\n",
      "Epoch 4/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5878 - accuracy: 0.6839\n",
      "Epoch 5/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5868 - accuracy: 0.6845\n",
      "Epoch 6/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5827 - accuracy: 0.6865\n",
      "Epoch 7/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5825 - accuracy: 0.6873\n",
      "Epoch 8/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5836 - accuracy: 0.6870\n",
      "Epoch 9/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5808 - accuracy: 0.6898\n",
      "Epoch 10/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5807 - accuracy: 0.6906\n",
      "Epoch 11/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5808 - accuracy: 0.6876\n",
      "Epoch 12/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5815 - accuracy: 0.6877\n",
      "Epoch 13/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5797 - accuracy: 0.6874\n",
      "Epoch 14/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5798 - accuracy: 0.6886\n",
      "Epoch 15/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5766 - accuracy: 0.6923\n",
      "Epoch 16/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5804 - accuracy: 0.6882\n",
      "Epoch 17/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5792 - accuracy: 0.6888\n",
      "Epoch 18/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5791 - accuracy: 0.6917\n",
      "Epoch 19/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5756 - accuracy: 0.6917\n",
      "Epoch 20/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5770 - accuracy: 0.6919\n",
      "Epoch 21/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5757 - accuracy: 0.6906\n",
      "Epoch 22/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5786 - accuracy: 0.6876\n",
      "Epoch 23/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5781 - accuracy: 0.6911\n",
      "Epoch 24/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5795 - accuracy: 0.6885\n",
      "Epoch 25/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5765 - accuracy: 0.6917\n",
      "Epoch 26/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5755 - accuracy: 0.6923\n",
      "Epoch 27/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5788 - accuracy: 0.6904\n",
      "Epoch 28/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5784 - accuracy: 0.6886\n",
      "Epoch 29/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5773 - accuracy: 0.6885\n",
      "Epoch 30/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5747 - accuracy: 0.6927\n",
      "Epoch 31/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5759 - accuracy: 0.6913\n",
      "Epoch 32/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5741 - accuracy: 0.6937\n",
      "Epoch 33/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5747 - accuracy: 0.6935\n",
      "Epoch 34/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5753 - accuracy: 0.6914\n",
      "Epoch 35/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5719 - accuracy: 0.6928\n",
      "Epoch 36/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5755 - accuracy: 0.6976\n",
      "Epoch 37/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5738 - accuracy: 0.6975\n",
      "Epoch 38/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5754 - accuracy: 0.6962\n",
      "Epoch 39/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5744 - accuracy: 0.6962\n",
      "Epoch 40/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5740 - accuracy: 0.6966\n",
      "Epoch 41/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5751 - accuracy: 0.6960\n",
      "Epoch 42/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5750 - accuracy: 0.6973\n",
      "Epoch 43/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5738 - accuracy: 0.6995\n",
      "Epoch 44/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5738 - accuracy: 0.6964\n",
      "Epoch 45/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5729 - accuracy: 0.6987\n",
      "Epoch 46/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5729 - accuracy: 0.6967\n",
      "Epoch 47/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5730 - accuracy: 0.6976\n",
      "Epoch 48/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5713 - accuracy: 0.6987\n",
      "Epoch 49/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5721 - accuracy: 0.6971\n",
      "Epoch 50/121\n",
      "2560/2560 [==============================] - 13s 5ms/step - loss: 0.5734 - accuracy: 0.6981\n",
      "Epoch 51/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5707 - accuracy: 0.6958\n",
      "Epoch 52/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5726 - accuracy: 0.6980\n",
      "Epoch 53/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5734 - accuracy: 0.6977\n",
      "Epoch 54/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5740 - accuracy: 0.6952\n",
      "Epoch 55/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5702 - accuracy: 0.6993\n",
      "Epoch 56/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5714 - accuracy: 0.6986\n",
      "Epoch 57/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5698 - accuracy: 0.6981\n",
      "Epoch 58/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5718 - accuracy: 0.6963\n",
      "Epoch 59/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5717 - accuracy: 0.6965\n",
      "Epoch 60/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5704 - accuracy: 0.6960\n",
      "Epoch 61/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5722 - accuracy: 0.6949\n",
      "Epoch 62/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5733 - accuracy: 0.6943\n",
      "Epoch 63/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5692 - accuracy: 0.6993\n",
      "Epoch 64/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5724 - accuracy: 0.6953\n",
      "Epoch 65/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5698 - accuracy: 0.6979\n",
      "Epoch 66/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5695 - accuracy: 0.6977\n",
      "Epoch 67/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5692 - accuracy: 0.6982\n",
      "Epoch 68/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5693 - accuracy: 0.6978\n",
      "Epoch 69/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5674 - accuracy: 0.6995\n",
      "Epoch 70/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5716 - accuracy: 0.6952\n",
      "Epoch 71/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5695 - accuracy: 0.6967\n",
      "Epoch 72/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5716 - accuracy: 0.6965\n",
      "Epoch 73/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5687 - accuracy: 0.6966\n",
      "Epoch 74/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5686 - accuracy: 0.6980\n",
      "Epoch 75/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5669 - accuracy: 0.6988\n",
      "Epoch 76/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5681 - accuracy: 0.6981\n",
      "Epoch 77/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5679 - accuracy: 0.6988\n",
      "Epoch 78/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5653 - accuracy: 0.7032\n",
      "Epoch 79/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5702 - accuracy: 0.6978\n",
      "Epoch 80/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5647 - accuracy: 0.7049\n",
      "Epoch 81/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5664 - accuracy: 0.6998\n",
      "Epoch 82/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5676 - accuracy: 0.6982\n",
      "Epoch 83/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5705 - accuracy: 0.6977\n",
      "Epoch 84/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5666 - accuracy: 0.7003\n",
      "Epoch 85/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5697 - accuracy: 0.6990\n",
      "Epoch 86/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5681 - accuracy: 0.6990\n",
      "Epoch 87/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5667 - accuracy: 0.6985\n",
      "Epoch 88/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5693 - accuracy: 0.6977\n",
      "Epoch 89/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5669 - accuracy: 0.7013\n",
      "Epoch 90/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5702 - accuracy: 0.6992\n",
      "Epoch 91/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5672 - accuracy: 0.6986\n",
      "Epoch 92/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5669 - accuracy: 0.6994\n",
      "Epoch 93/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5674 - accuracy: 0.7000\n",
      "Epoch 94/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5678 - accuracy: 0.6972\n",
      "Epoch 95/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5642 - accuracy: 0.7006\n",
      "Epoch 96/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5670 - accuracy: 0.7009\n",
      "Epoch 97/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5651 - accuracy: 0.7036\n",
      "Epoch 98/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5660 - accuracy: 0.7026\n",
      "Epoch 99/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5651 - accuracy: 0.7018\n",
      "Epoch 100/121\n",
      "2560/2560 [==============================] - 13s 5ms/step - loss: 0.5663 - accuracy: 0.7018\n",
      "Epoch 101/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5667 - accuracy: 0.7008\n",
      "Epoch 102/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5697 - accuracy: 0.6990\n",
      "Epoch 103/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5672 - accuracy: 0.6987\n",
      "Epoch 104/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5685 - accuracy: 0.6983\n",
      "Epoch 105/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5645 - accuracy: 0.7038\n",
      "Epoch 106/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5671 - accuracy: 0.7008\n",
      "Epoch 107/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5663 - accuracy: 0.7014\n",
      "Epoch 108/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5669 - accuracy: 0.6996\n",
      "Epoch 109/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5662 - accuracy: 0.6980\n",
      "Epoch 110/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5657 - accuracy: 0.7010\n",
      "Epoch 111/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5663 - accuracy: 0.6996\n",
      "Epoch 112/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5652 - accuracy: 0.6995\n",
      "Epoch 113/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5669 - accuracy: 0.6999\n",
      "Epoch 114/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5663 - accuracy: 0.6993\n",
      "Epoch 115/121\n",
      "2560/2560 [==============================] - ETA: 0s - loss: 0.5666 - accuracy: 0.69 - 12s 5ms/step - loss: 0.5666 - accuracy: 0.6994\n",
      "Epoch 116/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5650 - accuracy: 0.7020\n",
      "Epoch 117/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5657 - accuracy: 0.6995\n",
      "Epoch 118/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5660 - accuracy: 0.7007\n",
      "Epoch 119/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5671 - accuracy: 0.6984\n",
      "Epoch 120/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5653 - accuracy: 0.7021\n",
      "Epoch 121/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5619 - accuracy: 0.7037\n",
      " 76/640 [==>...........................] - ETA: 0s - ETA: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/640 [==============================] - 1s 1ms/step\n",
      "Epoch 1/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.6387 - accuracy: 0.6488\n",
      "Epoch 2/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5925 - accuracy: 0.6791\n",
      "Epoch 3/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5877 - accuracy: 0.6821\n",
      "Epoch 4/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5857 - accuracy: 0.6840\n",
      "Epoch 5/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5831 - accuracy: 0.6839\n",
      "Epoch 6/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5802 - accuracy: 0.6868\n",
      "Epoch 7/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5801 - accuracy: 0.6888\n",
      "Epoch 8/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5811 - accuracy: 0.6878\n",
      "Epoch 9/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5781 - accuracy: 0.6916\n",
      "Epoch 10/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5785 - accuracy: 0.6893\n",
      "Epoch 11/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5787 - accuracy: 0.6887\n",
      "Epoch 12/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5790 - accuracy: 0.6900\n",
      "Epoch 13/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5777 - accuracy: 0.6904\n",
      "Epoch 14/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5767 - accuracy: 0.6917\n",
      "Epoch 15/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5787 - accuracy: 0.6887\n",
      "Epoch 16/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5767 - accuracy: 0.6914\n",
      "Epoch 17/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5770 - accuracy: 0.6883\n",
      "Epoch 18/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5764 - accuracy: 0.6917\n",
      "Epoch 19/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5765 - accuracy: 0.6909\n",
      "Epoch 20/121\n",
      "2560/2560 [==============================] - 13s 5ms/step - loss: 0.5777 - accuracy: 0.6911\n",
      "Epoch 21/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5762 - accuracy: 0.6904\n",
      "Epoch 22/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5765 - accuracy: 0.6905\n",
      "Epoch 23/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5750 - accuracy: 0.6920\n",
      "Epoch 24/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5748 - accuracy: 0.6933\n",
      "Epoch 25/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5751 - accuracy: 0.6984\n",
      "Epoch 26/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5740 - accuracy: 0.6970\n",
      "Epoch 27/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5761 - accuracy: 0.6978\n",
      "Epoch 28/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5740 - accuracy: 0.6962\n",
      "Epoch 29/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5753 - accuracy: 0.6970\n",
      "Epoch 30/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5738 - accuracy: 0.6947\n",
      "Epoch 31/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5708 - accuracy: 0.7007\n",
      "Epoch 32/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5747 - accuracy: 0.6963\n",
      "Epoch 33/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5721 - accuracy: 0.6984\n",
      "Epoch 34/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5748 - accuracy: 0.6961\n",
      "Epoch 35/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5718 - accuracy: 0.6983\n",
      "Epoch 36/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5707 - accuracy: 0.6988\n",
      "Epoch 37/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5729 - accuracy: 0.6967\n",
      "Epoch 38/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5712 - accuracy: 0.6992\n",
      "Epoch 39/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5699 - accuracy: 0.6984\n",
      "Epoch 40/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5697 - accuracy: 0.7000\n",
      "Epoch 41/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5689 - accuracy: 0.7012\n",
      "Epoch 42/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5724 - accuracy: 0.6980\n",
      "Epoch 43/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5684 - accuracy: 0.6991\n",
      "Epoch 44/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5702 - accuracy: 0.6982\n",
      "Epoch 45/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5696 - accuracy: 0.6998\n",
      "Epoch 46/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5686 - accuracy: 0.6998\n",
      "Epoch 47/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5705 - accuracy: 0.6977\n",
      "Epoch 48/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5688 - accuracy: 0.6980\n",
      "Epoch 49/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5704 - accuracy: 0.6985\n",
      "Epoch 50/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5709 - accuracy: 0.6967\n",
      "Epoch 51/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5701 - accuracy: 0.6998\n",
      "Epoch 52/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5706 - accuracy: 0.6995\n",
      "Epoch 53/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5698 - accuracy: 0.6985\n",
      "Epoch 54/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5691 - accuracy: 0.6972\n",
      "Epoch 55/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5699 - accuracy: 0.6965\n",
      "Epoch 56/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5706 - accuracy: 0.6977\n",
      "Epoch 57/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5685 - accuracy: 0.6989\n",
      "Epoch 58/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5694 - accuracy: 0.6977\n",
      "Epoch 59/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5706 - accuracy: 0.6971\n",
      "Epoch 60/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5669 - accuracy: 0.6995\n",
      "Epoch 61/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5693 - accuracy: 0.6998\n",
      "Epoch 62/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5691 - accuracy: 0.6964\n",
      "Epoch 63/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5667 - accuracy: 0.7002\n",
      "Epoch 64/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5688 - accuracy: 0.6996\n",
      "Epoch 65/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5689 - accuracy: 0.6992\n",
      "Epoch 66/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5680 - accuracy: 0.7011\n",
      "Epoch 67/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5706 - accuracy: 0.6973\n",
      "Epoch 68/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5698 - accuracy: 0.6984\n",
      "Epoch 69/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5684 - accuracy: 0.6997\n",
      "Epoch 70/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5670 - accuracy: 0.6982\n",
      "Epoch 71/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5667 - accuracy: 0.7010\n",
      "Epoch 72/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5690 - accuracy: 0.6982\n",
      "Epoch 73/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5667 - accuracy: 0.6996\n",
      "Epoch 74/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5684 - accuracy: 0.6996\n",
      "Epoch 75/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5667 - accuracy: 0.6994\n",
      "Epoch 76/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5694 - accuracy: 0.6998\n",
      "Epoch 77/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5657 - accuracy: 0.7013\n",
      "Epoch 78/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5690 - accuracy: 0.6984\n",
      "Epoch 79/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5672 - accuracy: 0.7005\n",
      "Epoch 80/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5648 - accuracy: 0.7011\n",
      "Epoch 81/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5648 - accuracy: 0.7021\n",
      "Epoch 82/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5673 - accuracy: 0.6987\n",
      "Epoch 83/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5665 - accuracy: 0.7020\n",
      "Epoch 84/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5657 - accuracy: 0.6994\n",
      "Epoch 85/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5655 - accuracy: 0.7022\n",
      "Epoch 86/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5672 - accuracy: 0.6986\n",
      "Epoch 87/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5652 - accuracy: 0.7031\n",
      "Epoch 88/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5654 - accuracy: 0.7007\n",
      "Epoch 89/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5648 - accuracy: 0.7017\n",
      "Epoch 90/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5681 - accuracy: 0.6985\n",
      "Epoch 91/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5649 - accuracy: 0.7014\n",
      "Epoch 92/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5658 - accuracy: 0.7021\n",
      "Epoch 93/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5666 - accuracy: 0.7007\n",
      "Epoch 94/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5678 - accuracy: 0.6990\n",
      "Epoch 95/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5669 - accuracy: 0.6984\n",
      "Epoch 96/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5664 - accuracy: 0.7004\n",
      "Epoch 97/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5675 - accuracy: 0.7005\n",
      "Epoch 98/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5640 - accuracy: 0.7018\n",
      "Epoch 99/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5660 - accuracy: 0.7003\n",
      "Epoch 100/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5649 - accuracy: 0.7017\n",
      "Epoch 101/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5652 - accuracy: 0.7012\n",
      "Epoch 102/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5653 - accuracy: 0.7010\n",
      "Epoch 103/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5631 - accuracy: 0.7039\n",
      "Epoch 104/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5679 - accuracy: 0.6996\n",
      "Epoch 105/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5662 - accuracy: 0.6998\n",
      "Epoch 106/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5665 - accuracy: 0.7010\n",
      "Epoch 107/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5663 - accuracy: 0.7002\n",
      "Epoch 108/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5637 - accuracy: 0.7026\n",
      "Epoch 109/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5642 - accuracy: 0.7032\n",
      "Epoch 110/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5660 - accuracy: 0.7015\n",
      "Epoch 111/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5667 - accuracy: 0.7007\n",
      "Epoch 112/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5675 - accuracy: 0.6980\n",
      "Epoch 113/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5652 - accuracy: 0.7018\n",
      "Epoch 114/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5673 - accuracy: 0.6986\n",
      "Epoch 115/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5662 - accuracy: 0.7000\n",
      "Epoch 116/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5644 - accuracy: 0.7023\n",
      "Epoch 117/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5647 - accuracy: 0.7025\n",
      "Epoch 118/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5648 - accuracy: 0.7017\n",
      "Epoch 119/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5639 - accuracy: 0.7027\n",
      "Epoch 120/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5634 - accuracy: 0.7027\n",
      "Epoch 121/121\n",
      "2560/2560 [==============================] - 12s 5ms/step - loss: 0.5677 - accuracy: 0.6985\n",
      " 82/640 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/640 [==============================] - 1s 1ms/step\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.602   \u001b[0m | \u001b[0m 2.662   \u001b[0m | \u001b[0m 38.67   \u001b[0m | \u001b[0m 0.1196  \u001b[0m | \u001b[0m 0.214   \u001b[0m | \u001b[0m 121.3   \u001b[0m | \u001b[0m 2.123   \u001b[0m | \u001b[0m 2.542   \u001b[0m | \u001b[0m 0.4943  \u001b[0m | \u001b[0m 109.3   \u001b[0m | \u001b[0m 0.4275  \u001b[0m | \u001b[0m 0.1017  \u001b[0m |\n",
      "Epoch 1/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: nan - accuracy: 0.4574\n",
      "Epoch 2/91\n",
      "  33/5547 [..............................] - ETA: 26s - loss: nan - accuracy: 0.4510"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1765: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current - self.min_delta, self.best):\n",
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py:2511: RuntimeWarning: invalid value encountered in less\n",
      "  self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5547/5547 [==============================] - 26s 5ms/step - loss: nan - accuracy: 0.4543\n",
      "Epoch 3/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: nan - accuracy: 0.4575\n",
      "Epoch 4/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: nan - accuracy: 0.4541\n",
      "Epoch 5/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: nan - accuracy: 0.4543\n",
      "Epoch 6/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: nan - accuracy: 0.4564\n",
      "Epoch 7/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: nan - accuracy: 0.4564\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.08219450802251882.\n",
      "Epoch 8/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: nan - accuracy: 0.4572\n",
      "Epoch 9/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: nan - accuracy: 0.4586\n",
      "Epoch 10/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: nan - accuracy: 0.4549\n",
      "Epoch 00010: early stopping\n",
      "  92/1387 [>.............................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - 2s 1ms/step\n",
      "Epoch 1/91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:463: RuntimeWarning: invalid value encountered in greater\n",
      "  return (proba > 0.5).astype('int32')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5547/5547 [==============================] - 26s 5ms/step - loss: nan - accuracy: 0.4595\n",
      "Epoch 2/91\n",
      "  35/5547 [..............................] - ETA: 25s - loss: nan - accuracy: 0.4668"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1765: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current - self.min_delta, self.best):\n",
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py:2511: RuntimeWarning: invalid value encountered in less\n",
      "  self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5547/5547 [==============================] - 25s 5ms/step - loss: nan - accuracy: 0.4575\n",
      "Epoch 3/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: nan - accuracy: 0.4587\n",
      "Epoch 4/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: nan - accuracy: 0.4588\n",
      "Epoch 5/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: nan - accuracy: 0.4590\n",
      "Epoch 6/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: nan - accuracy: 0.4597\n",
      "Epoch 7/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: nan - accuracy: 0.4595\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.016438901604503765.\n",
      "Epoch 8/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: nan - accuracy: 0.4565\n",
      "Epoch 9/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: nan - accuracy: 0.4574\n",
      "Epoch 10/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: nan - accuracy: 0.4588\n",
      "Epoch 00010: early stopping\n",
      "  90/1387 [>.............................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - 2s 1ms/step\n",
      "Epoch 1/91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:463: RuntimeWarning: invalid value encountered in greater\n",
      "  return (proba > 0.5).astype('int32')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.6621 - accuracy: 0.6462\n",
      "Epoch 2/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5930 - accuracy: 0.6816\n",
      "Epoch 3/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5904 - accuracy: 0.6839\n",
      "Epoch 4/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5888 - accuracy: 0.6835\n",
      "Epoch 5/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5832 - accuracy: 0.6889\n",
      "Epoch 6/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5834 - accuracy: 0.6888\n",
      "Epoch 7/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5850 - accuracy: 0.6900\n",
      "Epoch 8/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5817 - accuracy: 0.6893\n",
      "Epoch 9/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5805 - accuracy: 0.6934\n",
      "Epoch 10/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5796 - accuracy: 0.6928\n",
      "Epoch 11/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5774 - accuracy: 0.6924\n",
      "Epoch 12/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5793 - accuracy: 0.6926\n",
      "Epoch 13/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5775 - accuracy: 0.6924\n",
      "Epoch 14/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5767 - accuracy: 0.6935\n",
      "Epoch 15/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5773 - accuracy: 0.6943\n",
      "Epoch 16/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5775 - accuracy: 0.6935\n",
      "Epoch 17/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5761 - accuracy: 0.6936\n",
      "Epoch 18/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5779 - accuracy: 0.6947\n",
      "Epoch 19/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5800 - accuracy: 0.6900\n",
      "Epoch 20/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5774 - accuracy: 0.6912\n",
      "Epoch 21/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5755 - accuracy: 0.6936\n",
      "Epoch 22/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5756 - accuracy: 0.6942\n",
      "Epoch 23/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5760 - accuracy: 0.6953\n",
      "Epoch 24/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5749 - accuracy: 0.6950\n",
      "Epoch 25/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5744 - accuracy: 0.6955\n",
      "Epoch 26/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5751 - accuracy: 0.6964\n",
      "Epoch 27/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5746 - accuracy: 0.6961\n",
      "Epoch 28/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5763 - accuracy: 0.6929\n",
      "Epoch 29/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5757 - accuracy: 0.6937\n",
      "Epoch 30/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5746 - accuracy: 0.6951\n",
      "Epoch 31/91\n",
      "5547/5547 [==============================] - 27s 5ms/step - loss: 0.5757 - accuracy: 0.6939\n",
      "Epoch 32/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5780 - accuracy: 0.6919\n",
      "Epoch 33/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5760 - accuracy: 0.6935\n",
      "Epoch 34/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5768 - accuracy: 0.6921\n",
      "Epoch 35/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5769 - accuracy: 0.6954\n",
      "Epoch 36/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5741 - accuracy: 0.6942\n",
      "Epoch 37/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5754 - accuracy: 0.6960\n",
      "Epoch 38/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5753 - accuracy: 0.6954\n",
      "Epoch 39/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5753 - accuracy: 0.6936\n",
      "Epoch 40/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5750 - accuracy: 0.6952\n",
      "Epoch 41/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5767 - accuracy: 0.6934\n",
      "Epoch 42/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5746 - accuracy: 0.6941\n",
      "Epoch 43/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5749 - accuracy: 0.6951\n",
      "Epoch 44/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5760 - accuracy: 0.6921\n",
      "Epoch 45/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5770 - accuracy: 0.6923\n",
      "Epoch 46/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5741 - accuracy: 0.6967\n",
      "Epoch 47/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5723 - accuracy: 0.6960\n",
      "Epoch 48/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5735 - accuracy: 0.6968\n",
      "Epoch 49/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5738 - accuracy: 0.6966\n",
      "Epoch 50/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5752 - accuracy: 0.6925\n",
      "Epoch 51/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5757 - accuracy: 0.6949\n",
      "Epoch 52/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5752 - accuracy: 0.6939\n",
      "Epoch 53/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5757 - accuracy: 0.6949\n",
      "Epoch 54/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5750 - accuracy: 0.6949\n",
      "Epoch 55/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5718 - accuracy: 0.6976\n",
      "Epoch 56/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5701 - accuracy: 0.6990\n",
      "Epoch 57/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5728 - accuracy: 0.6971\n",
      "Epoch 58/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5720 - accuracy: 0.6964\n",
      "Epoch 59/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5739 - accuracy: 0.6952\n",
      "Epoch 60/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5751 - accuracy: 0.6941\n",
      "Epoch 61/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5741 - accuracy: 0.6950\n",
      "Epoch 62/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5719 - accuracy: 0.6963\n",
      "Epoch 63/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5712 - accuracy: 0.6995\n",
      "Epoch 64/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5739 - accuracy: 0.6945\n",
      "Epoch 65/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5729 - accuracy: 0.6970\n",
      "Epoch 66/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5737 - accuracy: 0.6938\n",
      "Epoch 67/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5735 - accuracy: 0.6964\n",
      "Epoch 68/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5717 - accuracy: 0.6956\n",
      "Epoch 69/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5720 - accuracy: 0.6968\n",
      "Epoch 70/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5736 - accuracy: 0.6964\n",
      "Epoch 71/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5722 - accuracy: 0.6975\n",
      "Epoch 72/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5746 - accuracy: 0.6948\n",
      "Epoch 73/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5709 - accuracy: 0.6983\n",
      "Epoch 74/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5737 - accuracy: 0.6944\n",
      "Epoch 75/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5715 - accuracy: 0.6972\n",
      "Epoch 76/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5727 - accuracy: 0.6965\n",
      "Epoch 77/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5710 - accuracy: 0.6990\n",
      "Epoch 78/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5735 - accuracy: 0.6971\n",
      "Epoch 79/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5730 - accuracy: 0.6972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5722 - accuracy: 0.6962\n",
      "Epoch 81/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5719 - accuracy: 0.6966\n",
      "Epoch 82/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5719 - accuracy: 0.6967\n",
      "Epoch 83/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5710 - accuracy: 0.6973\n",
      "Epoch 84/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5726 - accuracy: 0.6956\n",
      "Epoch 85/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5716 - accuracy: 0.6957\n",
      "Epoch 86/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5731 - accuracy: 0.6971\n",
      "Epoch 87/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5745 - accuracy: 0.6959\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.003287780320900753.\n",
      "Epoch 88/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5690 - accuracy: 0.6975\n",
      "Epoch 89/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5705 - accuracy: 0.6957\n",
      "Epoch 90/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5694 - accuracy: 0.6984\n",
      "Epoch 91/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5678 - accuracy: 0.6982\n",
      "  84/1387 [>.............................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - 2s 1ms/step\n",
      "Epoch 1/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.6517 - accuracy: 0.6454\n",
      "Epoch 2/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5932 - accuracy: 0.6794\n",
      "Epoch 3/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5895 - accuracy: 0.6811\n",
      "Epoch 4/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5859 - accuracy: 0.6854\n",
      "Epoch 5/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5857 - accuracy: 0.6839\n",
      "Epoch 6/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5841 - accuracy: 0.6866\n",
      "Epoch 7/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5838 - accuracy: 0.6847\n",
      "Epoch 8/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5797 - accuracy: 0.6890\n",
      "Epoch 9/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5827 - accuracy: 0.6873\n",
      "Epoch 10/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5827 - accuracy: 0.6864\n",
      "Epoch 11/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5797 - accuracy: 0.6890\n",
      "Epoch 12/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5814 - accuracy: 0.6875\n",
      "Epoch 13/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5788 - accuracy: 0.6905\n",
      "Epoch 14/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5799 - accuracy: 0.6881\n",
      "Epoch 15/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5788 - accuracy: 0.6942\n",
      "Epoch 16/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5776 - accuracy: 0.6947\n",
      "Epoch 17/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5802 - accuracy: 0.6935\n",
      "Epoch 18/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5786 - accuracy: 0.6917\n",
      "Epoch 19/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5751 - accuracy: 0.6957\n",
      "Epoch 20/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5774 - accuracy: 0.6927\n",
      "Epoch 21/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5765 - accuracy: 0.6940\n",
      "Epoch 22/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5751 - accuracy: 0.6967\n",
      "Epoch 23/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5754 - accuracy: 0.6958\n",
      "Epoch 24/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5766 - accuracy: 0.6937\n",
      "Epoch 25/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5735 - accuracy: 0.6957\n",
      "Epoch 26/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5753 - accuracy: 0.6945\n",
      "Epoch 27/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5727 - accuracy: 0.6969\n",
      "Epoch 28/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5742 - accuracy: 0.6948\n",
      "Epoch 29/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5728 - accuracy: 0.6963\n",
      "Epoch 30/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5726 - accuracy: 0.6982\n",
      "Epoch 31/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5748 - accuracy: 0.6947\n",
      "Epoch 32/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5731 - accuracy: 0.6972\n",
      "Epoch 33/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5734 - accuracy: 0.6959\n",
      "Epoch 34/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5725 - accuracy: 0.6942\n",
      "Epoch 35/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5719 - accuracy: 0.6978\n",
      "Epoch 36/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5714 - accuracy: 0.6969\n",
      "Epoch 37/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5716 - accuracy: 0.6982\n",
      "Epoch 38/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5734 - accuracy: 0.6982\n",
      "Epoch 39/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5708 - accuracy: 0.6980\n",
      "Epoch 40/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5717 - accuracy: 0.6958\n",
      "Epoch 41/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5719 - accuracy: 0.6969\n",
      "Epoch 42/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5711 - accuracy: 0.6966\n",
      "Epoch 43/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5730 - accuracy: 0.6951\n",
      "Epoch 44/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5727 - accuracy: 0.6960\n",
      "Epoch 45/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5712 - accuracy: 0.6971\n",
      "Epoch 46/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5736 - accuracy: 0.6953\n",
      "Epoch 47/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5706 - accuracy: 0.6974\n",
      "Epoch 48/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5716 - accuracy: 0.6961\n",
      "Epoch 49/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5735 - accuracy: 0.6969\n",
      "Epoch 50/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5756 - accuracy: 0.6907\n",
      "Epoch 51/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5706 - accuracy: 0.6978\n",
      "Epoch 52/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5690 - accuracy: 0.6990\n",
      "Epoch 53/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5707 - accuracy: 0.6965\n",
      "Epoch 54/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5691 - accuracy: 0.6986\n",
      "Epoch 55/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5720 - accuracy: 0.6958\n",
      "Epoch 56/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5706 - accuracy: 0.6957\n",
      "Epoch 57/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5714 - accuracy: 0.6959\n",
      "Epoch 58/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5709 - accuracy: 0.6981\n",
      "Epoch 59/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5724 - accuracy: 0.6938\n",
      "Epoch 60/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5681 - accuracy: 0.6984\n",
      "Epoch 61/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5709 - accuracy: 0.6974\n",
      "Epoch 62/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5719 - accuracy: 0.6957\n",
      "Epoch 63/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5703 - accuracy: 0.6971\n",
      "Epoch 64/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5727 - accuracy: 0.6956\n",
      "Epoch 65/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5685 - accuracy: 0.6971\n",
      "Epoch 66/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5696 - accuracy: 0.6963\n",
      "Epoch 67/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5708 - accuracy: 0.6984\n",
      "Epoch 68/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5706 - accuracy: 0.6971\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 69/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5656 - accuracy: 0.7007\n",
      "Epoch 70/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5657 - accuracy: 0.7000\n",
      "Epoch 71/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5705 - accuracy: 0.6975\n",
      "Epoch 72/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5671 - accuracy: 0.6989\n",
      "Epoch 73/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5666 - accuracy: 0.7005\n",
      "Epoch 74/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5673 - accuracy: 0.6988\n",
      "Epoch 75/91\n",
      "5547/5547 [==============================] - 27s 5ms/step - loss: 0.5674 - accuracy: 0.6988\n",
      "Epoch 76/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5709 - accuracy: 0.6957\n",
      "Epoch 77/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5693 - accuracy: 0.6960\n",
      "Epoch 78/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5685 - accuracy: 0.6969\n",
      "Epoch 79/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5687 - accuracy: 0.6972\n",
      "Epoch 80/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5689 - accuracy: 0.6981\n",
      "Epoch 81/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5682 - accuracy: 0.6993\n",
      "Epoch 82/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5674 - accuracy: 0.6985\n",
      "Epoch 83/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5671 - accuracy: 0.6987\n",
      "Epoch 84/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5690 - accuracy: 0.6976\n",
      "Epoch 85/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5667 - accuracy: 0.7007\n",
      "Epoch 86/91\n",
      "5547/5547 [==============================] - 25s 4ms/step - loss: 0.5672 - accuracy: 0.6995\n",
      "Epoch 87/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5678 - accuracy: 0.6991\n",
      "Epoch 88/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5692 - accuracy: 0.7000\n",
      "Epoch 89/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5673 - accuracy: 0.6988\n",
      "Epoch 90/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5695 - accuracy: 0.6972\n",
      "Epoch 91/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5684 - accuracy: 0.6997\n",
      "  85/1387 [>.............................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - 2s 1ms/step\n",
      "Epoch 1/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.6469 - accuracy: 0.6475\n",
      "Epoch 2/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5935 - accuracy: 0.6772\n",
      "Epoch 3/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5901 - accuracy: 0.6808\n",
      "Epoch 4/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5856 - accuracy: 0.6840\n",
      "Epoch 5/91\n",
      "5547/5547 [==============================] - 25s 4ms/step - loss: 0.5844 - accuracy: 0.6857\n",
      "Epoch 6/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5828 - accuracy: 0.6870\n",
      "Epoch 7/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5852 - accuracy: 0.6857\n",
      "Epoch 8/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5830 - accuracy: 0.6872\n",
      "Epoch 9/91\n",
      "5547/5547 [==============================] - 25s 4ms/step - loss: 0.5816 - accuracy: 0.6886\n",
      "Epoch 10/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5827 - accuracy: 0.6871\n",
      "Epoch 11/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5816 - accuracy: 0.6882\n",
      "Epoch 12/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5811 - accuracy: 0.6877\n",
      "Epoch 13/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5821 - accuracy: 0.6871\n",
      "Epoch 14/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5785 - accuracy: 0.6924\n",
      "Epoch 15/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5807 - accuracy: 0.6874\n",
      "Epoch 16/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5787 - accuracy: 0.6900\n",
      "Epoch 17/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5781 - accuracy: 0.6895\n",
      "Epoch 18/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5767 - accuracy: 0.6927\n",
      "Epoch 19/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5791 - accuracy: 0.6891\n",
      "Epoch 20/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5774 - accuracy: 0.6910\n",
      "Epoch 21/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5809 - accuracy: 0.6883\n",
      "Epoch 22/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5772 - accuracy: 0.6883\n",
      "Epoch 23/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5768 - accuracy: 0.6929\n",
      "Epoch 24/91\n",
      "5547/5547 [==============================] - 25s 4ms/step - loss: 0.5785 - accuracy: 0.6906\n",
      "Epoch 25/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5779 - accuracy: 0.6906\n",
      "Epoch 26/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5757 - accuracy: 0.6903\n",
      "Epoch 27/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5765 - accuracy: 0.6904\n",
      "Epoch 28/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5772 - accuracy: 0.6907\n",
      "Epoch 29/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5783 - accuracy: 0.6896\n",
      "Epoch 30/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5776 - accuracy: 0.6880\n",
      "Epoch 31/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5785 - accuracy: 0.6892\n",
      "Epoch 32/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5770 - accuracy: 0.6917\n",
      "Epoch 33/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5775 - accuracy: 0.6928\n",
      "Epoch 34/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5764 - accuracy: 0.6906\n",
      "Epoch 35/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5776 - accuracy: 0.6916\n",
      "Epoch 36/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5773 - accuracy: 0.6917\n",
      "Epoch 37/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5774 - accuracy: 0.6909\n",
      "Epoch 38/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5760 - accuracy: 0.6911\n",
      "Epoch 39/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5771 - accuracy: 0.6908\n",
      "Epoch 40/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5773 - accuracy: 0.6880\n",
      "Epoch 41/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5769 - accuracy: 0.6934\n",
      "Epoch 42/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5734 - accuracy: 0.6928\n",
      "Epoch 43/91\n",
      "5547/5547 [==============================] - 25s 4ms/step - loss: 0.5763 - accuracy: 0.6909\n",
      "Epoch 44/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5762 - accuracy: 0.6916\n",
      "Epoch 45/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5741 - accuracy: 0.6922\n",
      "Epoch 46/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5750 - accuracy: 0.6902\n",
      "Epoch 47/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5751 - accuracy: 0.6939\n",
      "Epoch 48/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5756 - accuracy: 0.6912\n",
      "Epoch 49/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5774 - accuracy: 0.6889\n",
      "Epoch 50/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5749 - accuracy: 0.6929\n",
      "Epoch 51/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5748 - accuracy: 0.6919\n",
      "Epoch 52/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5744 - accuracy: 0.6950\n",
      "Epoch 53/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5744 - accuracy: 0.6945\n",
      "Epoch 54/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5737 - accuracy: 0.6978\n",
      "Epoch 55/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5735 - accuracy: 0.6971\n",
      "Epoch 56/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5748 - accuracy: 0.6967\n",
      "Epoch 57/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5746 - accuracy: 0.6963\n",
      "Epoch 58/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5753 - accuracy: 0.6972\n",
      "Epoch 59/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5732 - accuracy: 0.6970\n",
      "Epoch 60/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5724 - accuracy: 0.6992\n",
      "Epoch 61/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5745 - accuracy: 0.6994\n",
      "Epoch 62/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5731 - accuracy: 0.6993\n",
      "Epoch 63/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5731 - accuracy: 0.6994\n",
      "Epoch 64/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5726 - accuracy: 0.7000\n",
      "Epoch 65/91\n",
      "5547/5547 [==============================] - ETA: 0s - loss: 0.5761 - accuracy: 0.69 - 25s 5ms/step - loss: 0.5761 - accuracy: 0.6947\n",
      "Epoch 66/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5734 - accuracy: 0.6978\n",
      "Epoch 67/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5752 - accuracy: 0.6971\n",
      "Epoch 68/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5734 - accuracy: 0.6970\n",
      "Epoch 69/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5725 - accuracy: 0.6992\n",
      "Epoch 70/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5729 - accuracy: 0.6975\n",
      "Epoch 71/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5734 - accuracy: 0.6999\n",
      "Epoch 72/91\n",
      "5547/5547 [==============================] - 26s 5ms/step - loss: 0.5718 - accuracy: 0.6976\n",
      "Epoch 73/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5728 - accuracy: 0.6981\n",
      "Epoch 74/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5731 - accuracy: 0.6990\n",
      "Epoch 75/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5735 - accuracy: 0.6963\n",
      "Epoch 76/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5731 - accuracy: 0.6957\n",
      "Epoch 77/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5720 - accuracy: 0.6986\n",
      "Epoch 78/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5731 - accuracy: 0.6981\n",
      "Epoch 79/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5719 - accuracy: 0.6983\n",
      "Epoch 80/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5721 - accuracy: 0.6979\n",
      "Epoch 81/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5730 - accuracy: 0.6975\n",
      "Epoch 82/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5728 - accuracy: 0.6982\n",
      "Epoch 83/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5735 - accuracy: 0.6981\n",
      "Epoch 84/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5722 - accuracy: 0.6964\n",
      "Epoch 85/91\n",
      "5547/5547 [==============================] - 25s 4ms/step - loss: 0.5692 - accuracy: 0.7002\n",
      "Epoch 86/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5733 - accuracy: 0.6978\n",
      "Epoch 87/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5710 - accuracy: 0.6991\n",
      "Epoch 88/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5691 - accuracy: 0.7006\n",
      "Epoch 89/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5702 - accuracy: 0.7003\n",
      "Epoch 90/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5711 - accuracy: 0.6993\n",
      "Epoch 91/91\n",
      "5547/5547 [==============================] - 25s 5ms/step - loss: 0.5727 - accuracy: 0.6983\n",
      "  90/1387 [>.............................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - 2s 1ms/step\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.6021  \u001b[0m | \u001b[0m 0.3237  \u001b[0m | \u001b[0m 17.51   \u001b[0m | \u001b[0m 0.6364  \u001b[0m | \u001b[0m 0.09431 \u001b[0m | \u001b[0m 91.03   \u001b[0m | \u001b[0m 2.815   \u001b[0m | \u001b[0m 1.499   \u001b[0m | \u001b[0m 0.411   \u001b[0m | \u001b[0m 153.6   \u001b[0m | \u001b[0m 0.2288  \u001b[0m | \u001b[0m 0.3079  \u001b[0m |\n",
      "Epoch 1/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.7192 - accuracy: 0.5470\n",
      "Epoch 2/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6854 - accuracy: 0.5422\n",
      "Epoch 3/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6728 - accuracy: 0.5748\n",
      "Epoch 4/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6507 - accuracy: 0.6267\n",
      "Epoch 5/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6431 - accuracy: 0.6380\n",
      "Epoch 6/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6402 - accuracy: 0.6419\n",
      "Epoch 7/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6318 - accuracy: 0.6592\n",
      "Epoch 8/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6304 - accuracy: 0.6603\n",
      "Epoch 9/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6266 - accuracy: 0.6643\n",
      "Epoch 10/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6236 - accuracy: 0.6714\n",
      "Epoch 11/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6235 - accuracy: 0.6726\n",
      "Epoch 12/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6245 - accuracy: 0.6707\n",
      "Epoch 13/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6223 - accuracy: 0.6744\n",
      "Epoch 14/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6225 - accuracy: 0.6718\n",
      "Epoch 15/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6195 - accuracy: 0.6780\n",
      "Epoch 16/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6251 - accuracy: 0.6699\n",
      "Epoch 17/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6218 - accuracy: 0.6744\n",
      "Epoch 18/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6277 - accuracy: 0.6665\n",
      "Epoch 19/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6213 - accuracy: 0.6785\n",
      "Epoch 20/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6227 - accuracy: 0.6750\n",
      "Epoch 21/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6254 - accuracy: 0.6687\n",
      "Epoch 22/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6210 - accuracy: 0.6663\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.12764612879514753.\n",
      "Epoch 23/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6133 - accuracy: 0.6839\n",
      "Epoch 24/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6060 - accuracy: 0.6853\n",
      "Epoch 25/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6064 - accuracy: 0.6703\n",
      "Epoch 26/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6061 - accuracy: 0.6732\n",
      "Epoch 27/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6044 - accuracy: 0.6716\n",
      "Epoch 28/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6057 - accuracy: 0.6750\n",
      "Epoch 29/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6051 - accuracy: 0.6762\n",
      "Epoch 30/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6028 - accuracy: 0.6729\n",
      "Epoch 31/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6007 - accuracy: 0.6750\n",
      "Epoch 32/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6038 - accuracy: 0.6695\n",
      "Epoch 33/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5984 - accuracy: 0.6746\n",
      "Epoch 34/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5984 - accuracy: 0.6770\n",
      "Epoch 35/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5953 - accuracy: 0.6801\n",
      "Epoch 36/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5904 - accuracy: 0.6846\n",
      "Epoch 37/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5899 - accuracy: 0.6864\n",
      "Epoch 38/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5905 - accuracy: 0.6860\n",
      "Epoch 39/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5853 - accuracy: 0.6897\n",
      "Epoch 40/115\n",
      "1693/1693 [==============================] - 9s 6ms/step - loss: 0.5865 - accuracy: 0.6905\n",
      "Epoch 41/115\n",
      "1693/1693 [==============================] - 9s 6ms/step - loss: 0.5839 - accuracy: 0.6910\n",
      "Epoch 42/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5841 - accuracy: 0.6902\n",
      "Epoch 43/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5837 - accuracy: 0.6941\n",
      "Epoch 44/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5853 - accuracy: 0.6910\n",
      "Epoch 45/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5835 - accuracy: 0.6930\n",
      "Epoch 46/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5844 - accuracy: 0.6897\n",
      "Epoch 47/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5823 - accuracy: 0.6937\n",
      "Epoch 48/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5826 - accuracy: 0.6925\n",
      "Epoch 49/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5824 - accuracy: 0.6931\n",
      "Epoch 50/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5802 - accuracy: 0.6951\n",
      "Epoch 51/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5824 - accuracy: 0.6936\n",
      "Epoch 52/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5816 - accuracy: 0.6934\n",
      "Epoch 53/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5809 - accuracy: 0.6939\n",
      "Epoch 54/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5813 - accuracy: 0.6921\n",
      "Epoch 55/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5808 - accuracy: 0.6934\n",
      "Epoch 56/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5814 - accuracy: 0.6958\n",
      "Epoch 57/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5813 - accuracy: 0.6936\n",
      "Epoch 58/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5812 - accuracy: 0.6942\n",
      "Epoch 59/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5805 - accuracy: 0.6960\n",
      "Epoch 60/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5813 - accuracy: 0.6953\n",
      "Epoch 61/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5804 - accuracy: 0.6932\n",
      "Epoch 62/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5786 - accuracy: 0.6952\n",
      "Epoch 63/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5787 - accuracy: 0.6936\n",
      "Epoch 64/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5782 - accuracy: 0.6960\n",
      "Epoch 65/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5797 - accuracy: 0.6959\n",
      "Epoch 66/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5790 - accuracy: 0.6958\n",
      "Epoch 67/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5776 - accuracy: 0.6971\n",
      "Epoch 68/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5799 - accuracy: 0.6962\n",
      "Epoch 69/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5791 - accuracy: 0.6931\n",
      "Epoch 70/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5788 - accuracy: 0.6947\n",
      "Epoch 71/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5786 - accuracy: 0.6947\n",
      "Epoch 72/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5752 - accuracy: 0.6989\n",
      "Epoch 73/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5775 - accuracy: 0.6967\n",
      "Epoch 74/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5776 - accuracy: 0.6967\n",
      "Epoch 75/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5781 - accuracy: 0.6952\n",
      "Epoch 76/115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5779 - accuracy: 0.6953\n",
      "Epoch 77/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5780 - accuracy: 0.6950\n",
      "Epoch 78/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5775 - accuracy: 0.6980\n",
      "Epoch 79/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5751 - accuracy: 0.6979\n",
      "Epoch 80/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5770 - accuracy: 0.6952\n",
      "Epoch 81/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5767 - accuracy: 0.6967\n",
      "Epoch 82/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5749 - accuracy: 0.6970\n",
      "Epoch 83/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5778 - accuracy: 0.6960\n",
      "Epoch 84/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5761 - accuracy: 0.6976\n",
      "Epoch 85/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5764 - accuracy: 0.6960\n",
      "Epoch 86/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5771 - accuracy: 0.6970\n",
      "Epoch 87/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5758 - accuracy: 0.6950\n",
      "Epoch 88/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5764 - accuracy: 0.6968\n",
      "Epoch 89/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5751 - accuracy: 0.6968\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.02552922575902951.\n",
      "Epoch 90/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5767 - accuracy: 0.6955\n",
      "Epoch 91/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5747 - accuracy: 0.6994\n",
      "Epoch 92/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5748 - accuracy: 0.6972\n",
      "Epoch 93/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5739 - accuracy: 0.7005\n",
      "Epoch 94/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5714 - accuracy: 0.7002\n",
      "Epoch 95/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5729 - accuracy: 0.6998\n",
      "Epoch 96/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5730 - accuracy: 0.7001\n",
      "Epoch 97/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5755 - accuracy: 0.6954\n",
      "Epoch 98/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5723 - accuracy: 0.6986\n",
      "Epoch 99/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5736 - accuracy: 0.6974\n",
      "Epoch 100/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5725 - accuracy: 0.7004\n",
      "Epoch 101/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5751 - accuracy: 0.6980\n",
      "Epoch 102/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5738 - accuracy: 0.6993\n",
      "Epoch 103/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5737 - accuracy: 0.6987\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 0.005105845151805902.\n",
      "Epoch 104/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5750 - accuracy: 0.6976\n",
      "Epoch 105/115\n",
      "1693/1693 [==============================] - 8s 5ms/step - loss: 0.5735 - accuracy: 0.6993\n",
      "Epoch 106/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5714 - accuracy: 0.7010\n",
      "Epoch 107/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5734 - accuracy: 0.6986\n",
      "Epoch 108/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5758 - accuracy: 0.6965\n",
      "Epoch 109/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5736 - accuracy: 0.6990\n",
      "Epoch 110/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5723 - accuracy: 0.6991\n",
      "Epoch 111/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5716 - accuracy: 0.7006\n",
      "Epoch 112/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5738 - accuracy: 0.6995\n",
      "Epoch 113/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5745 - accuracy: 0.6969\n",
      "Epoch 114/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5715 - accuracy: 0.6999\n",
      "Epoch 115/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5733 - accuracy: 0.6968\n",
      " 91/424 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424/424 [==============================] - 1s 1ms/step\n",
      "Epoch 1/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6533 - accuracy: 0.6205\n",
      "Epoch 2/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5910 - accuracy: 0.6871\n",
      "Epoch 3/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5814 - accuracy: 0.6930\n",
      "Epoch 4/115\n",
      "1693/1693 [==============================] - 8s 5ms/step - loss: 0.5764 - accuracy: 0.6962\n",
      "Epoch 5/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5745 - accuracy: 0.6968\n",
      "Epoch 6/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5766 - accuracy: 0.6936\n",
      "Epoch 7/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5737 - accuracy: 0.6985\n",
      "Epoch 8/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5761 - accuracy: 0.6942\n",
      "Epoch 9/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5745 - accuracy: 0.6951\n",
      "Epoch 10/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5765 - accuracy: 0.6940\n",
      "Epoch 11/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5742 - accuracy: 0.6953\n",
      "Epoch 12/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5739 - accuracy: 0.6954\n",
      "Epoch 13/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5754 - accuracy: 0.6930\n",
      "Epoch 14/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5742 - accuracy: 0.6962\n",
      "Epoch 15/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5751 - accuracy: 0.6939\n",
      "Epoch 16/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5712 - accuracy: 0.6978\n",
      "Epoch 17/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5734 - accuracy: 0.6981\n",
      "Epoch 18/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5730 - accuracy: 0.6954\n",
      "Epoch 19/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5734 - accuracy: 0.6954\n",
      "Epoch 20/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5708 - accuracy: 0.6989\n",
      "Epoch 21/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5753 - accuracy: 0.6930\n",
      "Epoch 22/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5713 - accuracy: 0.6974\n",
      "Epoch 23/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5756 - accuracy: 0.6944\n",
      "Epoch 24/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5731 - accuracy: 0.6976\n",
      "Epoch 25/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5748 - accuracy: 0.6948\n",
      "Epoch 26/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5726 - accuracy: 0.6970\n",
      "Epoch 27/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5728 - accuracy: 0.6965\n",
      "Epoch 28/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5720 - accuracy: 0.6961\n",
      "Epoch 29/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5751 - accuracy: 0.6919\n",
      "Epoch 30/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5738 - accuracy: 0.6929\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0010211690303611805.\n",
      "Epoch 31/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5727 - accuracy: 0.6949\n",
      "Epoch 32/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5735 - accuracy: 0.6956\n",
      "Epoch 33/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5727 - accuracy: 0.6962\n",
      "Epoch 34/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5705 - accuracy: 0.6987\n",
      "Epoch 35/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5727 - accuracy: 0.6973\n",
      "Epoch 36/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5707 - accuracy: 0.6976\n",
      "Epoch 37/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5724 - accuracy: 0.6966\n",
      "Epoch 38/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5736 - accuracy: 0.6950\n",
      "Epoch 39/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5729 - accuracy: 0.6967\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 40/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5741 - accuracy: 0.6945\n",
      "Epoch 41/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5723 - accuracy: 0.6975\n",
      "Epoch 42/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5689 - accuracy: 0.6996\n",
      "Epoch 43/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5721 - accuracy: 0.6967\n",
      "Epoch 44/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5729 - accuracy: 0.6965\n",
      "Epoch 45/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5715 - accuracy: 0.6985\n",
      "Epoch 46/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5715 - accuracy: 0.6972\n",
      "Epoch 47/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5733 - accuracy: 0.6977\n",
      "Epoch 48/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5715 - accuracy: 0.6950\n",
      "Epoch 49/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5691 - accuracy: 0.6993\n",
      "Epoch 50/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5735 - accuracy: 0.6958\n",
      "Epoch 51/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5749 - accuracy: 0.6948\n",
      "Epoch 52/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5717 - accuracy: 0.6976\n",
      "Epoch 00052: early stopping\n",
      " 89/424 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424/424 [==============================] - 1s 1ms/step\n",
      "Epoch 1/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.7040 - accuracy: 0.5451\n",
      "Epoch 2/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6707 - accuracy: 0.5963\n",
      "Epoch 3/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6541 - accuracy: 0.6295\n",
      "Epoch 4/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6361 - accuracy: 0.6525\n",
      "Epoch 5/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6219 - accuracy: 0.6661\n",
      "Epoch 6/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6109 - accuracy: 0.6747\n",
      "Epoch 7/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6027 - accuracy: 0.6795\n",
      "Epoch 8/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5970 - accuracy: 0.6824\n",
      "Epoch 9/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5936 - accuracy: 0.6862\n",
      "Epoch 10/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5907 - accuracy: 0.6883\n",
      "Epoch 11/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5903 - accuracy: 0.6863\n",
      "Epoch 12/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5873 - accuracy: 0.6885\n",
      "Epoch 13/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5858 - accuracy: 0.6904\n",
      "Epoch 14/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5846 - accuracy: 0.6900\n",
      "Epoch 15/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5826 - accuracy: 0.6919\n",
      "Epoch 16/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5833 - accuracy: 0.6907\n",
      "Epoch 17/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5840 - accuracy: 0.6889\n",
      "Epoch 18/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5823 - accuracy: 0.6912\n",
      "Epoch 19/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5797 - accuracy: 0.6922\n",
      "Epoch 20/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5799 - accuracy: 0.6933\n",
      "Epoch 21/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5829 - accuracy: 0.6894\n",
      "Epoch 22/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5813 - accuracy: 0.6904\n",
      "Epoch 23/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5810 - accuracy: 0.6913\n",
      "Epoch 24/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5783 - accuracy: 0.6944\n",
      "Epoch 25/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5798 - accuracy: 0.6912\n",
      "Epoch 26/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5792 - accuracy: 0.6924\n",
      "Epoch 27/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5790 - accuracy: 0.6933\n",
      "Epoch 28/115\n",
      "1693/1693 [==============================] - 8s 5ms/step - loss: 0.5765 - accuracy: 0.6939\n",
      "Epoch 29/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5815 - accuracy: 0.6916\n",
      "Epoch 30/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5786 - accuracy: 0.6955\n",
      "Epoch 31/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5768 - accuracy: 0.6953\n",
      "Epoch 32/115\n",
      "1693/1693 [==============================] - 8s 5ms/step - loss: 0.5758 - accuracy: 0.6963\n",
      "Epoch 33/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5782 - accuracy: 0.6938\n",
      "Epoch 34/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5794 - accuracy: 0.6919\n",
      "Epoch 35/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5761 - accuracy: 0.6943\n",
      "Epoch 36/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5794 - accuracy: 0.6920\n",
      "Epoch 37/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5787 - accuracy: 0.6922\n",
      "Epoch 38/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5771 - accuracy: 0.6948\n",
      "Epoch 39/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5788 - accuracy: 0.6939\n",
      "Epoch 40/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5759 - accuracy: 0.6956\n",
      "Epoch 41/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5782 - accuracy: 0.6907\n",
      "Epoch 42/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5772 - accuracy: 0.6954\n",
      "Epoch 43/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5762 - accuracy: 0.6951\n",
      "Epoch 44/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5780 - accuracy: 0.6930\n",
      "Epoch 45/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5769 - accuracy: 0.6947\n",
      "Epoch 46/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5773 - accuracy: 0.6938\n",
      "Epoch 47/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5774 - accuracy: 0.6940\n",
      "Epoch 48/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5778 - accuracy: 0.6929\n",
      "Epoch 49/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5769 - accuracy: 0.6925\n",
      "Epoch 50/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5758 - accuracy: 0.6959\n",
      "Epoch 51/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5770 - accuracy: 0.6941\n",
      "Epoch 52/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5760 - accuracy: 0.6954\n",
      "Epoch 53/115\n",
      "1693/1693 [==============================] - 8s 5ms/step - loss: 0.5753 - accuracy: 0.6947\n",
      "Epoch 54/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5767 - accuracy: 0.6950\n",
      "Epoch 55/115\n",
      "1693/1693 [==============================] - 8s 5ms/step - loss: 0.5767 - accuracy: 0.6952\n",
      "Epoch 56/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5747 - accuracy: 0.6969\n",
      "Epoch 57/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5764 - accuracy: 0.6936\n",
      "Epoch 58/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5749 - accuracy: 0.6951\n",
      "Epoch 59/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5763 - accuracy: 0.6949\n",
      "Epoch 60/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5762 - accuracy: 0.6952\n",
      "Epoch 61/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5759 - accuracy: 0.6941\n",
      "Epoch 62/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5755 - accuracy: 0.6940\n",
      "Epoch 63/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5747 - accuracy: 0.6955\n",
      "Epoch 64/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5765 - accuracy: 0.6953\n",
      "Epoch 65/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5792 - accuracy: 0.6900\n",
      "Epoch 66/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5753 - accuracy: 0.6956\n",
      "Epoch 67/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5749 - accuracy: 0.6960\n",
      "Epoch 68/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5760 - accuracy: 0.6931\n",
      "Epoch 69/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5765 - accuracy: 0.6940\n",
      "Epoch 70/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5759 - accuracy: 0.6943\n",
      "Epoch 71/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5758 - accuracy: 0.6939\n",
      "Epoch 72/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5748 - accuracy: 0.6949\n",
      "Epoch 73/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5769 - accuracy: 0.6942\n",
      "Epoch 74/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5769 - accuracy: 0.6956\n",
      "Epoch 75/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5749 - accuracy: 0.6942\n",
      "Epoch 76/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5767 - accuracy: 0.6942\n",
      "Epoch 77/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5734 - accuracy: 0.6958\n",
      "Epoch 78/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5755 - accuracy: 0.6950\n",
      "Epoch 79/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5745 - accuracy: 0.6965\n",
      "Epoch 80/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5780 - accuracy: 0.6918\n",
      "Epoch 81/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5771 - accuracy: 0.6942\n",
      "Epoch 82/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5761 - accuracy: 0.6948\n",
      "Epoch 83/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5778 - accuracy: 0.6924\n",
      "Epoch 84/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5760 - accuracy: 0.6984\n",
      "Epoch 85/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5769 - accuracy: 0.6931\n",
      "Epoch 86/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5752 - accuracy: 0.6958\n",
      "Epoch 87/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5748 - accuracy: 0.6957\n",
      "Epoch 88/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5759 - accuracy: 0.6944\n",
      "Epoch 89/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5755 - accuracy: 0.6951\n",
      "Epoch 90/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5752 - accuracy: 0.6944\n",
      "Epoch 91/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5788 - accuracy: 0.6909\n",
      "Epoch 92/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5756 - accuracy: 0.6932\n",
      "Epoch 93/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5766 - accuracy: 0.6970\n",
      "Epoch 94/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5741 - accuracy: 0.6958\n",
      "Epoch 95/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5761 - accuracy: 0.6936\n",
      "Epoch 96/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5758 - accuracy: 0.6944\n",
      "Epoch 97/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5776 - accuracy: 0.6919\n",
      "Epoch 98/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5751 - accuracy: 0.6957\n",
      "Epoch 99/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5781 - accuracy: 0.6923\n",
      "Epoch 100/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5731 - accuracy: 0.6984\n",
      "Epoch 101/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5750 - accuracy: 0.6965\n",
      "Epoch 102/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5753 - accuracy: 0.6933\n",
      "Epoch 103/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5772 - accuracy: 0.6936\n",
      "Epoch 104/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5761 - accuracy: 0.6939\n",
      "Epoch 105/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5750 - accuracy: 0.6948\n",
      "Epoch 106/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6956\n",
      "Epoch 107/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5733 - accuracy: 0.6980\n",
      "Epoch 108/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5757 - accuracy: 0.6936\n",
      "Epoch 109/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5770 - accuracy: 0.6943\n",
      "Epoch 110/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5746 - accuracy: 0.6959\n",
      "Epoch 111/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5740 - accuracy: 0.6981\n",
      "Epoch 112/115\n",
      "1693/1693 [==============================] - 8s 5ms/step - loss: 0.5767 - accuracy: 0.6936\n",
      "Epoch 113/115\n",
      "1693/1693 [==============================] - 8s 5ms/step - loss: 0.5770 - accuracy: 0.6930\n",
      "Epoch 114/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5733 - accuracy: 0.6969\n",
      "Epoch 115/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5757 - accuracy: 0.6949\n",
      " 92/424 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424/424 [==============================] - 1s 1ms/step\n",
      "Epoch 1/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6772 - accuracy: 0.5714\n",
      "Epoch 2/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6480 - accuracy: 0.6455\n",
      "Epoch 3/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6292 - accuracy: 0.6602\n",
      "Epoch 4/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6143 - accuracy: 0.6726\n",
      "Epoch 5/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6033 - accuracy: 0.6792\n",
      "Epoch 6/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5973 - accuracy: 0.6834\n",
      "Epoch 7/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5915 - accuracy: 0.6862\n",
      "Epoch 8/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5912 - accuracy: 0.6863\n",
      "Epoch 9/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5884 - accuracy: 0.6887\n",
      "Epoch 10/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5836 - accuracy: 0.6923\n",
      "Epoch 11/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5860 - accuracy: 0.6879\n",
      "Epoch 12/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5823 - accuracy: 0.6913\n",
      "Epoch 13/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5830 - accuracy: 0.6910\n",
      "Epoch 14/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5826 - accuracy: 0.6906\n",
      "Epoch 15/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5818 - accuracy: 0.6931\n",
      "Epoch 16/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5806 - accuracy: 0.6914\n",
      "Epoch 17/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5801 - accuracy: 0.6938\n",
      "Epoch 18/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5773 - accuracy: 0.6949\n",
      "Epoch 19/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5800 - accuracy: 0.6925\n",
      "Epoch 20/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5790 - accuracy: 0.6937\n",
      "Epoch 21/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5770 - accuracy: 0.6979\n",
      "Epoch 22/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5802 - accuracy: 0.6917\n",
      "Epoch 23/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5767 - accuracy: 0.6947\n",
      "Epoch 24/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5769 - accuracy: 0.6944\n",
      "Epoch 25/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5771 - accuracy: 0.6942\n",
      "Epoch 26/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5791 - accuracy: 0.6940\n",
      "Epoch 27/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5777 - accuracy: 0.6949\n",
      "Epoch 28/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5800 - accuracy: 0.6905\n",
      "Epoch 29/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5789 - accuracy: 0.6908\n",
      "Epoch 30/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5758 - accuracy: 0.6961\n",
      "Epoch 31/115\n",
      "1693/1693 [==============================] - 8s 5ms/step - loss: 0.5772 - accuracy: 0.6947\n",
      "Epoch 32/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5767 - accuracy: 0.6936\n",
      "Epoch 33/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5773 - accuracy: 0.6938\n",
      "Epoch 34/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5750 - accuracy: 0.6950\n",
      "Epoch 35/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5768 - accuracy: 0.6941\n",
      "Epoch 36/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5780 - accuracy: 0.6932\n",
      "Epoch 37/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5735 - accuracy: 0.6971\n",
      "Epoch 38/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5767 - accuracy: 0.6914\n",
      "Epoch 39/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5762 - accuracy: 0.6949\n",
      "Epoch 40/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5778 - accuracy: 0.6934\n",
      "Epoch 41/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5748 - accuracy: 0.6948\n",
      "Epoch 42/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5753 - accuracy: 0.6961\n",
      "Epoch 43/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5740 - accuracy: 0.6973\n",
      "Epoch 44/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6934\n",
      "Epoch 45/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5774 - accuracy: 0.6933\n",
      "Epoch 46/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5777 - accuracy: 0.6928\n",
      "Epoch 47/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5760 - accuracy: 0.6945\n",
      "Epoch 48/115\n",
      "1693/1693 [==============================] - 10s 6ms/step - loss: 0.5756 - accuracy: 0.6955\n",
      "Epoch 49/115\n",
      "1693/1693 [==============================] - 10s 6ms/step - loss: 0.5774 - accuracy: 0.6932\n",
      "Epoch 50/115\n",
      "1693/1693 [==============================] - 9s 6ms/step - loss: 0.5791 - accuracy: 0.6912\n",
      "Epoch 51/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5742 - accuracy: 0.6960\n",
      "Epoch 52/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5738 - accuracy: 0.6955\n",
      "Epoch 53/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5772 - accuracy: 0.6925\n",
      "Epoch 54/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5784 - accuracy: 0.6927\n",
      "Epoch 55/115\n",
      "1693/1693 [==============================] - 9s 6ms/step - loss: 0.5739 - accuracy: 0.6967\n",
      "Epoch 56/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5736 - accuracy: 0.6970\n",
      "Epoch 57/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5749 - accuracy: 0.6958\n",
      "Epoch 58/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5743 - accuracy: 0.6961\n",
      "Epoch 59/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5749 - accuracy: 0.6939\n",
      "Epoch 60/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5725 - accuracy: 0.6970\n",
      "Epoch 61/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5752 - accuracy: 0.6948\n",
      "Epoch 62/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5750 - accuracy: 0.6956\n",
      "Epoch 63/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5746 - accuracy: 0.6956\n",
      "Epoch 64/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5751 - accuracy: 0.6941\n",
      "Epoch 65/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5759 - accuracy: 0.6935\n",
      "Epoch 66/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5730 - accuracy: 0.6974\n",
      "Epoch 67/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5757 - accuracy: 0.6940\n",
      "Epoch 68/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5754 - accuracy: 0.6970\n",
      "Epoch 69/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5749 - accuracy: 0.6952\n",
      "Epoch 70/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5743 - accuracy: 0.6951\n",
      "Epoch 71/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5727 - accuracy: 0.6958\n",
      "Epoch 72/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5729 - accuracy: 0.6963\n",
      "Epoch 73/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5773 - accuracy: 0.6936\n",
      "Epoch 74/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5749 - accuracy: 0.6948\n",
      "Epoch 75/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5745 - accuracy: 0.6942\n",
      "Epoch 76/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5762 - accuracy: 0.6934\n",
      "Epoch 77/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5736 - accuracy: 0.6956\n",
      "Epoch 78/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5756 - accuracy: 0.6936\n",
      "Epoch 79/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6947\n",
      "Epoch 80/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5740 - accuracy: 0.6975\n",
      "Epoch 81/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5748 - accuracy: 0.6940\n",
      "Epoch 82/115\n",
      "1693/1693 [==============================] - 9s 6ms/step - loss: 0.5763 - accuracy: 0.6940\n",
      "Epoch 83/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5760 - accuracy: 0.6945\n",
      "Epoch 84/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5749 - accuracy: 0.6950\n",
      "Epoch 85/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5742 - accuracy: 0.6953\n",
      "Epoch 86/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5763 - accuracy: 0.6939\n",
      "Epoch 87/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5760 - accuracy: 0.6932\n",
      "Epoch 88/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5736 - accuracy: 0.6961\n",
      "Epoch 89/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5761 - accuracy: 0.6939\n",
      "Epoch 90/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5758 - accuracy: 0.6943\n",
      "Epoch 91/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5740 - accuracy: 0.6970\n",
      "Epoch 00091: early stopping\n",
      " 83/424 [====>.........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424/424 [==============================] - 1s 1ms/step\n",
      "Epoch 1/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.7082 - accuracy: 0.5514\n",
      "Epoch 2/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6565 - accuracy: 0.6407\n",
      "Epoch 3/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6367 - accuracy: 0.6625\n",
      "Epoch 4/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6216 - accuracy: 0.6705\n",
      "Epoch 5/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6096 - accuracy: 0.6775\n",
      "Epoch 6/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.6046 - accuracy: 0.6772\n",
      "Epoch 7/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5978 - accuracy: 0.6837\n",
      "Epoch 8/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5943 - accuracy: 0.6843\n",
      "Epoch 9/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5909 - accuracy: 0.6884\n",
      "Epoch 10/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5908 - accuracy: 0.6859\n",
      "Epoch 11/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5892 - accuracy: 0.6868\n",
      "Epoch 12/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5866 - accuracy: 0.6907\n",
      "Epoch 13/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5857 - accuracy: 0.6904\n",
      "Epoch 14/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5857 - accuracy: 0.6885\n",
      "Epoch 15/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5824 - accuracy: 0.6934\n",
      "Epoch 16/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5810 - accuracy: 0.6944\n",
      "Epoch 17/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5810 - accuracy: 0.6926\n",
      "Epoch 18/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5823 - accuracy: 0.6910\n",
      "Epoch 19/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5805 - accuracy: 0.6932\n",
      "Epoch 20/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5800 - accuracy: 0.6944\n",
      "Epoch 21/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5812 - accuracy: 0.6915\n",
      "Epoch 22/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5786 - accuracy: 0.6941\n",
      "Epoch 23/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5779 - accuracy: 0.6939\n",
      "Epoch 24/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5775 - accuracy: 0.6946\n",
      "Epoch 25/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5775 - accuracy: 0.6942\n",
      "Epoch 26/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5780 - accuracy: 0.6927\n",
      "Epoch 27/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5802 - accuracy: 0.6915\n",
      "Epoch 28/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5751 - accuracy: 0.6953\n",
      "Epoch 29/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5766 - accuracy: 0.6936\n",
      "Epoch 30/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5781 - accuracy: 0.6943\n",
      "Epoch 31/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5754 - accuracy: 0.6961\n",
      "Epoch 32/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5775 - accuracy: 0.6948\n",
      "Epoch 33/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5745 - accuracy: 0.6964\n",
      "Epoch 34/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5738 - accuracy: 0.6972\n",
      "Epoch 35/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5752 - accuracy: 0.6951\n",
      "Epoch 36/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5746 - accuracy: 0.6949\n",
      "Epoch 37/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5739 - accuracy: 0.6971\n",
      "Epoch 38/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5769 - accuracy: 0.6941\n",
      "Epoch 39/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5757 - accuracy: 0.6958\n",
      "Epoch 40/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5768 - accuracy: 0.6946\n",
      "Epoch 41/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5747 - accuracy: 0.6986\n",
      "Epoch 42/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5748 - accuracy: 0.6959\n",
      "Epoch 43/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5753 - accuracy: 0.6946\n",
      "Epoch 44/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5753 - accuracy: 0.6950\n",
      "Epoch 45/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5710 - accuracy: 0.6988\n",
      "Epoch 46/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5756 - accuracy: 0.6975\n",
      "Epoch 47/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5722 - accuracy: 0.6976\n",
      "Epoch 48/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5763 - accuracy: 0.6942\n",
      "Epoch 49/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5767 - accuracy: 0.6937\n",
      "Epoch 50/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5760 - accuracy: 0.6938\n",
      "Epoch 51/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5743 - accuracy: 0.6968\n",
      "Epoch 52/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5732 - accuracy: 0.6989\n",
      "Epoch 53/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5733 - accuracy: 0.6957\n",
      "Epoch 54/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5760 - accuracy: 0.6945\n",
      "Epoch 55/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5753 - accuracy: 0.6934\n",
      "Epoch 56/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5737 - accuracy: 0.6966\n",
      "Epoch 57/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5738 - accuracy: 0.6953\n",
      "Epoch 58/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5771 - accuracy: 0.6936\n",
      "Epoch 59/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5739 - accuracy: 0.6966\n",
      "Epoch 60/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5748 - accuracy: 0.6967\n",
      "Epoch 61/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5740 - accuracy: 0.6964\n",
      "Epoch 62/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5747 - accuracy: 0.6953\n",
      "Epoch 63/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5761 - accuracy: 0.6941\n",
      "Epoch 64/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5736 - accuracy: 0.6983\n",
      "Epoch 65/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5738 - accuracy: 0.6955\n",
      "Epoch 66/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5758 - accuracy: 0.6944\n",
      "Epoch 67/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5764 - accuracy: 0.6939\n",
      "Epoch 68/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5737 - accuracy: 0.6961\n",
      "Epoch 69/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5728 - accuracy: 0.6959\n",
      "Epoch 70/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5738 - accuracy: 0.6976\n",
      "Epoch 71/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5731 - accuracy: 0.6971\n",
      "Epoch 72/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5754 - accuracy: 0.6958\n",
      "Epoch 73/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5730 - accuracy: 0.6956\n",
      "Epoch 74/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6974\n",
      "Epoch 75/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6963\n",
      "Epoch 76/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5741 - accuracy: 0.6962\n",
      "Epoch 77/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5745 - accuracy: 0.6971\n",
      "Epoch 78/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5727 - accuracy: 0.6955\n",
      "Epoch 79/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5737 - accuracy: 0.6959\n",
      "Epoch 80/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5724 - accuracy: 0.6973\n",
      "Epoch 81/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5738 - accuracy: 0.6969\n",
      "Epoch 82/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5740 - accuracy: 0.6962\n",
      "Epoch 83/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5737 - accuracy: 0.6982\n",
      "Epoch 84/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5748 - accuracy: 0.6971\n",
      "Epoch 85/115\n",
      "1693/1693 [==============================] - 9s 6ms/step - loss: 0.5743 - accuracy: 0.6954\n",
      "Epoch 86/115\n",
      "1693/1693 [==============================] - 9s 6ms/step - loss: 0.5748 - accuracy: 0.6960\n",
      "Epoch 87/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5725 - accuracy: 0.6977\n",
      "Epoch 88/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5737 - accuracy: 0.6956\n",
      "Epoch 89/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6959\n",
      "Epoch 90/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5758 - accuracy: 0.6944\n",
      "Epoch 91/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5727 - accuracy: 0.6979\n",
      "Epoch 92/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5744 - accuracy: 0.6958\n",
      "Epoch 93/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5732 - accuracy: 0.6976\n",
      "Epoch 94/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5737 - accuracy: 0.6968\n",
      "Epoch 95/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5750 - accuracy: 0.6950\n",
      "Epoch 96/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5750 - accuracy: 0.6938\n",
      "Epoch 97/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5712 - accuracy: 0.6981\n",
      "Epoch 98/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5731 - accuracy: 0.6974\n",
      "Epoch 99/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5722 - accuracy: 0.6965\n",
      "Epoch 100/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5737 - accuracy: 0.6946\n",
      "Epoch 101/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5743 - accuracy: 0.6973\n",
      "Epoch 102/115\n",
      "1693/1693 [==============================] - 9s 5ms/step - loss: 0.5726 - accuracy: 0.6967\n",
      "Epoch 00102: early stopping\n",
      " 89/424 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424/424 [==============================] - 1s 2ms/step\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.6972  \u001b[0m | \u001b[95m 0.783   \u001b[0m | \u001b[95m 58.94   \u001b[0m | \u001b[95m 0.3668  \u001b[0m | \u001b[95m 0.3     \u001b[0m | \u001b[95m 115.1   \u001b[0m | \u001b[95m 1.023   \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 0.6382  \u001b[0m | \u001b[95m 148.1   \u001b[0m | \u001b[95m 0.8226  \u001b[0m | \u001b[95m 3.891   \u001b[0m |\n",
      "Epoch 1/125\n",
      "1849/1849 [==============================] - 11s 6ms/step - loss: 0.6402 - accuracy: 0.6347\n",
      "Epoch 2/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5832 - accuracy: 0.6920\n",
      "Epoch 3/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5772 - accuracy: 0.6928\n",
      "Epoch 4/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5774 - accuracy: 0.6956\n",
      "Epoch 5/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5800 - accuracy: 0.6921\n",
      "Epoch 6/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5783 - accuracy: 0.6943\n",
      "Epoch 7/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5752 - accuracy: 0.6951\n",
      "Epoch 8/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5764 - accuracy: 0.6950\n",
      "Epoch 9/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5766 - accuracy: 0.6955\n",
      "Epoch 10/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5752 - accuracy: 0.6972\n",
      "Epoch 11/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5763 - accuracy: 0.6963\n",
      "Epoch 12/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5764 - accuracy: 0.6935\n",
      "Epoch 13/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5738 - accuracy: 0.6985\n",
      "Epoch 14/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5715 - accuracy: 0.6991\n",
      "Epoch 15/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5747 - accuracy: 0.6949\n",
      "Epoch 16/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5724 - accuracy: 0.6994\n",
      "Epoch 17/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5736 - accuracy: 0.6953\n",
      "Epoch 18/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5754 - accuracy: 0.6962\n",
      "Epoch 19/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5756 - accuracy: 0.6963\n",
      "Epoch 20/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5778 - accuracy: 0.6939\n",
      "Epoch 21/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5736 - accuracy: 0.6983\n",
      "Epoch 22/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5725 - accuracy: 0.6982\n",
      "Epoch 23/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5740 - accuracy: 0.6976\n",
      "Epoch 24/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5742 - accuracy: 0.6948\n",
      "Epoch 25/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5749 - accuracy: 0.6964\n",
      "Epoch 26/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5720 - accuracy: 0.6981\n",
      "Epoch 27/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5735 - accuracy: 0.6979\n",
      "Epoch 28/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5744 - accuracy: 0.6957\n",
      "Epoch 29/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5748 - accuracy: 0.6954\n",
      "Epoch 30/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5738 - accuracy: 0.6987\n",
      "Epoch 31/125\n",
      "1849/1849 [==============================] - 11s 6ms/step - loss: 0.5704 - accuracy: 0.7001\n",
      "Epoch 32/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5721 - accuracy: 0.6969\n",
      "Epoch 33/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5739 - accuracy: 0.6976\n",
      "Epoch 34/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5726 - accuracy: 0.6986\n",
      "Epoch 35/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5726 - accuracy: 0.6986\n",
      "Epoch 36/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5749 - accuracy: 0.6958\n",
      "Epoch 37/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5742 - accuracy: 0.6963\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.01788458607705055.\n",
      "Epoch 38/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5705 - accuracy: 0.6990\n",
      "Epoch 39/125\n",
      "1849/1849 [==============================] - 11s 6ms/step - loss: 0.5718 - accuracy: 0.6981\n",
      "Epoch 40/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5722 - accuracy: 0.6980\n",
      "Epoch 41/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5716 - accuracy: 0.6949\n",
      "Epoch 42/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5717 - accuracy: 0.6978\n",
      "Epoch 43/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5727 - accuracy: 0.6959\n",
      "Epoch 44/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5717 - accuracy: 0.6986\n",
      "Epoch 45/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5691 - accuracy: 0.7003\n",
      "Epoch 46/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5706 - accuracy: 0.6984\n",
      "Epoch 47/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5692 - accuracy: 0.7015\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0035769172154101105.\n",
      "Epoch 48/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5716 - accuracy: 0.6982\n",
      "Epoch 49/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5731 - accuracy: 0.6949\n",
      "Epoch 50/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5724 - accuracy: 0.6955\n",
      "Epoch 51/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5725 - accuracy: 0.6986\n",
      "Epoch 52/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5758 - accuracy: 0.6942\n",
      "Epoch 53/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5683 - accuracy: 0.7007\n",
      "Epoch 54/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5707 - accuracy: 0.7002\n",
      "Epoch 55/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5714 - accuracy: 0.6997\n",
      "Epoch 56/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5708 - accuracy: 0.6987\n",
      "Epoch 57/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5706 - accuracy: 0.6984\n",
      "Epoch 58/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5719 - accuracy: 0.6980\n",
      "Epoch 59/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5704 - accuracy: 0.6988\n",
      "Epoch 60/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5720 - accuracy: 0.6977\n",
      "Epoch 61/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5729 - accuracy: 0.6965\n",
      "Epoch 62/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5743 - accuracy: 0.6952\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 63/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5720 - accuracy: 0.6986\n",
      "Epoch 64/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5716 - accuracy: 0.6979\n",
      "Epoch 65/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5701 - accuracy: 0.6999\n",
      "Epoch 00065: early stopping\n",
      " 82/463 [====>.........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463/463 [==============================] - 1s 1ms/step\n",
      "Epoch 1/125\n",
      "1849/1849 [==============================] - 11s 6ms/step - loss: 0.7425 - accuracy: 0.5408\n",
      "Epoch 2/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6883 - accuracy: 0.5457\n",
      "Epoch 3/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6814 - accuracy: 0.5500\n",
      "Epoch 4/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6771 - accuracy: 0.5724\n",
      "Epoch 5/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6731 - accuracy: 0.5909\n",
      "Epoch 6/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6694 - accuracy: 0.6033\n",
      "Epoch 7/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6651 - accuracy: 0.6155\n",
      "Epoch 8/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6608 - accuracy: 0.6250\n",
      "Epoch 9/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6587 - accuracy: 0.6264\n",
      "Epoch 10/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6545 - accuracy: 0.6343\n",
      "Epoch 11/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6524 - accuracy: 0.6353\n",
      "Epoch 12/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6486 - accuracy: 0.6398\n",
      "Epoch 13/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6468 - accuracy: 0.6397\n",
      "Epoch 14/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6427 - accuracy: 0.6454\n",
      "Epoch 15/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6405 - accuracy: 0.6455\n",
      "Epoch 16/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6380 - accuracy: 0.6489\n",
      "Epoch 17/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6349 - accuracy: 0.6524\n",
      "Epoch 18/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6328 - accuracy: 0.6527\n",
      "Epoch 19/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6305 - accuracy: 0.6533\n",
      "Epoch 20/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6280 - accuracy: 0.6553\n",
      "Epoch 21/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6268 - accuracy: 0.6576\n",
      "Epoch 22/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6246 - accuracy: 0.6611\n",
      "Epoch 23/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6233 - accuracy: 0.6599\n",
      "Epoch 24/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6202 - accuracy: 0.6621\n",
      "Epoch 25/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6197 - accuracy: 0.6620\n",
      "Epoch 26/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6177 - accuracy: 0.6638\n",
      "Epoch 27/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6145 - accuracy: 0.6676\n",
      "Epoch 28/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6132 - accuracy: 0.6687\n",
      "Epoch 29/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6115 - accuracy: 0.6715\n",
      "Epoch 30/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6083 - accuracy: 0.6751\n",
      "Epoch 31/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6091 - accuracy: 0.6714\n",
      "Epoch 32/125\n",
      "1849/1849 [==============================] - 11s 6ms/step - loss: 0.6071 - accuracy: 0.6741\n",
      "Epoch 33/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6069 - accuracy: 0.6731\n",
      "Epoch 34/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6062 - accuracy: 0.6757\n",
      "Epoch 35/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6050 - accuracy: 0.6747\n",
      "Epoch 36/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6024 - accuracy: 0.6774\n",
      "Epoch 37/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6011 - accuracy: 0.6796\n",
      "Epoch 38/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6007 - accuracy: 0.6791\n",
      "Epoch 39/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5984 - accuracy: 0.6816\n",
      "Epoch 40/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5987 - accuracy: 0.6807\n",
      "Epoch 41/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5974 - accuracy: 0.6816\n",
      "Epoch 42/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5968 - accuracy: 0.6805\n",
      "Epoch 43/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5966 - accuracy: 0.6818\n",
      "Epoch 44/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5947 - accuracy: 0.6833\n",
      "Epoch 45/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5947 - accuracy: 0.6838\n",
      "Epoch 46/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5938 - accuracy: 0.6836\n",
      "Epoch 47/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5926 - accuracy: 0.6857\n",
      "Epoch 48/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5935 - accuracy: 0.6835\n",
      "Epoch 49/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5919 - accuracy: 0.6842\n",
      "Epoch 50/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5939 - accuracy: 0.6825\n",
      "Epoch 51/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5907 - accuracy: 0.6854\n",
      "Epoch 52/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5897 - accuracy: 0.6857\n",
      "Epoch 53/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5901 - accuracy: 0.6865\n",
      "Epoch 54/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5912 - accuracy: 0.6865\n",
      "Epoch 55/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5919 - accuracy: 0.6845\n",
      "Epoch 56/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5885 - accuracy: 0.6866\n",
      "Epoch 57/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5899 - accuracy: 0.6864\n",
      "Epoch 58/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5890 - accuracy: 0.6878\n",
      "Epoch 59/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5870 - accuracy: 0.6896\n",
      "Epoch 60/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5891 - accuracy: 0.6868\n",
      "Epoch 61/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5875 - accuracy: 0.6895\n",
      "Epoch 62/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5888 - accuracy: 0.6873\n",
      "Epoch 63/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5864 - accuracy: 0.6897\n",
      "Epoch 64/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5892 - accuracy: 0.6858\n",
      "Epoch 65/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5880 - accuracy: 0.6879\n",
      "Epoch 66/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5896 - accuracy: 0.6840\n",
      "Epoch 67/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5883 - accuracy: 0.6850\n",
      "Epoch 68/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5864 - accuracy: 0.6871\n",
      "Epoch 69/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5859 - accuracy: 0.6893\n",
      "Epoch 70/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5892 - accuracy: 0.6870\n",
      "Epoch 71/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5866 - accuracy: 0.6872\n",
      "Epoch 72/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5878 - accuracy: 0.6864\n",
      "Epoch 73/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5837 - accuracy: 0.6900\n",
      "Epoch 74/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5879 - accuracy: 0.6882\n",
      "Epoch 75/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5842 - accuracy: 0.6894\n",
      "Epoch 76/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5850 - accuracy: 0.6904\n",
      "Epoch 77/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5842 - accuracy: 0.6908\n",
      "Epoch 78/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5856 - accuracy: 0.6887\n",
      "Epoch 79/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5841 - accuracy: 0.6905\n",
      "Epoch 80/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5847 - accuracy: 0.6905\n",
      "Epoch 81/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5863 - accuracy: 0.6877\n",
      "Epoch 82/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5848 - accuracy: 0.6900\n",
      "Epoch 83/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5852 - accuracy: 0.6879\n",
      "Epoch 84/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5861 - accuracy: 0.6881\n",
      "Epoch 85/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5823 - accuracy: 0.6926\n",
      "Epoch 86/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5844 - accuracy: 0.6905\n",
      "Epoch 87/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5839 - accuracy: 0.6880\n",
      "Epoch 88/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5853 - accuracy: 0.6891\n",
      "Epoch 89/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5832 - accuracy: 0.6896\n",
      "Epoch 90/125\n",
      "1849/1849 [==============================] - 11s 6ms/step - loss: 0.5830 - accuracy: 0.6911\n",
      "Epoch 91/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5844 - accuracy: 0.6887\n",
      "Epoch 92/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5810 - accuracy: 0.6924\n",
      "Epoch 93/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5842 - accuracy: 0.6887\n",
      "Epoch 94/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5836 - accuracy: 0.6888\n",
      "Epoch 95/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5819 - accuracy: 0.6927\n",
      "Epoch 96/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5856 - accuracy: 0.6871\n",
      "Epoch 97/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5823 - accuracy: 0.6907\n",
      "Epoch 98/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5838 - accuracy: 0.6896\n",
      "Epoch 99/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5823 - accuracy: 0.6903\n",
      "Epoch 100/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5845 - accuracy: 0.6887\n",
      "Epoch 101/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5820 - accuracy: 0.6922\n",
      "Epoch 102/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5837 - accuracy: 0.6903\n",
      "Epoch 103/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5817 - accuracy: 0.6922\n",
      "Epoch 104/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5809 - accuracy: 0.6905\n",
      "Epoch 105/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5815 - accuracy: 0.6894\n",
      "Epoch 106/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5842 - accuracy: 0.6895\n",
      "Epoch 107/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5825 - accuracy: 0.6911\n",
      "Epoch 108/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5818 - accuracy: 0.6906\n",
      "Epoch 109/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5810 - accuracy: 0.6910\n",
      "Epoch 110/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5816 - accuracy: 0.6921\n",
      "Epoch 111/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5830 - accuracy: 0.6918\n",
      "Epoch 112/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5818 - accuracy: 0.6903\n",
      "Epoch 113/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5817 - accuracy: 0.6917\n",
      "Epoch 114/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5823 - accuracy: 0.6902\n",
      "Epoch 115/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5837 - accuracy: 0.6897\n",
      "Epoch 116/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5820 - accuracy: 0.6930\n",
      "Epoch 117/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5812 - accuracy: 0.6930\n",
      "Epoch 118/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5822 - accuracy: 0.6879\n",
      "Epoch 119/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5834 - accuracy: 0.6887\n",
      "Epoch 120/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5822 - accuracy: 0.6918\n",
      "Epoch 121/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5808 - accuracy: 0.6927\n",
      "Epoch 122/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5843 - accuracy: 0.6871\n",
      "Epoch 123/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5815 - accuracy: 0.6914\n",
      "Epoch 124/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5813 - accuracy: 0.6927\n",
      "Epoch 125/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5808 - accuracy: 0.6920\n",
      " 92/463 [====>.........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463/463 [==============================] - 1s 1ms/step\n",
      "Epoch 1/125\n",
      "1849/1849 [==============================] - 11s 6ms/step - loss: 0.7159 - accuracy: 0.4622\n",
      "Epoch 2/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6889 - accuracy: 0.5439\n",
      "Epoch 3/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6830 - accuracy: 0.5609\n",
      "Epoch 4/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6781 - accuracy: 0.5772\n",
      "Epoch 5/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6733 - accuracy: 0.5912\n",
      "Epoch 6/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6680 - accuracy: 0.6091\n",
      "Epoch 7/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6643 - accuracy: 0.6188\n",
      "Epoch 8/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6585 - accuracy: 0.6294\n",
      "Epoch 9/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6547 - accuracy: 0.6340\n",
      "Epoch 10/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6488 - accuracy: 0.6450\n",
      "Epoch 11/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6458 - accuracy: 0.6457\n",
      "Epoch 12/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6425 - accuracy: 0.6519\n",
      "Epoch 13/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6396 - accuracy: 0.6505\n",
      "Epoch 14/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6353 - accuracy: 0.6543\n",
      "Epoch 15/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6325 - accuracy: 0.6580\n",
      "Epoch 16/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6284 - accuracy: 0.6621\n",
      "Epoch 17/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6252 - accuracy: 0.6628\n",
      "Epoch 18/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6227 - accuracy: 0.6655\n",
      "Epoch 19/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6212 - accuracy: 0.6664\n",
      "Epoch 20/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6189 - accuracy: 0.6657\n",
      "Epoch 21/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6164 - accuracy: 0.6696\n",
      "Epoch 22/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6146 - accuracy: 0.6686\n",
      "Epoch 23/125\n",
      "1849/1849 [==============================] - 11s 6ms/step - loss: 0.6112 - accuracy: 0.6727\n",
      "Epoch 24/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.6117 - accuracy: 0.6710\n",
      "Epoch 25/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.6095 - accuracy: 0.6730\n",
      "Epoch 26/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6067 - accuracy: 0.6755\n",
      "Epoch 27/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6066 - accuracy: 0.6749\n",
      "Epoch 28/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6033 - accuracy: 0.6804\n",
      "Epoch 29/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6026 - accuracy: 0.6801\n",
      "Epoch 30/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6022 - accuracy: 0.6785\n",
      "Epoch 31/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6008 - accuracy: 0.6795\n",
      "Epoch 32/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5997 - accuracy: 0.6804\n",
      "Epoch 33/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5993 - accuracy: 0.6800\n",
      "Epoch 34/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5978 - accuracy: 0.6803\n",
      "Epoch 35/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5970 - accuracy: 0.6829\n",
      "Epoch 36/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5958 - accuracy: 0.6838\n",
      "Epoch 37/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5980 - accuracy: 0.6775\n",
      "Epoch 38/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5968 - accuracy: 0.6815\n",
      "Epoch 39/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5961 - accuracy: 0.6827\n",
      "Epoch 40/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5943 - accuracy: 0.6839\n",
      "Epoch 41/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5939 - accuracy: 0.6822\n",
      "Epoch 42/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5945 - accuracy: 0.6832\n",
      "Epoch 43/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5922 - accuracy: 0.6837\n",
      "Epoch 44/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5924 - accuracy: 0.6830\n",
      "Epoch 45/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5924 - accuracy: 0.6851\n",
      "Epoch 46/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5928 - accuracy: 0.6850\n",
      "Epoch 47/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5907 - accuracy: 0.6843\n",
      "Epoch 48/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5925 - accuracy: 0.6834\n",
      "Epoch 49/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5894 - accuracy: 0.6864\n",
      "Epoch 50/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5909 - accuracy: 0.6856\n",
      "Epoch 51/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5894 - accuracy: 0.6865\n",
      "Epoch 52/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5902 - accuracy: 0.6850\n",
      "Epoch 53/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5903 - accuracy: 0.6870\n",
      "Epoch 54/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5882 - accuracy: 0.6878\n",
      "Epoch 55/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5886 - accuracy: 0.6881\n",
      "Epoch 56/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5893 - accuracy: 0.6873\n",
      "Epoch 57/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5895 - accuracy: 0.6867\n",
      "Epoch 58/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5875 - accuracy: 0.6900\n",
      "Epoch 59/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5890 - accuracy: 0.6857\n",
      "Epoch 60/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5880 - accuracy: 0.6875\n",
      "Epoch 61/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5865 - accuracy: 0.6901\n",
      "Epoch 62/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5867 - accuracy: 0.6886\n",
      "Epoch 63/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5892 - accuracy: 0.6873\n",
      "Epoch 64/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5867 - accuracy: 0.6865\n",
      "Epoch 65/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5879 - accuracy: 0.6881\n",
      "Epoch 66/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5880 - accuracy: 0.6881\n",
      "Epoch 67/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5854 - accuracy: 0.6895\n",
      "Epoch 68/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5854 - accuracy: 0.6913\n",
      "Epoch 69/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5877 - accuracy: 0.6875\n",
      "Epoch 70/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5851 - accuracy: 0.6907\n",
      "Epoch 71/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5856 - accuracy: 0.6901\n",
      "Epoch 72/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5878 - accuracy: 0.6867\n",
      "Epoch 73/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5849 - accuracy: 0.6891\n",
      "Epoch 74/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5876 - accuracy: 0.6886\n",
      "Epoch 75/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5873 - accuracy: 0.6878\n",
      "Epoch 76/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5846 - accuracy: 0.6887\n",
      "Epoch 77/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5847 - accuracy: 0.6887\n",
      "Epoch 78/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5882 - accuracy: 0.6864\n",
      "Epoch 79/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5842 - accuracy: 0.6897\n",
      "Epoch 80/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5850 - accuracy: 0.6882\n",
      "Epoch 81/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5846 - accuracy: 0.6891\n",
      "Epoch 82/125\n",
      "1849/1849 [==============================] - 11s 6ms/step - loss: 0.5843 - accuracy: 0.6898\n",
      "Epoch 83/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5859 - accuracy: 0.6879\n",
      "Epoch 84/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5842 - accuracy: 0.6909\n",
      "Epoch 85/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5855 - accuracy: 0.6880\n",
      "Epoch 86/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5833 - accuracy: 0.6906\n",
      "Epoch 87/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5840 - accuracy: 0.6916\n",
      "Epoch 88/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5844 - accuracy: 0.6888\n",
      "Epoch 89/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5839 - accuracy: 0.6899\n",
      "Epoch 90/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5865 - accuracy: 0.6882\n",
      "Epoch 91/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5839 - accuracy: 0.6901\n",
      "Epoch 92/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5852 - accuracy: 0.6892\n",
      "Epoch 93/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5844 - accuracy: 0.6893\n",
      "Epoch 94/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5832 - accuracy: 0.6911\n",
      "Epoch 95/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5825 - accuracy: 0.6893\n",
      "Epoch 96/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5841 - accuracy: 0.6910\n",
      "Epoch 97/125\n",
      "1849/1849 [==============================] - 11s 6ms/step - loss: 0.5851 - accuracy: 0.6880\n",
      "Epoch 98/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5836 - accuracy: 0.6895\n",
      "Epoch 99/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5860 - accuracy: 0.6887\n",
      "Epoch 100/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5855 - accuracy: 0.6890\n",
      "Epoch 101/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5831 - accuracy: 0.6913\n",
      "Epoch 102/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5823 - accuracy: 0.6934\n",
      "Epoch 103/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5827 - accuracy: 0.6902\n",
      "Epoch 104/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5849 - accuracy: 0.6890\n",
      "Epoch 105/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5843 - accuracy: 0.6901\n",
      "Epoch 106/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5808 - accuracy: 0.6928\n",
      "Epoch 107/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5832 - accuracy: 0.6912\n",
      "Epoch 108/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5849 - accuracy: 0.6899\n",
      "Epoch 109/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5836 - accuracy: 0.6910\n",
      "Epoch 110/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5832 - accuracy: 0.6892\n",
      "Epoch 111/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5859 - accuracy: 0.6880\n",
      "Epoch 112/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5826 - accuracy: 0.6920\n",
      "Epoch 113/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5830 - accuracy: 0.6896\n",
      "Epoch 114/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5838 - accuracy: 0.6894\n",
      "Epoch 115/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5838 - accuracy: 0.6904\n",
      "Epoch 116/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5813 - accuracy: 0.6931\n",
      "Epoch 117/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5815 - accuracy: 0.6918\n",
      "Epoch 118/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5831 - accuracy: 0.6915\n",
      "Epoch 119/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5800 - accuracy: 0.6937\n",
      "Epoch 120/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5831 - accuracy: 0.6897\n",
      "Epoch 121/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5844 - accuracy: 0.6910\n",
      "Epoch 122/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5824 - accuracy: 0.6911\n",
      "Epoch 123/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5824 - accuracy: 0.6926\n",
      "Epoch 124/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5820 - accuracy: 0.6915\n",
      "Epoch 125/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5841 - accuracy: 0.6881\n",
      " 93/463 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463/463 [==============================] - 1s 1ms/step\n",
      "Epoch 1/125\n",
      "1849/1849 [==============================] - 11s 6ms/step - loss: 0.7103 - accuracy: 0.5434\n",
      "Epoch 2/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6902 - accuracy: 0.5409\n",
      "Epoch 3/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6849 - accuracy: 0.5417\n",
      "Epoch 4/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.6811 - accuracy: 0.5524\n",
      "Epoch 5/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6769 - accuracy: 0.5700\n",
      "Epoch 6/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6737 - accuracy: 0.5825\n",
      "Epoch 7/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6694 - accuracy: 0.6009\n",
      "Epoch 8/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.6659 - accuracy: 0.6112\n",
      "Epoch 9/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6628 - accuracy: 0.6185\n",
      "Epoch 10/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6592 - accuracy: 0.6274\n",
      "Epoch 11/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6561 - accuracy: 0.6327\n",
      "Epoch 12/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6530 - accuracy: 0.6365\n",
      "Epoch 13/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6502 - accuracy: 0.6391\n",
      "Epoch 14/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6465 - accuracy: 0.6418\n",
      "Epoch 15/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6429 - accuracy: 0.6476\n",
      "Epoch 16/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6408 - accuracy: 0.6482\n",
      "Epoch 17/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.6388 - accuracy: 0.6495\n",
      "Epoch 18/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6373 - accuracy: 0.6498\n",
      "Epoch 19/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6329 - accuracy: 0.6540\n",
      "Epoch 20/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6312 - accuracy: 0.6520\n",
      "Epoch 21/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6291 - accuracy: 0.6560\n",
      "Epoch 22/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6262 - accuracy: 0.6577\n",
      "Epoch 23/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6256 - accuracy: 0.6596\n",
      "Epoch 24/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6234 - accuracy: 0.6592\n",
      "Epoch 25/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6208 - accuracy: 0.6638\n",
      "Epoch 26/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6194 - accuracy: 0.6660\n",
      "Epoch 27/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6166 - accuracy: 0.6679\n",
      "Epoch 28/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6173 - accuracy: 0.6655\n",
      "Epoch 29/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.6149 - accuracy: 0.6676\n",
      "Epoch 30/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6137 - accuracy: 0.6719\n",
      "Epoch 31/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6124 - accuracy: 0.6707\n",
      "Epoch 32/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6109 - accuracy: 0.6726\n",
      "Epoch 33/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6103 - accuracy: 0.6719\n",
      "Epoch 34/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6093 - accuracy: 0.6712\n",
      "Epoch 35/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.6079 - accuracy: 0.6714\n",
      "Epoch 36/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6090 - accuracy: 0.6721\n",
      "Epoch 37/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6072 - accuracy: 0.6722\n",
      "Epoch 38/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.6061 - accuracy: 0.6768\n",
      "Epoch 39/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.6042 - accuracy: 0.6762\n",
      "Epoch 40/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6032 - accuracy: 0.6766\n",
      "Epoch 41/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6032 - accuracy: 0.6748\n",
      "Epoch 42/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6037 - accuracy: 0.6774\n",
      "Epoch 43/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6037 - accuracy: 0.6774\n",
      "Epoch 44/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.6028 - accuracy: 0.6765\n",
      "Epoch 45/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.6027 - accuracy: 0.6761\n",
      "Epoch 46/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.6009 - accuracy: 0.6776\n",
      "Epoch 47/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5989 - accuracy: 0.6801\n",
      "Epoch 48/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.6016 - accuracy: 0.6770\n",
      "Epoch 49/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5987 - accuracy: 0.6801\n",
      "Epoch 50/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5990 - accuracy: 0.6791\n",
      "Epoch 51/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5999 - accuracy: 0.6786\n",
      "Epoch 52/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5984 - accuracy: 0.6795\n",
      "Epoch 53/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5986 - accuracy: 0.6784\n",
      "Epoch 54/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5981 - accuracy: 0.6791\n",
      "Epoch 55/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5966 - accuracy: 0.6816\n",
      "Epoch 56/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5966 - accuracy: 0.6798\n",
      "Epoch 57/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5945 - accuracy: 0.6829\n",
      "Epoch 58/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5971 - accuracy: 0.6791\n",
      "Epoch 59/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5948 - accuracy: 0.6803\n",
      "Epoch 60/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5947 - accuracy: 0.6803\n",
      "Epoch 61/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5932 - accuracy: 0.6837\n",
      "Epoch 62/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5943 - accuracy: 0.6805\n",
      "Epoch 63/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5957 - accuracy: 0.6819\n",
      "Epoch 64/125\n",
      "1849/1849 [==============================] - 10s 5ms/step - loss: 0.5949 - accuracy: 0.6813\n",
      "Epoch 65/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5930 - accuracy: 0.6821\n",
      "Epoch 66/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5918 - accuracy: 0.6841\n",
      "Epoch 67/125\n",
      "1849/1849 [==============================] - 11s 6ms/step - loss: 0.5920 - accuracy: 0.6849\n",
      "Epoch 68/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5931 - accuracy: 0.6838\n",
      "Epoch 69/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5922 - accuracy: 0.6831\n",
      "Epoch 70/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5893 - accuracy: 0.6874\n",
      "Epoch 71/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5901 - accuracy: 0.6859\n",
      "Epoch 72/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5917 - accuracy: 0.6815\n",
      "Epoch 73/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5894 - accuracy: 0.6866\n",
      "Epoch 74/125\n",
      "1849/1849 [==============================] - 11s 6ms/step - loss: 0.5908 - accuracy: 0.6835\n",
      "Epoch 75/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5883 - accuracy: 0.6868\n",
      "Epoch 76/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5918 - accuracy: 0.6818\n",
      "Epoch 77/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5919 - accuracy: 0.6830\n",
      "Epoch 78/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5892 - accuracy: 0.6856\n",
      "Epoch 79/125\n",
      "1849/1849 [==============================] - 11s 6ms/step - loss: 0.5881 - accuracy: 0.6865\n",
      "Epoch 80/125\n",
      "1849/1849 [==============================] - 11s 6ms/step - loss: 0.5864 - accuracy: 0.6892\n",
      "Epoch 81/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5870 - accuracy: 0.6879\n",
      "Epoch 82/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5884 - accuracy: 0.6875\n",
      "Epoch 83/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5893 - accuracy: 0.6867\n",
      "Epoch 84/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5900 - accuracy: 0.6858\n",
      "Epoch 85/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5904 - accuracy: 0.6847\n",
      "Epoch 86/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5885 - accuracy: 0.6878\n",
      "Epoch 87/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5899 - accuracy: 0.6857\n",
      "Epoch 88/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5877 - accuracy: 0.6879\n",
      "Epoch 89/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5873 - accuracy: 0.6877\n",
      "Epoch 90/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5890 - accuracy: 0.6843\n",
      "Epoch 91/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5861 - accuracy: 0.6886\n",
      "Epoch 92/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5884 - accuracy: 0.6865\n",
      "Epoch 93/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5886 - accuracy: 0.6860\n",
      "Epoch 94/125\n",
      "1849/1849 [==============================] - 10s 6ms/step - loss: 0.5885 - accuracy: 0.6876\n",
      "Epoch 95/125\n",
      " 827/1849 [============>.................] - ETA: 5s - loss: 0.5885 - accuracy: 0.6829"
     ]
    }
   ],
   "source": [
    "nifty200_train_data = pd.read_csv(\"nifty200_train_data.csv\", index_col = 0)\n",
    "nifty200_train_data.dropna(inplace = True)\n",
    "X, y = nifty200_train_data.values[:,:-1],nifty200_train_data.values[:,-1]\n",
    "score_acc = make_scorer(accuracy_score)\n",
    "\n",
    "X = X.astype('float32')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# Create function\n",
    "def nn_cl_bo2(neurons, activation, optimizer, learning_rate, batch_size, epochs,\n",
    "              layers1, layers2, normalization, dropout, dropout_rate):\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad']\n",
    "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                 'Adagrad':Adagrad(lr=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'tanh', LeakyReLU]\n",
    "    neurons = int(round(neurons))\n",
    "    activation = activationL[int(round(activation))]\n",
    "    optimizer = optimizerD[optimizerL[int(round(optimizer))]]\n",
    "    batch_size = int(round(batch_size))\n",
    "    epochs = int(round(epochs))\n",
    "    layers1 = int(round(layers1))\n",
    "    layers2 = int(round(layers2))\n",
    "    def nn_cl_fun():\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(neurons, input_shape=(n_features,), activation=activation))\n",
    "        if normalization > 0.5:\n",
    "            nn.add(BatchNormalization())\n",
    "        for i in range(layers1):\n",
    "            nn.add(Dense(neurons, activation=activation))\n",
    "        if dropout > 0.5:\n",
    "            nn.add(Dropout(dropout_rate, seed=42))\n",
    "        for i in range(layers2):\n",
    "            nn.add(Dense(neurons, activation=activation))\n",
    "        nn.add(Dense(1, activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='loss', verbose=1, mode='min', patience=10)\n",
    "    rl = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=7, min_lr=0.001, verbose = 1)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(nn, X_train, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es, rl]}).mean()\n",
    "    return score\n",
    "\n",
    "params_nn2 ={\n",
    "    'neurons': (10, 200),\n",
    "    'activation':(0, 3),\n",
    "    'optimizer':(0,4),\n",
    "    'learning_rate':(0.001, 1),\n",
    "    'batch_size':(16, 64),\n",
    "    'epochs':(30, 150),\n",
    "    'layers1':(1,3),\n",
    "    'layers2':(1,3),\n",
    "    'normalization':(0,1),\n",
    "    'dropout':(0,1),\n",
    "    'dropout_rate':(0,0.3)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=42)\n",
    "nn_bo.maximize(init_points=10, n_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_nn_ = nn_bo.max['params']\n",
    "learning_rate = params_nn_['learning_rate']\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential', LeakyReLU,'relu']\n",
    "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
    "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
    "params_nn_['epochs'] = round(params_nn_['epochs'])\n",
    "params_nn_['layers1'] = round(params_nn_['layers1'])\n",
    "params_nn_['layers2'] = round(params_nn_['layers2'])\n",
    "params_nn_['neurons'] = round(params_nn_['neurons'])\n",
    "optimizerL = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','Adam']\n",
    "optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "             'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "             'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "             'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "params_nn_['optimizer'] = optimizerD[optimizerL[round(params_nn_['optimizer'])]]\n",
    "params_nn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note: </b>I realized later that this method of backtesting is incorrect. The model takes the past 5 days of data as input for one prediction. The logic for calculations done to find profits for this backtest were found to be erroneous. Hence, do no consider these. <br>\n",
    "These results have been kept for documentation only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ACC.NS\n",
      "2 ABBOTINDIA.NS\n",
      "3 ADANIENT.NS\n",
      "4 ADANIGREEN.NS\n",
      "5 ADANITRANS.NS\n",
      "6 ALKEM.NS\n",
      "7 AMBUJACEM.NS\n",
      "8 APOLLOHOSP.NS\n",
      "9 AUROPHARMA.NS\n",
      "10 DMART.NS\n",
      "11 BAJAJHLDNG.NS\n",
      "12 BANDHANBNK.NS\n",
      "13 BERGEPAINT.NS\n",
      "14 BIOCON.NS\n",
      "15 BOSCHLTD.NS\n",
      "16 CADILAHC.NS\n",
      "Could not run for  CADILAHC.NS\n",
      "16 COLPAL.NS\n",
      "17 DLF.NS\n",
      "18 DABUR.NS\n",
      "19 GAIL.NS\n",
      "20 GODREJCP.NS\n",
      "21 HDFCAMC.NS\n",
      "22 HAVELLS.NS\n",
      "23 HINDPETRO.NS\n",
      "24 ICICIGI.NS\n",
      "25 ICICIPRULI.NS\n",
      "26 IGL.NS\n",
      "27 INDUSTOWER.NS\n",
      "28 NAUKRI.NS\n",
      "29 INDIGO.NS\n",
      "30 JUBLFOOD.NS\n",
      "31 LTI.NS\n",
      "32 LUPIN.NS\n",
      "33 MRF.NS\n",
      "34 MARICO.NS\n",
      "35 MOTHERSUMI.NS\n",
      "36 MUTHOOTFIN.NS\n",
      "37 NMDC.NS\n",
      "38 PETRONET.NS\n",
      "39 PIDILITIND.NS\n",
      "40 PEL.NS\n",
      "41 PGHH.NS\n",
      "42 PNB.NS\n",
      "43 SBICARD.NS\n",
      "44 SIEMENS.NS\n",
      "45 TORNTPHARM.NS\n",
      "46 UBL.NS\n",
      "47 MCDOWELL-N.NS\n",
      "48 VEDL.NS\n",
      "49 YESBANK.NS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_length_of_data</th>\n",
       "      <th>length_of_high_confidence_data</th>\n",
       "      <th>mean_profit_per_trade</th>\n",
       "      <th>layman_profit</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>4.900000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>892.551020</td>\n",
       "      <td>892.346939</td>\n",
       "      <td>4.450496</td>\n",
       "      <td>12033.947247</td>\n",
       "      <td>1.228670e+36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>451.678723</td>\n",
       "      <td>451.924245</td>\n",
       "      <td>1.415099</td>\n",
       "      <td>30030.711415</td>\n",
       "      <td>7.934383e+36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>1.100784</td>\n",
       "      <td>-56.060885</td>\n",
       "      <td>6.935568e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>495.000000</td>\n",
       "      <td>495.000000</td>\n",
       "      <td>3.621473</td>\n",
       "      <td>459.812556</td>\n",
       "      <td>8.369040e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>970.000000</td>\n",
       "      <td>970.000000</td>\n",
       "      <td>4.119234</td>\n",
       "      <td>1984.375000</td>\n",
       "      <td>1.415094e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>5.102058</td>\n",
       "      <td>9672.078323</td>\n",
       "      <td>1.056188e+22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2340.000000</td>\n",
       "      <td>2340.000000</td>\n",
       "      <td>9.956630</td>\n",
       "      <td>186217.171075</td>\n",
       "      <td>5.543527e+37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_length_of_data  length_of_high_confidence_data  \\\n",
       "count             49.000000                       49.000000   \n",
       "mean             892.551020                      892.346939   \n",
       "std              451.678723                      451.924245   \n",
       "min              120.000000                      120.000000   \n",
       "25%              495.000000                      495.000000   \n",
       "50%              970.000000                      970.000000   \n",
       "75%             1080.000000                     1080.000000   \n",
       "max             2340.000000                     2340.000000   \n",
       "\n",
       "       mean_profit_per_trade  layman_profit       returns  \n",
       "count              49.000000      49.000000  4.900000e+01  \n",
       "mean                4.450496   12033.947247  1.228670e+36  \n",
       "std                 1.415099   30030.711415  7.934383e+36  \n",
       "min                 1.100784     -56.060885  6.935568e+03  \n",
       "25%                 3.621473     459.812556  8.369040e+12  \n",
       "50%                 4.119234    1984.375000  1.415094e+17  \n",
       "75%                 5.102058    9672.078323  1.056188e+22  \n",
       "max                 9.956630  186217.171075  5.543527e+37  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nifty_tickers = get_tickers(2)\n",
    "nifty_stats_dict = {}    \n",
    "i = 1\n",
    "total = 0\n",
    "for t in nifty_tickers:\n",
    "    print(i, t)\n",
    "    try:\n",
    "        nifty_backtest_df, length_t = get_backtest_df(\"ALL\", t, 0.75, 0.5, train_start_date, test_end_date, 5)\n",
    "        total += length_t\n",
    "#         nifty_backtest_df = nifty_backtest_df[nifty_backtest_df['buy_sell_signal_from_model'] == 1]\n",
    "    except:\n",
    "        print(\"Could not run for \", t)\n",
    "        continue\n",
    "    temp_dict = get_backtest_stats(nifty_backtest_df)\n",
    "    nifty_stats_dict[t] = temp_dict\n",
    "    i = i + 1\n",
    "    \n",
    "    \n",
    "    \n",
    "nifty_stats_df_from_dict = pd.DataFrame.from_dict(nifty_stats_dict, orient='index')\n",
    "nifty_stats_df_from_dict.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nifty_stats_df_from_dict.to_csv(\"nifty_200_conf_50_ALL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_length_of_data</th>\n",
       "      <th>length_of_high_confidence_data</th>\n",
       "      <th>mean_profit_per_trade</th>\n",
       "      <th>layman_profit</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACC.NS</th>\n",
       "      <td>1075</td>\n",
       "      <td>1075</td>\n",
       "      <td>3.429617</td>\n",
       "      <td>1531.510824</td>\n",
       "      <td>2.746118e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABBOTINDIA.NS</th>\n",
       "      <td>2340</td>\n",
       "      <td>2340</td>\n",
       "      <td>1.100784</td>\n",
       "      <td>5824.472656</td>\n",
       "      <td>1.397144e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADANIENT.NS</th>\n",
       "      <td>1045</td>\n",
       "      <td>1045</td>\n",
       "      <td>7.503856</td>\n",
       "      <td>186217.171075</td>\n",
       "      <td>3.564628e+33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADANIGREEN.NS</th>\n",
       "      <td>285</td>\n",
       "      <td>285</td>\n",
       "      <td>9.956630</td>\n",
       "      <td>6864.630140</td>\n",
       "      <td>2.124048e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADANITRANS.NS</th>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>7.133129</td>\n",
       "      <td>8571.327088</td>\n",
       "      <td>1.027537e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALKEM.NS</th>\n",
       "      <td>325</td>\n",
       "      <td>325</td>\n",
       "      <td>3.473691</td>\n",
       "      <td>168.762441</td>\n",
       "      <td>5.451545e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMBUJACEM.NS</th>\n",
       "      <td>945</td>\n",
       "      <td>945</td>\n",
       "      <td>3.370343</td>\n",
       "      <td>1449.609550</td>\n",
       "      <td>2.226889e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APOLLOHOSP.NS</th>\n",
       "      <td>925</td>\n",
       "      <td>925</td>\n",
       "      <td>4.070937</td>\n",
       "      <td>9883.190206</td>\n",
       "      <td>3.982650e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROPHARMA.NS</th>\n",
       "      <td>1495</td>\n",
       "      <td>1495</td>\n",
       "      <td>5.695153</td>\n",
       "      <td>14979.557154</td>\n",
       "      <td>4.765998e+36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DMART.NS</th>\n",
       "      <td>305</td>\n",
       "      <td>305</td>\n",
       "      <td>3.841281</td>\n",
       "      <td>459.812556</td>\n",
       "      <td>7.626889e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAJAJHLDNG.NS</th>\n",
       "      <td>1015</td>\n",
       "      <td>1015</td>\n",
       "      <td>3.676083</td>\n",
       "      <td>1476.645707</td>\n",
       "      <td>1.415094e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BANDHANBNK.NS</th>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>6.343244</td>\n",
       "      <td>-37.636112</td>\n",
       "      <td>2.470280e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERGEPAINT.NS</th>\n",
       "      <td>1150</td>\n",
       "      <td>1150</td>\n",
       "      <td>4.044372</td>\n",
       "      <td>31246.572983</td>\n",
       "      <td>1.431911e+21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIOCON.NS</th>\n",
       "      <td>935</td>\n",
       "      <td>935</td>\n",
       "      <td>3.698039</td>\n",
       "      <td>793.370519</td>\n",
       "      <td>2.250349e+16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOSCHLTD.NS</th>\n",
       "      <td>990</td>\n",
       "      <td>990</td>\n",
       "      <td>3.671092</td>\n",
       "      <td>4714.438244</td>\n",
       "      <td>1.570601e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COLPAL.NS</th>\n",
       "      <td>985</td>\n",
       "      <td>985</td>\n",
       "      <td>2.995188</td>\n",
       "      <td>2273.851816</td>\n",
       "      <td>2.503498e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLF.NS</th>\n",
       "      <td>800</td>\n",
       "      <td>800</td>\n",
       "      <td>5.334942</td>\n",
       "      <td>-43.195621</td>\n",
       "      <td>1.939968e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DABUR.NS</th>\n",
       "      <td>980</td>\n",
       "      <td>980</td>\n",
       "      <td>3.239398</td>\n",
       "      <td>6460.540838</td>\n",
       "      <td>1.978823e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAIL.NS</th>\n",
       "      <td>1590</td>\n",
       "      <td>1590</td>\n",
       "      <td>3.496580</td>\n",
       "      <td>464.608189</td>\n",
       "      <td>3.287930e+24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GODREJCP.NS</th>\n",
       "      <td>1075</td>\n",
       "      <td>1075</td>\n",
       "      <td>3.970098</td>\n",
       "      <td>9672.078323</td>\n",
       "      <td>5.874147e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HDFCAMC.NS</th>\n",
       "      <td>240</td>\n",
       "      <td>240</td>\n",
       "      <td>3.488963</td>\n",
       "      <td>56.943836</td>\n",
       "      <td>3.087227e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HAVELLS.NS</th>\n",
       "      <td>1080</td>\n",
       "      <td>1080</td>\n",
       "      <td>4.708848</td>\n",
       "      <td>83884.326167</td>\n",
       "      <td>8.660574e+22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HINDPETRO.NS</th>\n",
       "      <td>1320</td>\n",
       "      <td>1320</td>\n",
       "      <td>5.272777</td>\n",
       "      <td>689.749981</td>\n",
       "      <td>3.498494e+29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICICIGI.NS</th>\n",
       "      <td>235</td>\n",
       "      <td>235</td>\n",
       "      <td>3.198322</td>\n",
       "      <td>86.457393</td>\n",
       "      <td>1.435652e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICICIPRULI.NS</th>\n",
       "      <td>285</td>\n",
       "      <td>285</td>\n",
       "      <td>3.591034</td>\n",
       "      <td>74.256602</td>\n",
       "      <td>1.978787e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGL.NS</th>\n",
       "      <td>880</td>\n",
       "      <td>880</td>\n",
       "      <td>3.621473</td>\n",
       "      <td>1936.844979</td>\n",
       "      <td>2.071757e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUSTOWER.NS</th>\n",
       "      <td>395</td>\n",
       "      <td>390</td>\n",
       "      <td>4.436630</td>\n",
       "      <td>-1.414631</td>\n",
       "      <td>1.461801e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAUKRI.NS</th>\n",
       "      <td>770</td>\n",
       "      <td>770</td>\n",
       "      <td>4.860785</td>\n",
       "      <td>2814.955625</td>\n",
       "      <td>2.972392e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDIGO.NS</th>\n",
       "      <td>330</td>\n",
       "      <td>325</td>\n",
       "      <td>4.959564</td>\n",
       "      <td>72.140415</td>\n",
       "      <td>4.542731e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JUBLFOOD.NS</th>\n",
       "      <td>655</td>\n",
       "      <td>655</td>\n",
       "      <td>4.667399</td>\n",
       "      <td>1829.695730</td>\n",
       "      <td>4.338997e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LTI.NS</th>\n",
       "      <td>335</td>\n",
       "      <td>335</td>\n",
       "      <td>4.275180</td>\n",
       "      <td>934.596170</td>\n",
       "      <td>8.840443e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LUPIN.NS</th>\n",
       "      <td>1210</td>\n",
       "      <td>1210</td>\n",
       "      <td>3.938691</td>\n",
       "      <td>11615.829911</td>\n",
       "      <td>3.955134e+21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRF.NS</th>\n",
       "      <td>1040</td>\n",
       "      <td>1040</td>\n",
       "      <td>4.681546</td>\n",
       "      <td>7951.165343</td>\n",
       "      <td>1.056188e+22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARICO.NS</th>\n",
       "      <td>1645</td>\n",
       "      <td>1645</td>\n",
       "      <td>2.721772</td>\n",
       "      <td>27647.559313</td>\n",
       "      <td>4.021333e+20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOTHERSUMI.NS</th>\n",
       "      <td>1220</td>\n",
       "      <td>1220</td>\n",
       "      <td>5.102058</td>\n",
       "      <td>59807.736256</td>\n",
       "      <td>9.324485e+26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MUTHOOTFIN.NS</th>\n",
       "      <td>495</td>\n",
       "      <td>495</td>\n",
       "      <td>5.186472</td>\n",
       "      <td>814.425797</td>\n",
       "      <td>3.527098e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMDC.NS</th>\n",
       "      <td>740</td>\n",
       "      <td>740</td>\n",
       "      <td>4.580985</td>\n",
       "      <td>-56.060885</td>\n",
       "      <td>7.181412e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PETRONET.NS</th>\n",
       "      <td>885</td>\n",
       "      <td>885</td>\n",
       "      <td>3.743185</td>\n",
       "      <td>1825.176499</td>\n",
       "      <td>5.386421e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PIDILITIND.NS</th>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>4.119234</td>\n",
       "      <td>44680.716813</td>\n",
       "      <td>6.614179e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PEL.NS</th>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>4.684029</td>\n",
       "      <td>4030.807636</td>\n",
       "      <td>1.066474e+29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PGHH.NS</th>\n",
       "      <td>1140</td>\n",
       "      <td>1140</td>\n",
       "      <td>2.284536</td>\n",
       "      <td>3389.661599</td>\n",
       "      <td>8.369040e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PNB.NS</th>\n",
       "      <td>985</td>\n",
       "      <td>985</td>\n",
       "      <td>5.021491</td>\n",
       "      <td>307.968564</td>\n",
       "      <td>2.215090e+22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SBICARD.NS</th>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>3.683544</td>\n",
       "      <td>64.140356</td>\n",
       "      <td>6.935568e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIEMENS.NS</th>\n",
       "      <td>1255</td>\n",
       "      <td>1255</td>\n",
       "      <td>4.666809</td>\n",
       "      <td>13189.999472</td>\n",
       "      <td>1.732394e+26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TORNTPHARM.NS</th>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>4.056088</td>\n",
       "      <td>11749.668969</td>\n",
       "      <td>1.494018e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UBL.NS</th>\n",
       "      <td>635</td>\n",
       "      <td>635</td>\n",
       "      <td>5.169036</td>\n",
       "      <td>1984.375000</td>\n",
       "      <td>2.557345e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCDOWELL-N.NS</th>\n",
       "      <td>970</td>\n",
       "      <td>970</td>\n",
       "      <td>6.024819</td>\n",
       "      <td>11555.127607</td>\n",
       "      <td>6.181102e+24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VEDL.NS</th>\n",
       "      <td>1530</td>\n",
       "      <td>1530</td>\n",
       "      <td>5.791963</td>\n",
       "      <td>3760.913432</td>\n",
       "      <td>5.543527e+37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YESBANK.NS</th>\n",
       "      <td>965</td>\n",
       "      <td>965</td>\n",
       "      <td>5.492615</td>\n",
       "      <td>-5.667414</td>\n",
       "      <td>1.241480e+23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               total_length_of_data  length_of_high_confidence_data  \\\n",
       "ACC.NS                         1075                            1075   \n",
       "ABBOTINDIA.NS                  2340                            2340   \n",
       "ADANIENT.NS                    1045                            1045   \n",
       "ADANIGREEN.NS                   285                             285   \n",
       "ADANITRANS.NS                   455                             455   \n",
       "ALKEM.NS                        325                             325   \n",
       "AMBUJACEM.NS                    945                             945   \n",
       "APOLLOHOSP.NS                   925                             925   \n",
       "AUROPHARMA.NS                  1495                            1495   \n",
       "DMART.NS                        305                             305   \n",
       "BAJAJHLDNG.NS                  1015                            1015   \n",
       "BANDHANBNK.NS                   210                             210   \n",
       "BERGEPAINT.NS                  1150                            1150   \n",
       "BIOCON.NS                       935                             935   \n",
       "BOSCHLTD.NS                     990                             990   \n",
       "COLPAL.NS                       985                             985   \n",
       "DLF.NS                          800                             800   \n",
       "DABUR.NS                        980                             980   \n",
       "GAIL.NS                        1590                            1590   \n",
       "GODREJCP.NS                    1075                            1075   \n",
       "HDFCAMC.NS                      240                             240   \n",
       "HAVELLS.NS                     1080                            1080   \n",
       "HINDPETRO.NS                   1320                            1320   \n",
       "ICICIGI.NS                      235                             235   \n",
       "ICICIPRULI.NS                   285                             285   \n",
       "IGL.NS                          880                             880   \n",
       "INDUSTOWER.NS                   395                             390   \n",
       "NAUKRI.NS                       770                             770   \n",
       "INDIGO.NS                       330                             325   \n",
       "JUBLFOOD.NS                     655                             655   \n",
       "LTI.NS                          335                             335   \n",
       "LUPIN.NS                       1210                            1210   \n",
       "MRF.NS                         1040                            1040   \n",
       "MARICO.NS                      1645                            1645   \n",
       "MOTHERSUMI.NS                  1220                            1220   \n",
       "MUTHOOTFIN.NS                   495                             495   \n",
       "NMDC.NS                         740                             740   \n",
       "PETRONET.NS                     885                             885   \n",
       "PIDILITIND.NS                  1025                            1025   \n",
       "PEL.NS                         1400                            1400   \n",
       "PGHH.NS                        1140                            1140   \n",
       "PNB.NS                          985                             985   \n",
       "SBICARD.NS                      120                             120   \n",
       "SIEMENS.NS                     1255                            1255   \n",
       "TORNTPHARM.NS                  1025                            1025   \n",
       "UBL.NS                          635                             635   \n",
       "MCDOWELL-N.NS                   970                             970   \n",
       "VEDL.NS                        1530                            1530   \n",
       "YESBANK.NS                      965                             965   \n",
       "\n",
       "               mean_profit_per_trade  layman_profit       returns  \n",
       "ACC.NS                      3.429617    1531.510824  2.746118e+17  \n",
       "ABBOTINDIA.NS               1.100784    5824.472656  1.397144e+12  \n",
       "ADANIENT.NS                 7.503856  186217.171075  3.564628e+33  \n",
       "ADANIGREEN.NS               9.956630    6864.630140  2.124048e+13  \n",
       "ADANITRANS.NS               7.133129    8571.327088  1.027537e+15  \n",
       "ALKEM.NS                    3.473691     168.762441  5.451545e+06  \n",
       "AMBUJACEM.NS                3.370343    1449.609550  2.226889e+15  \n",
       "APOLLOHOSP.NS               4.070937    9883.190206  3.982650e+17  \n",
       "AUROPHARMA.NS               5.695153   14979.557154  4.765998e+36  \n",
       "DMART.NS                    3.841281     459.812556  7.626889e+06  \n",
       "BAJAJHLDNG.NS               3.676083    1476.645707  1.415094e+17  \n",
       "BANDHANBNK.NS               6.343244     -37.636112  2.470280e+07  \n",
       "BERGEPAINT.NS               4.044372   31246.572983  1.431911e+21  \n",
       "BIOCON.NS                   3.698039     793.370519  2.250349e+16  \n",
       "BOSCHLTD.NS                 3.671092    4714.438244  1.570601e+17  \n",
       "COLPAL.NS                   2.995188    2273.851816  2.503498e+14  \n",
       "DLF.NS                      5.334942     -43.195621  1.939968e+19  \n",
       "DABUR.NS                    3.239398    6460.540838  1.978823e+15  \n",
       "GAIL.NS                     3.496580     464.608189  3.287930e+24  \n",
       "GODREJCP.NS                 3.970098    9672.078323  5.874147e+19  \n",
       "HDFCAMC.NS                  3.488963      56.943836  3.087227e+05  \n",
       "HAVELLS.NS                  4.708848   83884.326167  8.660574e+22  \n",
       "HINDPETRO.NS                5.272777     689.749981  3.498494e+29  \n",
       "ICICIGI.NS                  3.198322      86.457393  1.435652e+05  \n",
       "ICICIPRULI.NS               3.591034      74.256602  1.978787e+06  \n",
       "IGL.NS                      3.621473    1936.844979  2.071757e+15  \n",
       "INDUSTOWER.NS               4.436630      -1.414631  1.461801e+09  \n",
       "NAUKRI.NS                   4.860785    2814.955625  2.972392e+17  \n",
       "INDIGO.NS                   4.959564      72.140415  4.542731e+08  \n",
       "JUBLFOOD.NS                 4.667399    1829.695730  4.338997e+14  \n",
       "LTI.NS                      4.275180     934.596170  8.840443e+07  \n",
       "LUPIN.NS                    3.938691   11615.829911  3.955134e+21  \n",
       "MRF.NS                      4.681546    7951.165343  1.056188e+22  \n",
       "MARICO.NS                   2.721772   27647.559313  4.021333e+20  \n",
       "MOTHERSUMI.NS               5.102058   59807.736256  9.324485e+26  \n",
       "MUTHOOTFIN.NS               5.186472     814.425797  3.527098e+12  \n",
       "NMDC.NS                     4.580985     -56.060885  7.181412e+15  \n",
       "PETRONET.NS                 3.743185    1825.176499  5.386421e+15  \n",
       "PIDILITIND.NS               4.119234   44680.716813  6.614179e+18  \n",
       "PEL.NS                      4.684029    4030.807636  1.066474e+29  \n",
       "PGHH.NS                     2.284536    3389.661599  8.369040e+12  \n",
       "PNB.NS                      5.021491     307.968564  2.215090e+22  \n",
       "SBICARD.NS                  3.683544      64.140356  6.935568e+03  \n",
       "SIEMENS.NS                  4.666809   13189.999472  1.732394e+26  \n",
       "TORNTPHARM.NS               4.056088   11749.668969  1.494018e+19  \n",
       "UBL.NS                      5.169036    1984.375000  2.557345e+15  \n",
       "MCDOWELL-N.NS               6.024819   11555.127607  6.181102e+24  \n",
       "VEDL.NS                     5.791963    3760.913432  5.543527e+37  \n",
       "YESBANK.NS                  5.492615      -5.667414  1.241480e+23  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nifty_stats_df_from_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.13150848564299"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of days when there is a buy call\n",
    "(nifty_stats_df_from_dict['length_of_high_confidence_data'].sum() / total ) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Stats VS Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ACC.NS\n",
      "1 ABBOTINDIA.NS\n",
      "2 ADANIENT.NS\n",
      "3 ADANIGREEN.NS\n",
      "4 ADANITRANS.NS\n",
      "5 ALKEM.NS\n",
      "6 AMBUJACEM.NS\n",
      "7 APOLLOHOSP.NS\n",
      "8 AUROPHARMA.NS\n",
      "9 DMART.NS\n",
      "10 BAJAJHLDNG.NS\n",
      "11 BANDHANBNK.NS\n",
      "12 BERGEPAINT.NS\n",
      "13 BIOCON.NS\n",
      "14 BOSCHLTD.NS\n",
      "15 CADILAHC.NS\n",
      "CADILAHC.NS No data fetched for symbol CADILAHC.NS using YahooDailyReader\n",
      "15 COLPAL.NS\n",
      "16 DLF.NS\n",
      "17 DABUR.NS\n",
      "18 GAIL.NS\n",
      "19 GODREJCP.NS\n",
      "20 HDFCAMC.NS\n",
      "21 HAVELLS.NS\n",
      "22 HINDPETRO.NS\n",
      "23 ICICIGI.NS\n",
      "24 ICICIPRULI.NS\n",
      "25 IGL.NS\n",
      "26 INDUSTOWER.NS\n",
      "27 NAUKRI.NS\n",
      "28 INDIGO.NS\n",
      "29 JUBLFOOD.NS\n",
      "30 LTI.NS\n",
      "31 LUPIN.NS\n",
      "32 MRF.NS\n",
      "33 MARICO.NS\n",
      "34 MOTHERSUMI.NS\n",
      "35 MUTHOOTFIN.NS\n",
      "36 NMDC.NS\n",
      "37 PETRONET.NS\n",
      "38 PIDILITIND.NS\n",
      "39 PEL.NS\n",
      "40 PGHH.NS\n",
      "41 PNB.NS\n",
      "42 SBICARD.NS\n",
      "43 SIEMENS.NS\n",
      "44 TORNTPHARM.NS\n",
      "45 UBL.NS\n",
      "46 MCDOWELL-N.NS\n",
      "47 VEDL.NS\n",
      "48 YESBANK.NS\n"
     ]
    }
   ],
   "source": [
    "no_of_decisions = []\n",
    "mean_profit = []\n",
    "min_rofit = []\n",
    "max_profit = []\n",
    "nifty_tickers = get_tickers(2)\n",
    "i = 0\n",
    "\n",
    "for t in nifty_tickers:\n",
    "    temp_no_of_decisions = []\n",
    "    temp_mean_profit = []\n",
    "    temp_min_rofit = []\n",
    "    temp_max_profit = []\n",
    "    try:\n",
    "        print(i, t)\n",
    "        d = data.DataReader(t, 'yahoo', train_start_date, test_end_date)\n",
    "    except Exception as e:\n",
    "        print(t, e)\n",
    "        continue\n",
    "    # Stats calculation for a specific ticker\n",
    "    for con in [x * 0.01 for x in range(0, 100, 5)]:\n",
    "#         print(con)\n",
    "        temp_df, t = get_backtest_df_pass_df(\"ALL\", d, \n",
    "                           con, 0.5, \n",
    "                           train_start_date, \n",
    "                           test_end_date, \n",
    "                           5)\n",
    "        temp_stats = temp_df.describe()['profit_percentage_per_trade']\n",
    "        temp_no_of_decisions.append(temp_stats['count']/ t)\n",
    "        temp_mean_profit.append(temp_stats['mean'])\n",
    "        temp_min_rofit.append(temp_stats['min'])\n",
    "        temp_max_profit.append(temp_stats['max'])\n",
    "    no_of_decisions.append(temp_no_of_decisions)\n",
    "    mean_profit.append(temp_mean_profit)\n",
    "    min_rofit.append(temp_min_rofit)\n",
    "    max_profit.append(temp_max_profit)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAACgb0lEQVR4nOzdd5wbxd348c+o1zuddL373HvvBpsOppdAIEAgJKQSwpP80p70kE568gRSILRQEiB0013ADfd2Pl/vTXc69b7z+0PC2MYGE+5cwrxfL71O2l3tzK5O3x3NzM4IKSWKoijKh4fueGdAURRFObZU4FcURfmQUYFfURTlQ0YFfkVRlA8ZFfgVRVE+ZFTgVxRF+ZBRgV85oQghbhdCeIUQPUKISiFESAihP975GglCiM8KIXqzx+jJ/q05wrY3CCFeP9Z5HAn/TcdyslKB/yQjhGgRQiSEEPmHLN8qhJBCiOpjnJ9lQggtG7SCQog6IcSN/+G+KoEvA5OklMVSyjYppUNKmc6uXymE+OQR3msRQgwJIU4/zLpfCyH+lX2+RAixVgjhF0IMCiHeEELMfZc8jRNC/DN7MfILIXYIIf7ng16MhBBG4FfA2dljHMj+bfog+z0WhBBlQoiUEGL0YdY9IYS443jkSzl6KvCfnJqBq996IYSYCtiOX3boklI6gBzga8BfhBCTDt1ICGF4j/1UAgNSyr73mwEpZQx4BLj+kDT1ZM7VvUKIHOAZ4PeAGygDvg/ED7fPbGDbALQDU6WUucBHgDmA8/3m8RBFgAXY/QH3c8xJKTuBV4DrDlwuhHADy4F7j0e+lPdBSqkeJ9EDaAG+Bbx5wLI7gP8FJFCdXWbOLm8DeoE7AWt2XR6ZANgP+LLPyw/Y30rgh8AbQBB4Ecg/Qn6WAR2HLOsHrgBuyO7j18AAcDuQC9yX3aY1eyw64EwgCmhACPg7UJ09JgPwIyANxLLr/3CYvCzK5td2wLLlQF92H3OAofdxrh8Ann2PbS4iE7yHsudt4iGf1VeAHYCfzIXJAowDwtljCwGvZreXwJjscw/wFBAANmY/j9cP2PcE4CVgEKgDrjxg3d+BPwLPZs/HBmD0AesnH/DeXuCb2eU64OtAY/bzehRwH+G4rwEaD1n2OWBr9vlb+wkCe4BLD9juhreO5cDP+JD/v08e8PoTQC2Z/9UXgKrscpH93+rLnqedwJTj/R09GR7HPQPq8T4/sEwwOTP7ZZ8I6IEOoIqDA/+vs4HDTaZ0+jTwk+w6D3A5mV8JTuCfwL8PSGNl9ks7DrBmX//0CPlZRjbwZwPHpUASGJ/9gqeAW8gEXiuZoP9kNt1qYB9w06H7yr4+KCgcGhCOkJ99wLUHvH4I+E32eU42oN0LnAfkvce+eoAb32X9WwH8LMAIfBVoAEwHfFYbgdLs51ALfOZwx5ZddmDgf5hM4LUDU4BO3g6WdjK/Qm7MnteZgJdMFRlkAv8AMC+7/kHg4ew6J9BNpkrNkn09P7vuVmA9UE6m4HAX8NARjt1K5mK25IBl64AvZZ9/JHvcOuCq7Hkqya67gaMM/MDF2XM6MXss3wLWZtedA2wGXGQuAhPfSkM93iOOHO8MqMf7/MDeDvzfAn4CnEum9GbIfoGqs1+CMAeX8hYCzUfY5wzAd8DrlcC3Dnj9OWDFEd67jEwpfYhMCXIb8NHsuhuAtgO21QOJtwJUdtmngZUH7OuDBv5vAS9mn+cAEWDmAesnkgmMHWQuSk8BRUfYVxI4913S+jbw6AGvdWQC9LIDPqsDL0I/B+483LFll0lgTPY8JYEJB6z7MW8Hy6uANYfk5S7gu9nnfwf+esC65cDe7POryZbKD3M8tcAZB7wuyebDcITt/wr8Oft8bPazLTzCttuAiw/4vzjawP882YLBAec4QqagczqZC/0CQHe8v5sn0+O96lyVE9f9wGpgFJlS9IEKyJTmNwsh3lomyAQUhBA2Mr8IziVT7QPgFELoZbYhlUxp9y0RwPEueemSUpYfYV37Ac/zyZSMWw9Y1kqmrn243A98VwhRSub4GqWUW99aKaWsJRN4EEJMIFOd8xsOaDM5wACZ4HckpRxwLFJKTQjRzsHHc+h5LD2KYyggcyE/8NwdeM6qgPlCiKEDlhnIHPuR0n3r86sg82vucKqAJ4QQ2gHL0mTaIzoPs/29wFNCiC+Sqe9/QWbbZ4QQ1wP/Qyawk00//zD7eC9VwG+FEL88YJkAyqSUrwoh/kCmWqtKCPE48BUpZeA/SOdDRTXunqSklK1kGnmXA48fstpLpr58spTSlX3kykwDLGR+5o8n8xM/Bzg1u1ww/OQh+UqS+TK/pZLDB5X32tfhN8iclzXAtWSC0REbGqWUe8mUjqccYZOXyVSJHUkXBxyLyFxlKzj64zmSfjK/RioOWFZ5wPN2YNUBn61LZnoEffYo9t0OHLbLaHbdeYfs1yIzjbmH8zqZX3kXkznf9wIIIaqAvwBfADxSShewi8P/f4Wzfw/snFB8SJ4+fUierFLKtQBSyt9JKWcDk8hUvf2/dzt4JUMF/pPbTcDpUsrwgQullBqZL96vhRCFsL8L3jnZTZxkLgxD2Z4Y3z0Wmc3+mngU+JEQwpkNEP9DptR9NHo5ctA60L1kgs5iMvXbQKaEL4T4shCiPPu6gkxJf/0R9vNdYJEQ4hdCiOLse8YIIR4QQriyx3K+EOKMbPfML5PpIbT2KI/nsLLn6XHge0IIW7aH1McP2OQZYJwQ4johhDH7mCuEmHgUu38GKBFCfEkIYc5+DvOz6+4k89lUZY+1QAhx8bvkU5L5tfkzMvXsT2dX2clcpPuz+7mRI1xcpZT9ZC6U1woh9EKITwAHdhO9E/iGEGJydl+5QoiPZJ/PFULMz577MJmGfw3lPanAfxKTUjZKKTcdYfXXyDSKrRdCBMiUXsdn1/2GTOOcl0zQWzHCWT3QLWS+pE1kSoz/AO4+yvf+FrhCCOETQvzuXbZ7jExj6itSyu4DlgeB+cAGIUSYzLHvIhOw30FK2UimbaQa2C2E8Gf3vQkISinryJR0f0/mXF4IXCilTBzl8bybL5CpHukh86vkngPyFQTOBj5K5ldHD5nga36vnWbfe1Y2rz1APXBadvVvybR5vCiECJI5P/MPt58D3Efm18gjUsp4No09wC/JNPb2AlPJ9O46kk+RKakPkOlxtP/CKaV8IntsD2f/j3eRaZiHTBvOX8j09mnNvv8X75FfBRCZi7aiKIryYaFK/IqiKB8yKvAriqJ8yKjAryiK8iGjAr+iKMqHzElxA1d+fr6srq4+3tlQFEU5qWzevNkrpSw4dPlJEfirq6vZtOlIvRYVRVGUwxFCtB5uuarqURRF+ZBRgV9RFOVDRgV+RVGUDxkV+BVFUT5kVOBXFEX5kFGBX1EU5UNGBX5FUZQPGRX4FUVRTkCJvgj9f95BrN437PtWgV9RFOUEFKsdIN7kJ9EdGvZ9q8CvKIpyAkp2ZgK+ucY17PtWgV9RFOUElOyPAmAssQ/7vlXgVxRFOQFp/jgYdOj0wx+mVeBXFEU5AWmxNHr7yIyjqQK/oijKCSYdSYIm0edZRmT/KvAriqKcYBKtAdKGMEMVrxCLdQ37/lXgVxRFOcHEWwPEnW20W/9IJNI87PtXgV9RFOUEk+wKE3d0AmC3jx32/avAryiKcoJJDUSJOzoxGHIxmd4xc+IHpgK/oijKCUYLJkg4u7DbxyCEGPb9q8CvKIpyApFJDS2RJuHoHJFqHlCBX1EU5YSS7I+QNvlJG0I4TrbAL4SoEEK8JoTYI4TYLYS4NbvcLYR4SQhRn/2bN1J5UBRFOdkkOoLEHZkunCdjiT8FfFlKOQlYAHxeCDEJ+DrwipRyLPBK9rWiKIoCJNqDJPb36Bk3ImmMWOCXUnZLKbdknweBWqAMuBi4N7vZvcAlI5UHRVGUk02qJ0Lc3olBn4vJlD8iaRyTOn4hRDUwE9gAFEkpu7OreoCiI7znZiHEJiHEpv7+/mORTUVRlOMu5Yvtb9gdiR49cAwCvxDCATwGfElKGThwnZRSAvJw75NS/llKOUdKOaegYPj7sSqKopxopCZJhxPEHZ04nCNTzQMjHPiFEEYyQf9BKeXj2cW9QoiS7PoSoG8k86AoinKySPtipE1DaMbIiDXswsj26hHA34BaKeWvDlj1FPDx7POPA0+OVB4URVFOJsn+KHH7yA3V8JaRGew5YzFwHbBTCLEtu+ybwE+BR4UQNwGtwJUjmAdFUZSTRrIztH+MnpHqww8jGPillK8DR2qZOGOk0lUURTlZJTozXTkNwjViPXpA3bmrKIpywkj1RYk7urBbx4xoOirwK4qinACklCSHYiTsnThyR65HD6jAryiKckLQQklS+gE0YxSHc/yIpqUCv6Ioygkg1R85YKiGkWvYBRX4FUVRTgjJvuiIzrp1IBX4FUVRTgDJnjAJeycG8jCZ3COalgr8iqIoJ4BkV6YPv81QM+JpqcCvKIpyAkh6IyQcXdgdmWoeKSWx2toRSUsFfkVRlONMi6dJaH1ohhjOvAkAhDdtZs3nf4P3qRXDnp4K/IqiKMdZqj/y9lAN2cC/5/7X2DfuKgIlU4c9PRX4FUVRjrNkX4SE/e0xelL9/dT352I3xKieXTrs6anAryiKcpyl+jNdOQ3pPIxGFy0PPM1Q7hgmLylFpxv+yVhU4FcURTnOkr1hEo5ObLpRyFSK3esH0MkUUy+cPCLpqcCvKIpynCV6QsTtXdgsYxh4cSVdOVOoGaXDYjeOSHoq8CuKohxHMq0Rj3YiDXEcOePY+fhWNL2ZmVfNGbE0R3IiFkVRFOU9pAZi+2fdMqfzaE7ayM+JUDjKNWJpqhK/oijKcZTqixB3dAHQ9WQTUWsB089Xo3MqiqL810pmR+U0JPPYt9eCmRhjl44mGEvyzI4uvKH4sKepAr+iKMpxlOqLkrB3YQoXMZA7gYmzctEbdOzpCvCFf2xlV6d/2NNUgV9RFOU4SvRlBmdL9bsRMs30K2cD0NgfBmB0gWPY01SBX1EU5TiRUhINtCP1CcKBaioLkzhcFgAa+0NYjDrKXNZhT1cFfkVRlOMk7U8QN7cDEAxUMuPKWfvXNfaHqMl3qDt3FUVR/pscODgb4RzKphTtX9fYH2J04fBX84AK/IqiKMdNsi/To0cfy2P84rEIkSndx5JpOnxRRhfYRyRdFfgVRVGOk1RvmLi9E32wlIkXTt+/vNkbRsqRadgFFfgVRVGOm3BdFwlHN8ZYKUaTfv/yxv4QoAK/oijKf51orAepT5CbN/Gg5Q19IYSAUfmqqkdRFOW/RnhvM+mcPgBc+ZMOWtfYH6bMZcV6wK+A4aQCv6IoynFQ/8Da/T16nPkHl/gb+0IjVs0DKvAriqIcc1oshrffQtzeiSHqxlKY//Y6TdLkVYFfURTlv0rzwy+it3pIODoxh8vQ55j3r+vyR4klNcbk6eGRa6GvdtjTV4FfURTlGNu5qhOnLk3C3o1Fq0IccHfu/jF6up6C2mcgpUbnVBRFOan5Nu6g21iDM8+L1CexGWoOWt/Yl+nKWVJ7D8/nf4o+Q+mw50EFfkVRlGNo+8PrQWdEs3UDYLeNPWh9Y3+IXH2CmHCwwWsnGo0Oex5U4FcURTlGkoM+Gn1uKkwBEtkePQ7XuIO2aWjvZrTWQm/lBQAUFRW9Yz8flAr8iqIox8iee14kZnYzYWYpcUcXhqgHc77n7Q2kpLF3iNEmHz3WsbhcLiwWy7DnY8QCvxDibiFEnxBi1wHLvieE6BRCbMs+lo9U+oqiKCcSqWns2RHGooXILyvM9OgJlWHwvB3Y/VuewJu2M3r8FHr6vCNS2oeRLfH/HTj3MMt/LaWckX08N4LpK4qinDA6n3+DQWs1EyaaSHlDJOzdmMLl6LMTr5CM0fjK3wConryAwcFBiouLRyQvIxb4pZSrgcGR2r+iKMrJZPvTexFaiunXLyHib0bqUthkFUKf7cq58S4aA5mQ7DYkkFKefIH/XXxBCLEjWxWUd6SNhBA3CyE2CSE29ff3H8v8KYqiDKtwUxvtqTIqcwPY8+yEE40AWM2jsxt4YfUdNOYtxqgXEBkCRqZhF4594P8TMBqYAXQDvzzShlLKP0sp50gp5xQUFByj7CmKogy/7X9fRdpgYeZHZpD2xYhbOwCwO7NdOVf+FBJhGnMWUO2x09/Xi8lkwuVyjUh+jmngl1L2SinTUkoN+Asw71imryiKcqyl43Hq2ky4GKRs3hiS/VES9k6MkQLMnjzo3web7oY5N9IYyIzB39PTQ1FREYHgVqSUw56nYxr4hRAlB7y8FNh1pG0VRVH+G9T/4xUilgKmLMzUXKT6IsQdXZjCpZkePS99G0x2kqd8jbaBCDUFNnp7e3G54JV/3kR359PDnqeR7M75ELAOGC+E6BBC3AT8XAixUwixAzgNuG2k0lcURTkR7Hq9F1M6zKSPLgIg0RcgYe/OdOWM7oR9K+CUL9Mas5LSJBV2SSKRIO7bQctL5QTac4Y9T4Zh32OWlPLqwyz+20ilpyiKcqLp37CLXkMFk8qCGM1GAML+ZnCnMYXLMKz/LuRWwvzP0FA3BEAuEQCGGpow2Q2Mnbt42POl7txVFEUZIdse2QTArOszwVtKSTjWAIA1WYTo2wZnfQ+Mlv3z7OpiQwgg0grlM1wkkj3Dni8V+BVFUUZArKuXJr+HUpOX3OpCALRQkrixDaTAHghD+VyYfBmQGZytOMeCz9uPURdBaBJzxXpisa5hz5sK/IqiKMMsvHYtr3/2F6SMdmZc/PZ8usm+CAlHJ8ZoPqZUF5zzYxCZG7ga+8OMLrTT1dUKoQi5FZBbWIDLNXfY86cCv6IoyjCRySR9v/4NTZ/6HM2FSykpN1N9xtT961P9UeKOTkyhMgzFHqjI9GiXUtLUF2J0nolgMAahGK5x3RQVX4wQwx+mVeBXFEUZBsnOTlqvu56Bu+7Ce84XSOjtLLpmCkK8PbtWos9Pwtab6dEz77z9y/uDcYLxFEWGIQBMMoqrPEbOttNIeo/DePxCiHc0KR9umaIoyodV4IUXabr0MuL19Xh+cgcNchzV0/Iprsk9aLtw3w7QZXv0VFXtX97wVsOubwsARVVhiiIXkXgzghZKDHt+j6bE//ujXKYoivKhosVidH//+3TeeiumqipG/fsJGplAIpZi/kUHT6mIlIQjdQCYQ2Xo3W8Px/zWPLvhziZEKknemDpcXcsweCyYqo5hP34hxEJgEVAghPifA1blAPphz4miKMpJJN7YSOdt/0N83z7cN32CwltvJRKV7Hh1HWPnFJFf7jhoe233CmJWH0iBRVeFzvR2GG3sC2E1SKJhGyZdnByLC9Flw3Z20UFVRcPl3W7gMgGO7DbOA5YHgCuGPSeKoignASkl/sceo+f2H6Gz2aj4y59xnHIKAJsfq0NLS+ZdOOrgN6WTpF64i3ipGVO8CJP74FJ8Y3+IKWIvaYMNd04XRUMfBQG2WSMzOucRA7+UchWwSgjxdyll64ikriiKchJJB4P0fPd7BJ57DtuCBZT+/GcYCzN99APeKLtf72Li4hJchbaD37jpHpJDkBjbmanf91gPWt3Q6+c0fy24K3AVt2FvuxZ9hZlVX/k0Mz/9edxzh7dL5xHr+IUQv8k+/YMQ4qlDH8OaC0VRlBNcdOdOmi+7nMALL1DwpS9R+be/7g/6ABufaUYIwZzlh5T2o0Ow8ickcueTsPVi8pccNN1iOJ7C5wvijCYBKBQeCOjp8m1kW9BLf2f7sB/Lu1X13Jf9e8ewp6ooinKSkJrG4N/vpe9Xv8JQWEDV/fdhmzXroG0GukLUbehhxhkVOPLMB+9gzR0Q9REaNRp0Wnae3bdL/I19fsaH6pBmK0JoVIeXI8x69tWtwZLWGHPBhcN+TO8W+H8BnAEsl1J+bdhTVhRFOcGlBgbo+sY3CK9eg/OsMym5/Xb0ubnv2G7j080YzXpmnVt18IrBZthwF8z4GJGBboDMzVsHBP6tTRuYEqyFqmJsVj/W9ioYb6K7NsQETzF6g3HYj+vdunOWCCEWARcJIWYKIWYd+Bj2nCiKopxAgq+8QvMllxJZv4Hi736Hst/97rBBv7clQNPWfmacWYnVYTp45cvfA50BufR/icoWkDpMkeKDqnrqtq3FnfShWcy4DVZICpq7X0cKwZQzzxmRY3u3Ev93gG8D5cCvDlkngdNHJEeKoijHUbKnh57bbyf08iuYx42j4q9/wTJ+/BG33/BkIxaHkRlnVhy8omMT7Pk3LPsGqaSLuL0TU7IYg9WOzpIJvdFoG+l9HSSMVlJpAyWJavQeC3t3rcUViVN23gUjcozv1qvnX8C/hBDfllL+cERSVxRFOUHIdBrfg/+g/ze/QWoahV/5Mu6PfxxhPHJVS0edj/ZaH4uvGIPJckg4XfkTsHlg4RdI1WcGZ7PEKg8q7bc0Poh7wMdQ+TjMQH6ggOiMOP5NEWa58tE77CNyrO85EYuU8odCiIuAU986HCnlMyOSG0VRlOMgtmcP3d/5LrFdu7AvWULxd7+DqaLiXd8jpWT9vxuxu8xMObXs4JXtb0LDy3DWD8DsIN7XQ8Lah7N7wf76fU1LsHPVCgzSCflWiIFbOmloX4tO05iw7MyROtz3DvxCiJ+QmRT9weyiW4UQi6SU3xyxXCmKohwDWiRC/+//wOB996F3uSj95R3kLF9+VHfLtuwcoLc5wLKPjcdgOmQwg1U/zZT2534SgPBgA7g1TAPFGKZkSvx9fS/SvdNKnykfqz6EnXwcNXnsW7WeokAEz1lnDfvxvuVopl48H5ghpdQAhBD3AlsBFfgVRTlpBVeupOcHPyDV1Y3ryisp/PL/HLbx9nCkJtnwZCO5BVYmLCo5eGX7xrdL+6ZMVU0oUg9uDurKuXfT/aR8BnZ7JrE4MYQ77cSb00M8maDaZMdUWUk8HcesNx+a/Ad2tMMyuw54fnRnRlEU5QSU7O2j49Yv0fGZz6Kz2ah68AFKfvD9ow76APWbehnoDDP/ohr0+kPC6MqDS/tSSqLpJpA6jOFi9B4L4XATzW92IvV66h2jSSRNeHQ51Desx5xMUbxgLn/Y+gfO/OeZ7B7YPZyHDxxdif/HwFYhxGuAIFPX//Vhz4miKMoIkpqG7+GH6f/Vr5GJBAVfuhXPJz6BMJne+80HSKc1NjzdjKfMwZjZhQevbN8Ija8cVNpP+xPELR2YU6XopBGDx0pj0wMMNeQQLhtPuSmIBDylbt587V+M8gX5tvw3O3donFZxGmbd8Jf43zXwi8zULxqwAHhrsIivSSmHf/ZfRVGUERKr20fPd75DdPt2bAsXUPK972GqqnrvNx5G7RvdBPqjnP+5aQjdIW0Bh5T2AVL9EeKOTmzJ0QiLAWlOs2fNS2ipXPY6KzlFRkDCG94XMEqJOxxkwtLL+PH4K6ha81tIaR/k0A/rXat6svX6X5VSdkspn8o+VNBXFOWkoEWj9P3ylzRffjmJtjZKf/ZTKu+++z8O+qlEmk3PNlNck0vVVM/BK98q7S++dX9pHyDe6yNp68McK8eQb6Hfu4K+3SZcpfnsTBRQpkn0UhBpaSQnnqBiwal8+5TvUbXvJdj5KCRCH+QUHNbR1PG/LIT4ihCiQgjhfusx7DlRFEUZRvH6epouvIiBv/yV3Isvoua5Z8m9+OIPNL79zlWdhP0JFlxS8879HKa0DxAa2AdCYvQVY/BYqd18P1GvldqibnJSTjQtidVkIjegp6Lfj+fMsyEZRa77I97cafgdo//j/B7J0dTxX5X9+/kDlkmg5jDbKoqiHHfx5mZab/wEQggq77sX+7x5H3ifiWiKLStaqZzkpmxc3sErD1O3/5ZQuB6cYOwtYodxI22betD0udgqHJwzZGRAF8JtNBIVgpJABPspp8CW+5B+L7u2llO5YA+5Cxd+4Pwf6Ghu4Br1XtsoiqKcKBLt7bTdcCNISeV992KuGZ4y6raX24iFk8y/+DD7W/lTsOW/o7QPEEk2ZsboCRfREbsXX30OJTMmEk4PsAyN10SKUGcn5WlB7pSpGJx2tNd/Q3NDKUWNvRSbj2F3TiHEfCHEdiFESAixTggxcdhTVxRFGUbJri7aPn4DMhaj8u67hy3oR4MJtr3czuiZBRQeOgfuEer2+yJ9fGXF/5Awd5CI5iKkAWfYh5bUM3VxDcaBcRh0mbl2Nf8gJS2dOJYtg+0PIQe7iezRY5w39x1DQA+Hd6vj/yPwFcBDZpC23wx76oqiKMMk2dtH6w03kg4Gqbj7b1jGjxu2fW9+oZVUIs28QydQh+yYPPkw96b9i1oDrVz33HX0tXUQd3SSr8+Um3uao7hK8klZ1zHBP4leXRAAm5TkByM4Tl1CevUvaa0vwpBIUfaVrwzbMRzo3QK/Tkr5kpQyLqX8J1AwIjlQFEX5gFJeL2033kja66Xyr3/BOnnysO075Iuxa2Un4xcU4y45ZNC0tg3Q+OpBpf09A3u4/vnriaVjfHPMbSSt/djS1fhSXYT7TUxedjp+71amx9y0GgPoEnGqhQFTaQnm5C7wthHea8S8eDG1qRSpVGrYjuUt71bH7xJCXHak11LKx4c9N4qiKO9Tyuej7cZPkOzupvIvf8Y6ffqw7v/NZ1uQUjL3gsM0d6766UGl/Y3dG/nia18k15TLXWfdhWltHZ1miSHgYXdkAzq9Dvc4P7w5DwuCkBxCF4tQVNuA47zlpF77OZ11BRgSaXrOO5cXn3qKXik4b/bMYT2mdwv8q4ALj/BaAirwK4pyXKUDAdpuuolEWxsVd92Jbc6cYd3/UG+E2rXdTFlaRs4hE6TvL+2f9UMw2Xm59WW+uvqrVOVUceeZd1JkL2Jf8BkwAz122oeaGDN3Pr7QCnK6b6CZOGmZwqXX4fAHcU2xotvSQKiuHPvSJTzX2IzP6aL6XeYC+E+923j8Nw57aoqiKMMkHQrR9qlPkahvoPz//oh9wYJhT2PjM83oDYI551W/c+UBpf1/7vsnt6+/nWn50/jDGX8g15wZ9yeSbERIA73NEVLpFONOmU5vz9OUDRXzSGofwiKokAJhMSO6nqCzzoM+mWb13PkYOloZe97FTHTYhv24jnaQNkVRlBOGFonQ/unPENu9h7Lf/BrHKacMexqd+3zUv9nL9NMrsOUcMp5PtrQvF9/Kn/c+yA/W/YDFpYv589l/3h/0ZVIjZmjFmC6gxV+LM9eNyN1JbvcpSMCfbAGgsrYO99JRGLr3EtxngWXLqOvuxF9UxrWVY4f9uEAFfkVRTjJaLEb75z5PdOtWyu74Bc4zzhj2NAa7wjx/505cRTZmnnOY4R1W/RRpL+DXws/vt/6eC2ou4Len/xar4e3qoER7kIS9Cxmx0B/vYMqCM+nrfRZXz+k0OcCt86NHYm9swlLUQvdeNyIleWTMBAxamkunL+OR72+kcUvfsB+fCvyKopw0tESCjlu+SGTDBkp/+hNyzj132NMI+eI8/ftt6A06LrxlOmbrITXi2dL+cyVjuKf+Ua6deC0/WvIjjLq3p2hMDcXof3QbSVs/gb4UAh3l8z3oenLRhx2sDjeA0YRLCCyFSWyBFoL1VryLlpAOD5E7YTKh7VHMdgOVkw8ZE2gYHM2QDQghFgHVB24vpbxv2HOjKIpyBDKZpPNLtxFes4aS239I7kUXDXsa8WiKZ/6wnXgkxaVfnkVOvvUd26Rfu52w0cL3k+3cOuc2bppy00Hj9miRJN67dxEx1wPQ3wpljrEExSpc3achzHr627fiLndRMOjDsUhPz14XpCVPVtdgEymumnMqT/5iFzq2012fS+WU4e2p9J4lfiHE/cAdwBIyQzPPBYa36VxRFOVdyFSKzv/3VUKvvkrRd76N64orhj2NdErj+Tt34usOc96np1JQ6XzHNqHGl9E3r+YvTitfW/wDPjn1kwcFfZlM4713D8mBCINzHkdLCYZarYwpn4m3+zWcvbOJ1ZgoTXSDToerZx9u+gg0WNk5Zz7OdIx5Cxexb+0AQieZon+EvBGYb/1oSvxzgElSSvl+diyEuBu4AOiTUk7JLnMDj5D59dACXCml9L2f/SqK8uEi02m6vvFNgitWUPj1r+G+5prhT0OTvHJvLZ11Ps68YSIVk945AHFPuIeef3+SCr2emcv/wOljLnjHPgYeqiPRFiB56W7CoTr6tnuw6tw4JwXQuiYhUgZ2hvaAJTPvbnHFIL21uUhNsLVmFA6blSXT5vLwE5vI1W/lpXmnMy3tH/bjPZo6/l1A8X+w778Dh1bAfR14RUo5FngFNZOXoijvQmoa3d/9LoGnn6bgttvw3HDDiKSz7olG6t/sZcElNYxfUPKO9c3+Zn76r0uZEfQRnnvTO4O+lAw92UBszwC283NpjfyBdFLQs6mAGsd0Au7Xyes+HUO+hZa96wjaPQgpGa9vxN9gY92M2ViExvIzz2Dv671oacnjcz38uewKVhjLh/14jybw5wN7hBAvCCGeeuvxXm+SUq4GBg9ZfDFwb/b5vcAl7yeziqJ8eEgp6b39dvz/eoz8z32O/E/fPCLpbH+1na0vtTF1aRmzDtODZ7d3Nx9//uN8rK+dlDWPytO/945tgq+2E97Qg3NpOe05/4emxfBuGY3RbKHSNYpIvBXLYA2x6jTpvnZidje5MsDgHgea1NFRU4WnoJCJ4yazfWUbjcUh3qiYiXvgXuh+c9iP+Wiqet55lP+5Iilld/Z5D1B0pA2FEDcDNwNUVlYOYxYURTnRaYkE3d/6FoGnnsbzyZvIv+ULI5JOw+Y+Xv9nPTUzClhy1bh3TK6yrmsdX3rtSyxO65kbDsLZt79jvP3wmz0EXmrFNrOQ5LwOvNtfIhWx0L3NyNwzlpNM7iSnK3NzWfPQDjShw2qC0lQ3/kYb66fNBIOeytllfPee31IVn8NrU0o5q/N+KhsdlE5ePOzH/Z4lfinlqsM9PmjC2TaDI7YbSCn/LKWcI6WcU1CgxodTlA+Lt8beCTz1NPlfvIWCL3/5A82adSRd9UO8fM8eikflctYnJqE7ZP7cFS0r+Nwrn6PUUcpPNRfYC2DOJw7aJrp3EN8T9ZjHusi9rJI9e25DSujfOB+rM4epk08nULIWV/cyjKOd7N24mu7cGgxoOHqCpIWO/lHlhJ1hvrPnu+Q3TqO50MBF4Uf5RMNujNKIMT407Md+NL16Fggh3syOy58QQqSFEIH/ML1eIURJdr8lwPDfmaAoykkr3tREy5VXEdu5k7Jf/ZKCz31uRIL+YFeY5/60A6fHwvmfm4bBpD9o/YO1D/LVVV9lWv407p/yeUwtb7xjvP1Ee5DBB2sxljjwXDuR+sYfk0z60EWn0lvXx4LLP0oi1IHQzBiiLgbz+on4h+i3ZQqy5sYIm6dMJWY20VTUxPnWO7DHTGiFTXyr8a9sYhrG4rEsmDll2I//aOr4/wBcDdQDVuCTZMbq/088BXw8+/zjwJP/4X4URfkvE163jpaPXo0WiVB1373kLF8+IukceoOWxfH2jVdSSn635Xf8dONPWVaxjLvOugvHG7/PlvbfHm8/2R/B+/dd6Jwm8m+YTCTZSGfXQwhhpmN1GTkFRUw78zz64yvI7TwFYdZR3/omRruT2Y42AJz+AF3V5XhqPFx22p8wbTUScev5Ue+X2cFEBsnhCxde+I78D4ejunNXStkA6KWUaSnlPbyzt847CCEeAtYB44UQHUKIm4CfAmcJIeqBM7OvFUX5kPM9+ihtn7oZY1Eh1Y88gnXGjBFJJ3OD1jbi0RQX3DL9oBu0UlqK7679Ln/Z+RcuH3s5v1r2KyydW6FpJSz+Epgyg6Wlgwm89+wGIP8TU9A5jGzbfhMgcSQ/RX9rK0uuuha9wYBP9wbO3nnI8VbqN6+n1amjTPNjiUZpHTcGzZlL2dIruGtlCwWBNPOdq7HqU2wQM1lUs4zgH7cT3eUd9vNwNI27ESGECdgmhPg50M3RtQ1cfYRVwz+whqIoJyWZTtP3y18xePfd2JcsoezXv0LvfOeNU8MhndR4/s4d+LojXPCF6RRUvJ1ONBXlq6u+ysqOlXx62qf5/IzPZ6qYVv30oLp9LZ7C+/fdaMEEBTdPw5hvpaHh58Tj3bhy5rP1vt0UVI1iwuKl9PWtwOKrQqcZ+d2+n1GU1phhDdGTqCQn6KexqhrHglO4rbmfT9UnsOeZmB69iwZRSVCMo7R1kB+Ofojvlv6Md94//MEcTYn/uux2XwDCQAVw+TDnQ1GUDxktEqHji7cyePfd5F1zNRV3/mnEgr7UJK/cV0tn3RCnXz/hoBu0/HE/n37p06zqWMX/zv9fvjDzC5mgX/f8QaV9mdIYeKCWZHcI97UTMVU4iURaaG37MzqdBfovx9/bwynX3EAs3sXu2q9h7TiFDlMvzr4YJncu5yU24bO40Ol09FVV8DtjLrP8cdw9CfIKX6R7nMa+KVamz3mQrgX/w5WjNtIy+PKwn4/3LPFLKVuFEFagREr5/WHPgaIoHzrJ3l46Pvs5Ynv3UvTNb5J33bUj0oj7lrdu0Fp46eiDbtDqCffwmZc+Q1uwjTuW3sHZ1WdnVnjr4fGboWQ6zL0JqUl8j9UTrx8i74pxWMe7kVJj67brAcmYUT/g6dsfp2LyNCqnTmHj5ithMB+zNNJftQ1bXZqK0W101xagTdejzRzgqdHTcaR7uGBHHUlTDbZJ/6JHWoim43QQRaTyKc0/l/mlpw/7+TiaXj0XAtuAFdnXM47mBi5FUZTDie3ZQ8uVV5FoaaH8//6I+/rrRjTob3/l7Ru0Zp799j1BTUNNXPf8dfRGernzzDvfDvqxADx8DeiNcNWDYLTif6GFyNY+cs6uwj4nc/tRQ+PPicU6yc2dQ/vGONGAn1Ou+TgNjXeQ6O8mp2c+bQt+QEfvKyAki2P1dEQy772n8ONYDGb+4EoR75yKxdnEuM29/LK1ku95NULa1Zgn/ovfbB3PS0OOYT8nR1PV8z1gHjAEIKXcBhxm8klFUZR3F3z1VVo+di3odFQ99A+cy5aNaHoNm/t4/V/vvEFrW982rl9xPcl0knvOvYd5JfMyb9A0eOIzMNAIH7kXXBUEX+8ktKoD+4ISnKdVZI4jWEtb218RwsjYqp+x6ZknGDd/MYbcTrqaHiF31ycIjHmSHPNcgs3V6N0WErssDLg9pIXA7yzmn7Omkto9GoREOv7FVcXleI0Rvmf8Mped9nX+seoNLn3hQSa27Bn283I0gT8ppTx0lKD3NWCboigfblJKBu6+h47PfwHzmDFUP/IwlhGYS/ZA7XsGefmePZTUHHyD1qr2VXzqxU+Ra8rl/uX3M8E94e03rf451D0L5/wYRp1CZEc//mebsEz24LpoNEII0un4/l48Y8d8g81Pv0AqEWfuZeeyfduXyd/yJcJj/o3OaCDHcCPRQJCFogF/m5WGUaMYtOfwj1njqEzr2Luuiy7XNn5dOkR5ooLbu/+H8664ls/saWXujjewuT2Mnbdo2M/N0QT+3UKIawC9EGKsEOL3wNphz4miKP+VZDJJz3e/R9/Pf47zrLOouu9ejIWFI5pmyw4vz/zfdlzFNpZ/9u0btJ6of4JbX7uVGlcN9513HxXOirfftPc5WPkTmH4NzP800T0DDD5Sh6kyB89HxyOyF459+35AItGL3T4Oh/Fstr/0PFNOO5M9TV+nZPsNSFcjsbx6xk34DltfW49m1FNcFyBuMhKz2ahx5TLH5eDZR9aiaZKVVc9w+cBivtv6GcqWT+WHLT34Wpso6WhiUomHRDQ97OfnaAL/LcBkIA48BASALw17ThRF+a+TDgRo//SnGXr0UTw330zZb36NzjrcnRMP1rilj+fv3El+mYNLbpuJxWFESslfd/6V76z9DvOK53H3OXfjsR4ws1X/vkxjbulMuODXhDf1MvDAHowldvI/PglhzFw4BgZW09X9MCCYMvk3rH30AXQGPfZRzRTuWYwlWox3wmPk559BjuV02rdvYazZS6TDwiNnXoAllWTxlEn8c+tjdGyP0OrexVcGFnFT3xWsz2mhtayUezq9XN2wmbn5PSzV/oZ/zUPDfo6OpldPBPjf7ENRFOWoRHfupOvr3yDR1kbJj36E6/LLRjzNfRt7ePnvtRRV53BBdtpETWr84s1f8EDtA5w36jx+tPhHGPVv361LzA8PXw1GC/LK+wm+3k/ghRbMY114rp2EzpwJ+onEALt23wZARfkNhPsN7H1jFeMXTcfeEiW381RaT/sJeoOdiqKvcte3/h8CjTHtXqIWK76SQkrCQ6zoX0H9rkEWpS/hdFMOU4Oz2WxcTdn8Rdy2r4M5Ms6c1qc4tbiep9ILMZSfyXDfw3zEwP9ePXeklMM/75miKCe9RFsb/b/5DYHnnkfv8VD5t79inzdvxNOtXdvFq/fvpWysi+Wfm4bJYiCZTvK/b/wvzzc/z7UTr+X/zf1/6MQBFR2alinp+1qQ1z+Ff3WC0NoubDMKyLtiHMKQ2VZKyZ7ar5NK+TGZCqipuY1//+ynmK028ux95DdcT9ecfxM3NDCq8Mc88L3bSQT8LPN0ot+qY83i+RgDYdDDM73PcnXnt8nJFUyNjUWKf7PNkEOdrYBoPM3/br+X+UX7eC09nVWTfsAdU8uG/Vy9W4l/IdBOpnpnAzBy/a0URTnppXw+vH/6E76HHkbo9Xg++xk8N92E3jH83REPtWt1J6v+UUfFJDfnfWYqRpOe9mA7P1j3A9Z3r+e22bdx4+Qb39ltdOVPYN8K5Dm/ZHCdh+j2LhxLyshdPmp/nT5AV9fDDAy8CsCE8T+gs7ae1h1bGT+lnMqGq+mr3kXI8ywO/dk896tnCEfClE41UPaCD58lh1BxHjFHG5GkizNbLsOScjApJbCY1/Kyrhtj5XRejaa4T1/LvKF/sFUby8+d3+Ty8nwSaQ2z4eBB5D6odwv8xcBZZAZouwZ4FnhISrl7WHOgKMpJTYvFGLzvfgb+/Ge0SATX5ZeR/4VbMBaNbAPuW7a/0s7r/6ynelo+Z31yIuv63uDhvQ/zeufr6IWeHy7+IZeMueSdb6x9Glb/HG3aDQzsnk+8oZ/c80bhOLXsoAtEONxI3b4fAjo8nqV4PGey4mdfojCnjGnRjzCY201ixtOk+ty8+aSPQCLF+MkJzty7ia7uPGqnjWdLYT2VgTEIzUZVcBZ2vSDP0ItOdyd1hut4yl3OLaKF01/9Io1aCV9Mfo0Lc/LZ8mgDBTo9Fy9+5wQxH8QRA7+UMk3mpq0VQggzmQvASiHE96WUfxjWXCiKctKR6TT+J5+i/3e/I9XTg+O00yj88v9gHjPmmOVh84oW1v+7iYrpLvoWbeHip79GR6gDj8XDzdNu5opxV1BsP8zMsX174YnPkC5airfjepI9Q+R9ZBz22QfPDaVpCXbvuQ3Q0OmMjB/3PWpXvEq0x89ZlVcTMfuxXNJC245WmldMIJxOMXviIPP37qFzo5ueojy2TSrH2rkHp9WKQTrR4kaqbFBp/jYvaxMYcrgYbQzy8Vd/x6rgx1iTOJer0ybEtgATMVAVP6qxNN+Xd23czQb888kE/Wrgd8ATw54LRVFOGlJKwmvW0HfHL4nv24dl6lRKf/6zY1KPf2Ae3nymmTefbSFa3cMPHV8nti3KrMJZ3DrrVs6oPOPgBtwDRYfg4WtI6avwBr5JOhTDc/1krBPeOcF6U/NvCQYzlRw1o76CGHSy4dGHWFZ6BVKfwnSNmV1vPkjTihpiCE4d38mE2la6NuXRVG5j6/zTmGCJkmiaha4GimMVGASkLI8Q0GBnYjmuwUomP6PjSe1baEhsNthrhIoICAENRQZmDPP5e7fG3fuAKcBzwPellLuGOW1FUU4y0V276bvjDiLr12OsqKDs17/Cee65IzrkwqFiqRiP3reS4EYTtQXrebP8SS4cfQFXjb+K8e73uClMS8NjnyQxqMOr+xkgyf/kVMxVOQdvpiVobb2L1tY70evtmM0llFiuYd3P7meeezkmg5nAZXG6tv6OxmcrSemNnDuukdJaL32bctkxSsee+WdSXFLKnAcfYeXsC8khRCxkA0OK/sBcHkpdiQ3w6P2Um7fyCDXs0xuYM2UsBVuDGCWEr6ri8kmH+cXyAb1bif9aMqNx3gp88YAPVpCZOTHnSG9UFOW/S6KjM9NT55ln0LtcmYHVPnoVwmQ6ZnnoCHbwaN2jtDwfY3znAtoqtrPg0ip+MuZlnKajHNXztR8Rq+thQN6Bzm4m/6YpGAttB23i92+ldu83CIfrsdnGEIk0MKb4m3TftZ0cXOQY3bScuhXR2cWefxvAbODiUbU4aiMMbHaweYxg7wUXYOixMKOohh79OMpMDoQWRZe2INISsykEjp0ssz/FeN1ubg59ldeFg89UxhjcFiQ3BZ2XlPKzxTUjclF9tzr+4a9YUhTlpJIeGsJ75134HnwQdDo8N9+M51OfHLHhkw+lSY21XWt5eO/DrG5fwynNH2FS7yIKFxj47PW3otO9jzC1+99EVq5nMHU7hgIn+TdNwZBr3r86lQrT1PQr2jvuxWQqpLz8ejo7H6Io70KS/zDTF91LqXk0taOexK2fxMp7X8HoEFxavhPqwL/ZxsZxguiXP4dx5QD51iq2rjDB+Ksx6behT9lpMSb5lufz1JuKmZMYoEx4+X7wa7xqGM9FQ6vxNp1NcRzqzivkN2eMRzdCv6SOZiIWRVE+RGQ6TWTTZgIrnifwzLNooRC5l11KwS23YCwe/mqHw+ZBSl5pe4Vfb/41bcE2PGYPnxr8DrrePGafW8X8i99nSbh3D6FHn2Ao+XVMlU7yb5iCzvZ2G4B3YCV1e79NLN5Nvud0orE2OjruI8c+g5yXz6cn1kEpo9kjX8AzagEr/3YPNneSKwt3M1RnJ7FFz5uTjJR999fser4O0nq0tiKKB9djK5tGryGMKVaIK/c1XHgpShopo4f7Yl/jXsMkZg7toMCyhLyIYPOyPP54/iSMupGrPlOBX1EUpKYR3bKFwPMrCLz4Aul+L8JiwXn6aXg+/Rks48cds7x0hbr4yYafsLJjJePyxvHTRT9Dv7KCxn39zLtwFHPPf3+DA8vwAIG/3E8w/kksY+y4r5+GzvT23bj19T+ip/dJLJYKcpxT8Q68gtVSyYTinyOeqWAoNEhhoog6/0bk/BxW/u3vOAtjfNS9i669+chtGtun5ZJ/4W/ZdW8rQzl9OGQp8dwtzE0U86IJdLo0PdLKtx3P4E/YqdS6WaH/FD/WpjHKFGO+KMUZs/PGPCf/d/lUbHodkYCftY8+yJKrr8diH957IVTgV5QPKalpRLdtJ7DieYIrXiDV14cwm3Gceio5552LY9kydDbbe+9omKS0FA/WPsgft/0RgK/M+QofHXs1r95dR+O2fhZeNppZZ7+//uwylWLoDw8TDi3HNlFP3rUzEXqBlJKe3iepr7+dZDKIwzGJUKiWVCrA6PKv4di6iNhTPuKWKDkJO63h3TRZtxB43o+rOMLVudtpqitDvy1J0/gJBIpvYWBtgEhpMwaMlGgap6TmoPPYGDR2A2C195Ef6wBghbaMb4VPx2wWnOX145TVrJ5q5bcfm4HLaKB522Ze+NNviAaD1MyaS82sucN6rlXgV5QPESklsR07MiX7F14g1d2NMJmwn3oKOeeeh2PZMvQO+zHP147+Hfxg3Q+o89WxrHwZ35j/DfLI56W/1NK6c4BTrhrLtNMq3ntHB9DiaQb/8CQx3zScE3zkXH8hQgii0Q721n2LwcE1mEzF6HQRwuEGKkpvxNN2EbH7A8TkEMaJeWh7vXipZ0P/M0gJ7pIIH8vZyu66aizbY3SULaSl+BpqJheQKGijf0eUJckJTEiXkR5qRoyX0K0h7XCL5W6Iwxrm8tvYdXh1klsMYJHVvDHexHeunYaHNC//7a9sf/FZ8iuquOwb36ewumbYz7cK/IryX05KSWzX7kzJ/vkVJLu6wGjEsWQJObd9Ccfppx+TYRUOJ5gI8tstv+XRukcpsBXwm2W/YYZxPjue7KBu/T7SKY1lHxvP5FPex3g1mkZ05UqGXouSTuaTW7Md5w1fQMo0be330dBwB6Ch1ztIJHooLDif0ugNxB9LEg0OYZrqpsnQTvlWScDWyKu7n0QC+cUxrs3ZzObacTh3hugoPZXohTdzxSVjSSZ9/O3BTYxJFzNOFBHdcg/WKXm0TfwCWv8z6NEoi3fSqqvgzsTF1OqsfKbQg2VfhM01Rj5y0Vjc/R3c/4df4evuZPb5l7Dko9djGKFeUyrwK8p/qURHJ0OPPEJgxQqS7e1gMGBfvIj8W27Becbp6HOOX49sKSUvtL7Azzb+jMHYIB+b8DEusVzLvif7eXjPRvRGHeMXFDP9tArcpUf5CyQeIr3hUYZeCxGNzsSg76NgQRvmC24mGNpL7Z6vEwztRK+3kU7HcDimUmW9Be1lO9GuMMYKB9vP7qV/7SZO756Dr2ADK/e8gkRPUVGCa3LfZH3tJPJ2DtE09jSm/fyHlI/JY2hlK0+vfga9Tsf0gioi9/8v5qoyin7wBx7/1kaSzgCTRD1pdNyZuIINyWoutDpw7ouws9JIQWWE4q2v8tBjD+HI8/CRb/+IyinTR/T8q8CvKP9lEi0teP/8F/xPPQVSYl+4kPzPfBrnGWegd7mOd/boCHZw+4bbeaPzDSbnTuWbzp/T/3yal3vqsOWamH9RDZNPLcXqOMrS7mATcsNfiWxsZyh6DRILOdOCOC+7BM0oaGz5PS2td+3vBWQyFTCq8EsY1o4mvnsQXW6SPct93O37I5e9NI/TA3NpynmCbbvqSUb0lOdH+YhrE2/smUL+rkHWLT6Xq/70c3T7/HT/ahMt/k46TANMqxqL9f47kFYjtu/+kge/swYRF0hXEqsMs8e4iGfjU5iWNjKhJ01dqQGCK5i6I8raxn1MXLKM0z/xmWFvyD0cFfgV5b9EbN8+Bu76M4Hnn0cYjeRdfTWemz5xzLpgvpekluS+3fdx5/Y7sSdyuVW7HbEmj33hIAWVTs68cRJjZheiNxxF33wpoWklbLiL5N5d+FKfJ6Gdh6lUR97VMzEW2BgcfIPavd8iFmsDQK/PZVTZ53HULiHyTD+afoh9p/r4m/YIjQ31/KrncxT5SlmVuJOeZj8gmVrm5yzHTlbumUbxLi//Ou8SrvnSV4ncW0u80Y/fFOMV404cDgfTX36exMAAwU//gpf/sBeDjDLb8wAvMgWdTPGLyFkUJuycFjHRXKDH4H2EUQOd+M0Wzr/1q0xYdOqInv8DqcCvKCe56K7dDNx1J8GXXkZns+H5xI24b7gBQ37+8c7aftv6tvH9dd9nqCPG5YFbyGkvJy6hZnoe088op2SM6+j65SfCsP1h2PhnZF8DQd31BJKfRZgN5J0/GtucImKxLvbs+Ape7wuAQAgDFWU3UNB/GZEHB4lE+2ic4ePvjsfZ1L+Z6cYa7uv6Oj09nTw9+EfS6TQea4zzq+pxSz+v7p5O6e5+7rngCpYs+xjmO3eRsOipK/ayxrcdBJzqGyK4s4F9i79O/w495fotnOn5PY9alkESjJqRlvgEroiY6HFGMfY8Rn6gj/Ip0zn387fhdB/bz0oFfkU5SUW2bMV7558Ir16Dzukk/3OfI++6azHk5R3vrO3nj/v5zabfsHnDPmb3XUL+UAVGs56Jy0qYdloFuQVHOQ2jrxU2/hm23g8xP/G8C/GZf0IqYMQ6vQDXBTVgk7S0/IHmlv9DyiQAHs8yqvRfIv50jFB/Hy1jBri/+FnWDqzHg4efVF1HxYoi1navYCDWhUmX4szqDqZY21lnn4Zvo4FRu3v4y0VX4px4AbPW9mOdV8RL/k3sbdmH3qCnymQm+so2ts37XzRNz9LcO8nN2cbNE76Bpy6CXYb5V/xMLouY8OubsHW9gDkZY9TMOVz61e8g3s/dx8NEBX5FOYlIKYls2ID3T3cS2bABfV4eBbfdRt41Vx+zYRSORiQZ4V+7Hmf1KzsY3T6Xc+Kn4HCbmX5FBRMXl2K2HkXokRJa1sCGu6DuOUCgjfsI/tT1hHdr6F1mPDeMwTI+D6/3Fep2/JB4vAPQYTDkMKb4fzGuGUu43kd7kZcHF77IqqHXyQ3l8uWZX2B2zyAtDwV5ZWgNAsnU/D7Oyq+j1VLCr8OXsOyJLVQPpnjw0gtpnXkh/+c3kvP58Ty+8mnqW+opLS2lr7MP03bBrsmfpNBQzzLX73iwZhmPj32YH04aw7M7f0dMMzAuOJp4/CVskV0kDFYMRiPnfu624xL0AYSU8rgk/H7MmTNHbtq06XhnQ1GOGykl4dWr8f7pTqLbtmEoKMB90yfIu/LKY3qT1Xvxx/08+Oaj1K7sYVT3LMxpK7lVJhaeO45R0/LR6Y8i0CXCsOMR2PgX6NsDVjdy9o1EnVcx9PIQWiiBY3EZOWdVEUu3sW/fDxgYXI1OZ0bT4hR4zqOk6xP41viotbbzRslO6qIN2ISN2fmzqbHk0bt7D36vRkqkMRkhzxIjKYx4hQtDPA1SkDIa0HR6Enoj+RYbTpeDoaEhwuEwJSUl9Lb7cAQLEDiYZF6FPr+RH427mStLC/hUcA2pzf/k9vAleONuxrStR6aD7HZNY1pwN9NOP4szP/n5Ef88hBCbpZRz3rFcBX5FOXFJTSP48st477yT+J5aDKUl5H/qU+Redhk6s/m9d3CM9EX6uH/lv+heF6eqfwpCCAqmWFh2/jSKqo+y2+hgE2z8K2x9AOJ+KJ4G8z9NquJChp7tIFY7iLHUTt5lY9EV62hp+SOtbX9DCIGUaYwGF5bo9bRtcNCa7iUoYodPR0pEOoVOShz6CHZdDJ/eiRZMU+AdQk+KeJmNVyoWIA1mLsqzYU0laG5uJhKJYDYYiSeTR5yMVkcavYgR1EksaQctsSFqevt5Nu90bqqME1z3HDf++i7cpcM/l+6hjhT4VVWPopxgpJTE6+sJrVpF4KmniNc3YKyqpORHPyL3ogsRxiNMMHIctA618uCKp4husVIcmECVMcmoU3JZet40nG7Le+9A06DpVdjwZ6h/EXR6mHgRzP80snQu4Q09+H+3G6Qkd/ko7ItK6fM+Q8OGnxGN9qLT5SCln2BgErt3TSGZGgC89Nu89Fn7mDl+JldNvQoZ6OW1ux8k0NaJAcns/B4WeupY75zGqu7xnP/aSxjjccKnajx5xhX81X0F5zkd/GxaNR6Djscee4x4IMBZYQN7wqOJ6woosa6lxP0KUaORilgfnTo7r9srqcOAXuoxp+yAhSguXimbwZxxE0it/h01s+cdk6D/blTgV5QTgBaPE9mwgdDKlYRWrsrcXQtYpkyh9I47yDn3HIThxPm67u7Zw2PPvIJhVyG5sck47DEmX5jPotMnYTqa+vtYALY/lGmwHWgAewGc+v9gzifQDPmENvYQfnATaX8C87g88i4ZQ9jQxKurvk17uxebNQdPfi/xeILG+tPQDUxCbw6wtnQjHaZOzht9Ht+Y8Q2cMRcv/O0eOnZsRKZ9jHKkObN4EwPOfG5P38jSZzZyac/rxCfp6LrMyM9LvkyXmMFfJlZyYUke6XSaJ554goGXX2H+oIMdnrNJGTTszte4xPp7IikTvzbPZk1lJT26ViR7SIcmoA3OpshrR3isTHZ10ESC6zxe3ggGmHPBpSP/Ab2HE+c/SVE+ZJI9PYRWrSa0ciXhdeuQsRjCZsO+cCGez34Gx6lLj9mE5UdrXcObrHhmIzkNlXhSk9EKwiy8spIZ82uOrv7eW58J9tv+AYkQlM2By/4Cky4m6UsTerWLyOYmZFLDVJND6nQ3+1Id7HnsX/T0JLDZbIwb14nN7icVmUvuzkuxy1b+MfppeqWXxaWLuWPWL8kPl7P6rpdo376WdGIvDgOcWbGHwtwodzk/SsmaJq7cvQJR6sL7mTStk4v4qf7rnOocwyMzqskzGkilUjx7//04Hv03WslZ1OdPwmJuoj+nmz7RxXznlURz96AzdqIlc5ADpzO+exKjB7bgSr9J7VwHY33V7DQ1kPQ8wdpnxlM4ajTlE6eM/Af1HlTgV5RjRKbTxHbuJLhyJaFVq4nX1gJgLC/HdcUVOJYuxTZv7glVdw+ZqqcXt6xm3Yo63B3VFMgJ6EaFOfvi8YweX/re/e+1NNS/BBvvgsZXQW+CyZfB/JuRpbOIN/oJPdBArG4QdILERAvNuYPUtm7D+/wAAFbrENOmNeJw1mI0FFLWeSvbGyL8sfTvNBs6mOSexA9n/RhPewlv/ORJfF07kOlehJDM9fSwIL+FR/LPoXuPleVrXsNgMZO4ajT9i2rZYFzIk/IWfj15PGcWuUin0+xY/QwDj/8VT5OVPVWfBKGjwPEMr3jCbHd2EbK3AqCLTyDaczELvZOZM1RLMvo0AbvGnpljqRnMwy/NlOvOZiIO5GAHm6b0cnq4izLH8a3qUY27ijKC0sEg4ddfJ7RyFaHVq0n7fKDXY5s5E8dpy3AsXYpp9OhjOmft0YrF4zyz5jVqX+vFPVBBSpfANjXJRZcsoajkKO4ViAVgy33w5l/A1wLOEphzE8y+AWnxENnWT+iNTpLdYSK2NB2VYeqj7XT39gDgcvnw5O+josKIweAjHu+iQLuA/o1Tudv9PDus+yh3lPOFybdge1PP7pUvEg/VAxpOU5yZrm4m53az2TONFQPzuPiFl3GGojguPJv6BTtJ57byT3E1OY6P891xNuwtaxjc8m+MHetoHahma+JjhORonMYGnih7k478HcSMEWTaSUosItYxk/xAHhdFDeT4XyWd2EVrQZpd7mUs0cWIpQSpiMb3vvVFnv/9L+jpbObhU1tAJ/jm/G9yQc0FI/65q149inIMSCmJ19URWr2G8Jo1RLZuhVQKfW4u9qWn4li6FMeSJehzc493Vg8iNclQX4S+lgAN+zppqu9Ges3opYGYOUThPCOXXLQUp/Mouo6GvbDhzkyDbdwPlQth3s0w8ULSUUl4Qw+hdV1EQhHaXD6abV7aBzNj1rvyUrjzdpJf0ERR0RT0egcDA69h1hWj33sp9yW383rOVvJMeXwi/1rMb4TorluH1KLohcZYp5e5ng7yLAlWeJbxcnoG5zzzGqM6urDOnEn66rPYa/sdmi7Fo+kv8plEgEUtjyL69wLwRnIua4PXY0uUY9AF2F2yipUVryFFmsJoIRHjMjoG56H3pliQMrIwmCAZeRKR7KZtXD4TZ1xFz84N6KJhpha6Wf7ZWxnq6eKBb3yJU6/9BCXL5vHN17/J1r6tnFd9Ht9a+C1yTCM3WJ4K/IoyQtKBAOG16witWU14zeuk+voAME+aiGPJKTiWLcM6fRpCrz/OOX1b2B+nryVAb0tg/99ENA1AUhen39GOuURj5pQJnLtsCUbjUdQK+ztg7R9g898hFYOJF8KS26BsFsneMKHXu/Bt7aRV66Mld5D2WB+a1Mh1mSjIbyLXtQmnE3JyppNOhwkEtgESR+BMnmyTPJuzAbtm4arg6ei2d5MIewFJoTnMXE87Y3MG6DKUc3/ZpWwNl/ORl15g6r5aDEVFFNzyOd4w78Zoe4heUURH11JubfozFiFoEVU8Gl1MR/Q0xkes6EnTW/Aqz1S/RtpkJCc+iWneAlJaISuCY8lJC64KmslJDRKJ/BOdFmXKtVdg0RWyav0G9LEw55x6CnOXX4QQgmd/9wuatmzk5v/7O2abnZSW4m87/8aftv+JAlsBP17yY+YWD+9EK29RgV9RhomUknhtLaHVawitWUN02zZIp9Hl5GBfvAjHKadiX7IYY+GJ0TCbiKXobw0eFORDvjgAQgcpV5gm8246bA3oCuOcNWMpF4+9iELbUeZ/oBFe/3VmDB0kTLsKFn8J6RlLvN6Hb00bjU2NNBr6aNd7Sck0OTl2ysrC2OyvYrV2YrVWYDTmEgrtQ8oEZlGGxTePVV0RHrJuYFSnmXkdlej8YUBi0SeZmtvDLHcnBqNknW0efyu/BtE2xLUvPEVldwd6twv3KdXEKyw8Uy0ZY1tDXWo68/ryqbLk8FK7iSe9ZRCrZk7EjE3qGHJt4Jma54kbo0QcFzLkPpul23cy3t/JM4nJTAvnMCdpIJVoIBF5Bp3ZwOVf+x6b1m5gd3sX5kSM6z5+A+XjJwAQ8Pbz11tuYtZ5F7Ls+k8ddNp29u/kG69/g7ZAGzdMuYFbZtyCUT+8XXVPqMAvhGgBgkAaSB0uYwdSgV853tJ+P+E33iC05nVCr68h3e8FwDJpEvZTT8Fx6qlYp007IbpcptMa3Q1+WnZ4aa8dZLA7DNmveU6BFU+ljX5nG28kXubN1BqEEc6sOpPLx17O3OK56MRRDiPQvQNe/xXseTLTYDvrelh0C8lEPuHNvTRs2cu+aBsthn4SpLBZbdSMdpLr2owQryGEAau1nESin3Q6jEHLJce7kMHOCp7V1dMSrmVii4O8oAkB6NCocgSY72mh1Bqk2+Liaevp/KvkCiZt28KVrz6P2+/DWJJD/oQQNs8gT5R+hsC49dTo9tHJxcwpup6nV25mZSdo8RIWR014NB1BRz0vjnqKuLEFd2Qeb477JPaeOOP21DPX3k17qpAJg+Owa4KA9iqmwHacxcVc9uVv8fj999KdlOTqBZ/64q04cl37T9GqB+5m87P/5pO/+ys5Be+8kEaSEX7+5s95rP4xJron8tNTfkqNa/hm3DoRA/8cKaX3aLZXgV851rRolNiuXUQ2bSK0eg3R7dtB0zJ19YsXZ4L94sUYCgqOd1YBiIWTtO4aoGWnl7bdgySiKfQGHWXjXBSPzqWwykm/s52nOp5gRcsKoqkoY1xjuHzs5VxQcwEui+voE2tdlwn49S+CyQnzPkl62s1EG6DzzUZq+xtp0PcQEjFMBiPjxtdQXNKFJp8gmexGr3Mg0JPS/AjNjKNvFrauuWxOx1mjfwNze4CqXjt6CSDJM6WY7elgUk4XabOgzlbGP0zL2ZYzh8Xr13PB669ijccwl+lgYorVU87ltYILiOXUc4X+Hhwygt55G2s2mFjVrSOWyOO0mInKlJ6odYBVlY/jy9nBJZtTtCcu4t8VZyCiaUBynnEvBSJGwcAcREpHn3yAXP8gNTPnMfuCS/jnvXcTtLsodbv4xOe+gOGAC7+/r5f7v/ZFqqfP4oIvfe1dT+krba/wvbXfI5aK8eU5X+aq8VcNS8OvCvyKcgRSSpKdXUS3bdv/iO3dC6kUkLmJynHqKdhPOSVTqj8B6uqllAz1RmjZkQn23Y1+pCax5pionuKhelo+5RPyiBDi6aanebz+cRqGGrAarJw36jwuH3s5U/OnHn1wkRIaXoE1v4S2tWDzIOd+nmjOFfi2D7KnqY56XTd9Oj8CQXVlOaPHGXE6N+MdWAFpiUHkkNL5QOqwD0wmp2shCcbzknktLa3bKG83YUq9nWSNM8yywj3kmuL0u03sNI7lcXEuvboiTlu9gWVb1iOA0Og8Vi6bx+NTLqXV5GS03McN2t+p0dURp4o39l3I861lJNMOTksYGRfTkTLGWV/2DF35qznTa+CiR6O8ULSEP0+9iMm+dkqNBgpcYfSmIHb/GEjoicQexRFKMv/iqzA7Hbz00osk3EWMqxnFR6+9Dl12wLXwkI8NTzzK9peeR6fTcfXtdxzVvLn9kX6+/ca3eaPrDU4tP5XvL/o++dYPNlzziRb4mwEfmR+gd0kp/3yYbW4GbgaorKyc3draemwzqfzX0uJxYrt3E926lei2bUS2bdtfdSOsVqxTp2KdMQPrzBlYZ8w4YYY5PrAKp2WHF39/FABPuYNR0/KpmuqhqCqHuBZndcdqnm9+nlUdq0hqSabmT+XysZdz7qhzsRvfx2TqWhpqn84E/J4dSGc5iYlfJRSZw75djexLd9Cm95JGw51jp2pMkry8LSSTW9ElHOg0CynzAAiJxV9DTt8i3NYz2WFtYf2+FzHWDWGNvx2DdAKm5HmZ727E4EjTWWJlizaF12OLCUf0nLF6C7P27SFpslA3fRY/uvgqejyFWNOSZb5uzrc8RJ59LRIXrzefzqMNC0hpFs7QTEwK65AyzY6SlTQUv8BVoQHSzXM57aVdbCsbj99WjcE9Fm+JRsIygEgbsYeq8IomPP1bMaYkZ3zyczRu2cDu9m6S7kJmTp/OhRdfjE6nIxYOsenpx9n83JOkk0mmnHYWCy+/Gqfn6IO3JjUe2vsQv9r0KxwmBz9Y9AOWViw9+s/rECda4C+TUnYKIQqBl4BbpJSrj7S9KvEr/ykpJanu7v0BPrptO7HaWkhmxms3VlTsD/C2GTMwjxt3QtTTvyUWTtK2e4CWHV7a9gwSj6TQGQTl4/OonppP9bR8nG4LKS3Fxu6NPNv8LK+0vUI4GcZj8XDeqPO4ZMwljHePf/eEpITIIPiaM33ufc0w2AJt62CwkVTufCIFt9DeaaU22EajvoeoSGA26ikrH8LlXo/N2oMhmYtOmEia+gEwhotxBZdQmHcBsVwnL277J95NtZjDiUy6wgIyhlEnmZXXzkx3F5FCaC3MYWd4OrWhSSS8Uc55fQfV3Z2EHXnsm3UGPzn/HErMDuZ2xJnvD1A5+gUGcp4hLQWvti7l341noaVMzMfE9JAOa1pPff4mtpc9z8diDcyIurnT8DG+eO/9xM25bJn+GXwFIWLWHoTUYwmXMWAOIvS1FNf3YLHaOeXjN7L2Xw/Rb7SRzPWwcOFCzj77bFKJOFtXPMObT/6LWDjE+EWnsugjH/tA4/E0+Br42pqvsc+3j18u/SVnV5/9H+3nhAr8B2VAiO8BISnlHUfaRgV+5f1K9vbiu/9+/E8/Q6q3FwBhsWCdMmV/oLdOn37CzFKlaZJAf5TBrjADXSEGOkMMdoUZ6o0gJVidxv2BvnxCHiaLASkl2/u381zzc7zQ8gKDsUGcRidnVp3J8prlzC2ai153QLVUKgH+9rcDu68l8xjM/k0ED8pT2jaaqOk8vKlT2OsboF7fzYAuhADy8/vJL9yFO68TiyxGI0rKMARSYPGPxpVcQEHpOZjKqli/8Xl2rVyJ8A1ldqzzoBMJtHQQqyHN7Lw2JhX0MVCup95VwBr/6Xi9Nqr2dbFkRx3ugJ+Bwkoik89k49lnUO3VmNUaJdcq6Jm5Aa/5AfQEeaNrHk/WL6cyaWO23oV9QMMsdXQ5G9hc/QJLZSNfHGjhoZLLWNG/kO/8448M5VSwfuGFRJw9gMQUL8BcbKVSi5LctY/ucAMFxdWMPXUJ6x57mETlWGIWO0uXLuWUJYvZ9epLrH/8YcJDPkbNnMOSj15/VNU6RyORTvBg7YNcM/EazPr/7G7uEybwCyHsgE5KGcw+fwn4gZRyxZHeowK/crRidXUM3n0P/mefBU3Dcdpp2BcswDpjBpYJ44/7yJZSSiKBBIOdBwf4wa4wqaSW2UhATr4VT6md/AonlZPdFFXlIHSZ+vgGXwPPNT/Hc83P0RnqxKw3s7R8KctHLWdJ+ZJMkEgloOFlqH8hM9yxryXTz15qb2dGb4a86szDPQryqklbRhHuK6SzPkhbdycdYpAOvRcJOG1DFJbupbioF7vOQ1z2oukiiLQB2+AUXCymoOJMIjbB9k1raNz0Jklff2b0Yl0hwpSHRd9NNBzAaYwzx91BWfkQe4omsjF1DqIxRnFjAxPqa3FGwsSNJnqqp+KpXopj8hycbSHQIFRpY2vZNkymu3GZe6gdGMeq2uWMiZQxSnOTDGgkRYpmz1ZaS3biduv5WcML6ISRH+d/krKtlUzf+wp1E6bQX5RC6lIYpYdFU+bgbu5l5+6X6I21YjJZmXra2QwN9tCwaSO6ybMJpOHMM8/ArSVZ+88H8ff1UjZhEks+ev0JMQbPoU6kwF8DPJF9aQD+IaX80bu9RwV+5d1IKQm/sZbBu+8mvHYtwmbDdfnluD9+Paby8uOWr1Qyjbc9E9wHusIMdoYY6AwTCyf3b2PNMeEpteMpc+B+62+JHaP54AbkzlAnzzc/z3PNz1Hvq0cv9CwoWcDymuWcXnE6DpMjU13TuTnTn37XYxAdBEsu5I/LBvhRBwV5HMWg0xEdCNG8di8tdU10BXrpFUMkReZmLrMxRnFpHVVVUaxCEtVakCKFPuHAPjADt/EUHEXz6Ip0sGXdKoYa6zMXHQRCX4JmKcbmkRgH6wiEIuSZIsz29JDnHkcgciaJriA+7w7y23ZjSiUJ2Bw0jpuGrXIas4rnYojq0YIJIiYdGyotbNbXMS7nXsbnNdATKGXv9kvwBMZhjNhBQp99kN2FL9Gcv518x0Ru9Pbz0f41bLZM4vW+L2EI5pA2tjHkGUTTxxHCw/KZi9Ftr2NH/asMxruxWnOYdf5FWN0u3nzyX/i9/Zhnn4I3GGLu5Il4N6zG295KQXUNp3z0eqpnzD4hh9yAEyjw/ydU4FcORyYS+J97jsG77yG+bx+GggLyrruOvKuuPC5DIiSiKbqb/HTVD9HdMERvSwAtlfl+Gc36TGAvteMuc+Apc+AptWN1mo64v85Q5/5G2q19WwGYUTCD5TXLObvqbDxWT2ZDXwvseDQT8AcbwWCB8cth+kdh9OlwyE1BkUiElt0NNO2op727g/7kEJrI5NNpSJOT2427tIOiQiu6VICobMkcQ6QQx8Bs8qynkLaXUtexg4atm0j5MvX5CDvCWIl0eyh352L3N9HZV0coqVFgDjHbHSVfm0NPv4Ohvj3kdzWik5Iedz7bJk9EP2Es53hOw9GSRMZSBJDs9Bh5xayxw9/KBaOeZEHJJob6JtC1+3z0vrGQ1mPJNbDL1cibuY/gt3UyJ27lmwOdjE2ESKFnTfRq9gydT8IUIpTTStoYRkuZWThxIa7dTexqW00g6cWZk8+M5ReQSiXY8coKwr5BXGWVJEdPpre/n8JUlEj9bvJKSll05bWMX7DkuE2deLRU4Ff+a6QDAXyPPILv/gdI9fVhHjsG942fIOeC89GZjhxIh1skkKC7YYiu+iG6GoYY6AghJeh0goIqJyVjXJSMziW/3IHTbdlfVXMkg7FBNnZvZH33ejZ0b6Aj1AHA2LyxLB+1nHOrz6Xcmf0FE/XB7n9npihsW5dZVrUEpl8Fky7OlPTJdvscGqKtrY3mukbamtsYjA4BoJMCj85Crn0QW1EtnsogVqObWKyTJJlgbvHX4PDNxmGdz2Aqzc66N/E1H1CqN5SSthZjK/FQZhKkeproHmwkoWno0Ci3BXA77AQCLnJaOijtHQSgsaySDVMmkRhTwYXp2ZT3CJrR2GOBvXYdu5JJmgMxLPool497jUWeNwm1zmWgYSkymo/Ua3hLAvQ617Pb/jI+Y5LFkSi3+YbISeXRFZtBf3ga7bFJhE16wo5GkuYgtnCUcuGmNG1nb886wik/7oIyJp1+BkO9Xex9YzWpZILK6bMpnjWfHfUN9A8MYOloIs+kZ+HlVzNl2ZnoToAuvUdDBX7lpJfo6MR3/30M/fNfaJEI9kULcd94I/YlS0b8p7aUkuBAjK6GIbrrh+hq8DPUGwHAYNRRVJND6RgXJWNdFI/KfUdVzeGEk2E2927eH+j3+fYB4DA6mFM8hwUlC1hYsvDtOzlTCWh4KVOy37cC0olMNc60q2DaleCqRNM0ent7aWtro62tjdbmVkKREAAmaaBIy8VjltjdTegrNmIryEEkLUQSjWgigkgbsQ1Mxhmei9CPpXGgg+Z920gOHViqryLuzqW01I07Emawow5vKDPIml2foNo+iElnJhYU5DUFcA0lSQvBrjHjWTt1BkPVJZziH4VtQE+tTmOPGWoTSaJpDZCMcQc5bVQHoyzNmAYChFpnE+qZgpB6Qs5Bclx7ybM8zRNOH3vMZsbGU5w5VMjo3vn0xWYwlMpHM8RImUKk9R1EHBFEMs20ndsxWYto0QeIpcMUlY1m1II5dO2ro3XnNnR2J56ps0jZc+ns6UHTNEinyfX1cOp55zP9rOUYjmHBYjiowK+ctKI7dzF4z90EXngRhCBn+Xl4brwRy8SJI5pucDBG666B/VU3b41vY7YZKBmdS8kYF6VjXRRUOtEb3vsnfzKdZHv/djb0bGBD9wZ29u8kJVOYdCZmFs5kfsl85pfMZ5JnEgZdtkuplNCxCXY8DLsez9Tb2/Jh6hUw7SoSBVPo7OzcH+jb29pJJDNdJe2YKUq7KNJyyXP40RVvJVK6C3NuPlosRSTdACKNPuHE3jcDS3IuPUO5NHXtJtTTCKkIb5XqU9YiRFkuo5w2DP19dHXUEk1lLihFlhCjrANYEoKwV4ezNYklrhEzGtk8YSpvTJ9NS3U55YNO9D066oSkO51pQzDoYEl1mHml7bhj7TCQIDlYSmywhkSwOHMKjGGKnVs4RfcYAWsXv3Tn8YbNgi1pwtO3iIXdS7HqY6SMIVLGMClDCHSZuJbWBEXeBCV1a+jw5JISkvLRkykcN4aGbZsZDAYRniKky0M0kWl70acS6PyD5Bh0zDl1GbPOuQDzCTSh/fuhAr9y0pDpNPH6eiKbNxNc8QKRN99E53DguupK3Nddh7G4eMTSDg/FadjSR+PmProb/QDYck2UZoN8yRgXnlL7e1bbQOZmnL2De9nQnQn0W/q2EE1F0Qkdkz2T9wf6GQUzsBgOmJ821AetazOPhpcPqrcPj7+cNn01bR2ZYN/d3Z0pmQJ5wk5x0kWR5iLfoEPnbiTs3kO8vAWzxUMi5iVGpvrIECxF1z+TSGAqre1RwoNNaLFWIAkYSVmKSRYUkF9RTHkiTaClid6BJjSZxqiDKruPaoMXqz+Fv9eMtVuilzDozGHttNmsmzqThtJSDD4d/k5IJzJ5LM0xceroIOOsjZiH+tC8JpKDlcR8lch0psuiMIbItbQymh1UGHZTYqplde5o/uDKpU7Xj04zUjY4i5lDozHq45Btn9BpOnTCTHvahiuhZ2w8TV7fbtpSbWhCR8WECRgLDfQMNpO26NDZjegNSYzGFFZjEhkdwECUHLcNR0EORitImaaw4BxKyz6KxTxy/3cjRQV+5YSlRaNEd+wkumUzkc1biG7bhhbKlCaNZWXkXXstro9cgd7hGJH0I4EEjVv6aNjcR1fDEEjwlNkZM7uI0bMKcBXZjqoqSUpJ41AjG3s2srFnI2/2vEkgEQCgJrdmf6CfWzz34DHYh9qygf6NzN+Bhsz+DDYGS06hzbOUNq2Its5uBgYyM1LphSAfB8VJN8WaC4/ORCqvnkjxDtIlQwgHyHiaWKKDlBgCTYccGEukdwZ97aMI9/vRko1oqQ5AktIbiRW4cI0fzaScKtINvXS07MYfzVTx5Jj0VDn6qE71Yfam8HVbMQ9lzkljaQVrp81m/aQpNNgKkX0C3UACO4LJhVZmV/RQkGxGPxBGDrhIDI4iFc3eDa1LYrV3Uqqro4adFBv3YTAG2ZMznrWWqdRrBXSk9tKcsxcpJKMDoxk/NB5LyoYhYcScTpMgRSDgJy86gCnlR0u/Pe6DkJLiVIDcK71YChNH/uw0gZAWTJY8TGYXBoMDg8FJOh3B51uPEDoK8s+mvPw6XK55J2wvnkOpwK+cMFKDg0S3bCGyeQuRLZuJ7d6zf1wc89ixWGfPwjZrFtZZszGWHcXUfv+BaChB09Z+6jf10bXPh5SQV2xjzJwixswuxF3y3sMaSClpD7azoWcDb3a/ycaejQzEMoG5zFHGvOJ5zC2ey/yS+W8PcSxlZt7Zt4J827rMTVVAypxHd+Ey2iwTaU/k0O4NEg5n2hFMOh2F2CiJF1Ks5eEyaERKNhMvaSXtHiJtDBKP96LJWDYdQTpYTMRXQaB7CsGOEpLhTpLpOnTJIQBiDgPW8eVMq1iAo0/QsW8XXYP7SGpxdAjy7Xoq7D2MDnUhegT+biu6mCCl07Ft3CTWTp3NulET8aZcOPrjTIpIRnt0lJTWY0m0Y+izILxVJPxlIDNtHkZrP3nWJmrkTsp1dbgMbdTmlLM+p4at0sVQLE0qFcoMi2yMMmQaIqlPUjlUzoz20eT4U+iiA4hklP3/FUJidqawGOPYQymcXUkcgRQWLUm82kzLaSXEDRasBidFxdXIoQjtW3cSC8TJLahm2mkXMnHxWZis1sN+zpFIK51d/6Cr65+kUn7s9rGUl11HcfElGAzvY/iL40AFfuW4kFKSbG3dH+SjW7aSaG4GQBiNWKZNywT52bOwzZiB3uUasbzEwkmatvXTsLmPjr0+pCbJLbQy9q1gX2p/z4tMd6h7f4l+Y89GesKZaQILrYXMLZnL/OJMiX5/7xstDb27DijRr4NIZlygsL2SdvcS2gw1tEctdPX7SKczVSJ2k6RAs1AaLaPADGZnH9HCOhLuLpKWfpJyYH+eRNqBNlROdLCcwFAZUV8JMZ+VuOghTD2GcA/mRDozMnOpi3E1CyjTldHf1ESvt56BWCcSiVkP5VY/o1L9eIaiJAYMxAaNCE0QtNlYP2UmayfNZHvReAqDBub2p5gk0xjLtjGga0QfdGEZGEdiqBqkDqFLYM1pIsdci0lfT8zcTJNdR6PFRZfeiC+dIKKLkdKnDzrHQgqsSTO2mJ6ckI5JTXY8QQeayYrTM4ijwI8pJ4HZmcARgJxtRiw7EhiDadJ6Hd1lRbSWV9Of78ZqNjJxyhQqKirYu+oVmrdtRqfXM3beImaccz5lEyYfdcEinY7R2/sMHZ33EQzuRq93UFJyGeVl12K3jz6qfRxrKvArx4SWSGQGQNuylei2rUS2biPtzQQ6fW4u1lmzsM3OlOYtUyaPePfLeDRF8/Z+Gjb10V47iJaW5ORb9pfs88sd7/rF90a9bOx+O9C3BzOl8zxzHnOL5zKveB7zSuZRnVN98H56dsLGv2S6XMb9aIDXOYX23Pm0i1JaAxo+f6Y0r9NpOOx+8nVQKnLwCDPS2UUsr5G4sx1NF8/uVIdZV4H0VxDpKWaopxjfoItEKIWW7icpukilezEkom+1bSL0Bkoqp1LunEgyECUw2ExPZB/hVCZtjyFKRWqIgkAEc28aLZIpmScNeuoqa9g1ejxbx04h7KpgSa9g+SCY7A3sdtYSSRrRDVUTHxwNmgFEGqezAb11Dz22Ot5wdVN/QNMFABIccSMWzYZZOrCmrFiTVuxhPTn+GDn+ONYgGA2lREzltFtK6c5LcuncB6jRp7EEK5DNVpI7+7A19mINRUjrdPQUF9M3ZjSGeXMYNWky4yZOIhkK0Lx1E9tffh5/bw/2PDfTzjiXaWecg8Pt+Y//p6SUBALb6Oh4gN6+55AyQV7eIirKr8PjOR2d7sQZ60kFfmVEpLzezABoW7YS3bqV2K5dyLcGQKusxDZzBtZZs7HNnoWppmbEb3hJpzT6WgJ01Pno3Oeju9GPlpI43RbGzC5kzJxCCiqdRwz2sVSMLb1bWNu1lrXda6n31QPgNDqZXTx7f4l+bN7Yd05YkkpA7VOw8S8k2jfRqauivfA02mQh7b4E8USmZGs0JsjJ6SEvt49Cg5FcYSdt9BF3tqIZMyNu6oQZh2MShthooh1FeJty6W2HSDyATHvR0v1oWj9Ce/su4LhdYCzMozJvIgVaFSKgQwa68EZ20x3tIyUlejRK0kEKA2FcvXEs8cwvDF+uk22jJ7F79Hj2VI7G58xlciDEx3us1IghOq0t9MgIkWAx0YHR2YZYDaejBbt1NwOWOl5397HTmiAlJDoNyvpt5PkLMOrcmI055OisWFNWpCYgGsMa9GKIBNDF4+j0xUhjBYO2AvpcJmIOPzl6jSrNzhmaiaA3RLp3G86mWpz+ITQhGCgtITl/Pu7zzqNq0iTiPi+dtbvp2LuHzro9xIKZ9pWyCZOZee4FjJm7EP0wD8CXSHjp6nqUjs5/EI93YzaXUF52DaWlV2IyHf9xoFTgVz4wmU4Tb2jMDGe8dQuRrdtItrUB2WqbKVOwzpyJbdbMzHDGx2AANC2t0d8WonOfj846H10NQ6QSGgjIL3dQPsHN6FkFFFXnHDbYSynZ59vHuq51rO1ay+bezSS0RKaLZdFMFpYsZEHJAia4Jxw84NmB+/B34nv9Htp3rKIjbqddV0mvlofM1kLbbD7c7k7y8rpw5QjM0kRSDKDpwwAIzYCV0VhME0kNVeNvKaarPsFQsJ10qgst7QUtsD+9pF4ylJtCX5hDYeUoxudPwTkgEG0NaAPtDER9dEfihNOZC5MtnaAwGCZ/MIYnFAUdtJYWsWnszEygHzWGhBHGR1o5O9rG7OQgoYSBUNRMKFRIeGAsWjLTndFu68Bt3UW/rYW17n52WQJEdJlGU0fMgTtSQn6shNK4GzOZ86VJ0GIxrMEBjGE/ulgUnb4QnaECnSMXS36CYnuYan0O9nA1oZiNPmMQv+zF1LSF/MY68oaGkEC4qgr90qUUXXQBKanRuXc3nXt3011fRzKead9wFZdQNmEy5RMmUz5xCq7ikmH4T3t3mpZiYOBV2jvux+dbixAmigqXU15+Hbm5M0Y8/SNRgV95X6Smker3kmhsILJ1a6bqZvv2/b1t9Pn5mdL8jJlYZ83EMnnkq20y+ZJ4O0N01mUDff0QiVimJO0utVM2Po/ycXmUjnNhsR9+QDZv1Mu6rnWZR/c6vNFMVdQY1xgWlS5iUekiZhXNwmo4fGNfIpGgs6ODlm0v0tmwh66IhQiZwGjQJXDk9OP2dJLvCWB3pJBahLSWCfJoesyhMizR0Ri0cQR91XS15zMw2E0y2Y6W7EBLdZLpVgkRi56BnBh44owrzmO8p5AirNh7ukj2dDEUiNIVMNIVyyVK5njNqRTuYBR3OIY7FMVg0misLmfDmDlsHjOdhopqcnRDTGU7M1M7qfEFkb5iYoPVxHxVJMNvzypmtXbjttbRZe9no2uQemsPAd0gCDBoRoqihRRFiiiMFWJN2kmmJLpkDHM8hCkaQpeIoYtG0Ond6AxV6E0l5OaaKbGaKJMudGkzfUY/A84oXn2IAX8Pxa0tVDe3UNjXhwCSNTVYTz8NOW0y3X3ddO7dTW9TY6b3jhAUVFZnAv3EyZRNmIwjz/1B/80+kHC4kY7OB+jufpx0OsT4cd+nvPza45IXFfiVg0hNI+X1kuzsJNnRmfl74KOra3+VDUJgHjcO68wZmYbYmTMxlpcfky5tUkoGu8N01g1lSvX7fMTDmR5AriIbZeNclI3Po2xcHracw1944uk4W3q37C/V1/nqgEw9/YLSBSwqXcTCkoUU2YsOm77PN0Bj41ZaWvbR1dnP0JCGJFOa9jBInq0fe3mK/AoDZkuMWKyDZDIzNIE5UY65vxpLoBpDeDR+fznNQxJvvAct1U461Y6W6kTIzDGFbGn0hT5GFwomWCVlsQC54QGMqRjdASetQ3l0RHLplU4SIlOitiSSeEIxXJEoBmMSf1EObRXV7KqczMbyWfTn5WOVYaYndzPF20N1v8Q26CYaKCYWe7uu22HoxWVsIWKJ0WxPssfZS7e5k0FzLyldCiEF7ribgkgR7kABroANSyyELeZHl4ghEgmEkOjNdpDFCFGE0OdjtxRQanLi0etImoYYcATxWeL0JYYIxyMgJSVeL+O7eyhoaECXSEBhIek5s+gvK6KtpwNveytIiU5voHj0WMomZgJ96biJWOwj0833g0qlwuze/SUGBlcxfdpf8XhOPeZ5UIH/Q0ZLJEh7vSR7e0l2db0zuHd1IRMH92vWezwYy8owlZdhLMs8TJWVWKZNG7E+9IdKxtP0tQbobc48upv8RAOZfDo9FsrH5+0P9I68w49RHklG2OndyY7+HWzu3cym3k3E03GMOiOzCmexsHQhC0sXMsE94bATi3u9zezZs4bGxma6u5MkEpl6YaNIUCZ7qKSLAkcY67RqkuWlhCINBIM7kTKNTtqx+qbg6J6E3TuFRNRNZzxNR6yfwXgjSa0Jkr3otMzwBDI3QnFxglEuSZkIURLqxpRKEA4YaRosojOcjzdlZsBoIJ1tH7HFE+TE4iSMcSJuB71Vo9hbNYXN5bMIm5wAuGNBRvf7KOlNMc6bJC9sIpbKgWz1k0PXT66xjbg5RJ9NT7c5RbepB6+pmz5rHxFjpvHXlrRSGHBT7LVT1SOwRJMILY0mBNJqxmRyIihGJ8szwV6Xi0mnI98gMOsjJOyDBJ0hfLoovsjb1VUej4dRJhPFtbVYN2xE5/ORNhnxFuXTbNYzaDOBEBgtVkrHTaB8wmTKJk6meMw4jKb3HpteSokWjqCFwwiTEZ3NhjCZjnn/+1QqxOYtVxGNdjBn9j9xOMYd0/RV4P8vIBMJUoODpLwDpLz9pAcGSPV7SQ1kX3sHSHkzr7VA4B3v17vd+wO6saw0G+TLM69LS9EdoR/ziB2PJvH1ROht8dOTDfSDnZmBzgByC6wU1+RSOs5F+fg8cvLfmT8pJW3BNrb3b2d733a292+nfqgeLTvu/BjXGBaUZEr1s4tmYzPaDnl/mqGhPdTVraOpqZmuriShUOYiZzJGKXP6GZPsZHRwDznmEENT5jFYnMdgbA/JpA+kwBiuwdY7mZyBqVj9NfiSgq54kJbkbvoTOzFH/eg1iVGkycsPU1qQotgaozTZjyseIB4w0DWYS2uomIGEFZ9OT8BihGyQsiVSCF2KnjwdHVWVNI2ZTWv+RMIGF0KTFATjTPT1Ue2NUjQgsQVtpNNvX6jtugGcxnbSxiD9VvCadSTTKYZMA/Rae+m19uIz+zLVNyk9JT4bpX1myvpN2KIWQsYc0mYnJl0eJlmCTV+MQ7hISh0amUuJQ5cA3QARUy9xV5ygSJHKDstgt9spLS0lz2HHEgpgWb8B85at2AZ8SKDfaaMzz0F0dDWemjEUVI3a/8ixO9ECAdKBAGl/gHTAn3ntzy4L+NH2Pw+g+f2Z58Hg/ntD9hMCYbWis1jQWSz7nwurBZ3Fis5qQVgOWeZ0YJs5E+v06f/xXA6xWBdvbroMnc7M3DmPHdNGXxX4TxLpUJj4vjpie/cSr9tHoqWF1ICXdL+XtN9/2PfoHA4MHg/6gnwMnnwM+fkY8j3o8/MxFBRgeiuw24/vzSbRYCJTkm8J0NPkp68lsL9+3mwzUFidQ9GoHIqyf62Od1bdRJIRdnl3ZQJ9/3Z29O/AF/cBmcHNpuZPZXrhdKYXTGdq/lRyzQcPz5xKBRka2kpb25s0NbXQ2ZXCP+RB0wzoRJpiR5AxZj8TUk0UBHYQcuoZKPbQX5hHRGQHKovnYuifTN7AFHIG/397bx42SXLXd35+eVZl1vHWex99z9E9h2Y0h0AgG4PAsta7CLOwBgl4kNeGXcleax8wrLGNsSRswbKAdzELlmGXfVYgxCGxWiTMYgnMIQ1Ic09Pz9nT93vXkVWVd2bsH1nv1cd0z0xf029+nieeiIzKyoyozPpG5C8jfnEvEtdZyUKOZy/xQvRXaP1VnFgxaftMjIdMtRJmDI/5cJXU0+muVznRn2EldumIQdexSUfeHvVcYQG9hsGze5scvfMB+hN3MzCnsBOY7mbc3m1zoDNgug1Wv4ZShSBppDSMRSyjTWIGdK2MdVORUjxdDMwBS5VlVipLrFXXSPUMyWGyazO/VqXRncAM5lH6FLpMY8sUNb1JSzOoI0QZxCO5sM2YJF9kqJ0jqPlkdlEGEWFyYoKJeo0KGarbpn/qFbRjLzC/3mXaG6Ir8Bs1ovvfQuWd72Ty4CEaCLKySnzyFPGpU8SnTpKcPEXW7V76htJ19HodrdlAbzTRGw30ZgOtMdpuNtBcFxUn5GGICgPyICQPA1QQFnlBQB7uzMsDv0gHAYwaL811cd7+dtx3fD21d7wDc9++1/T04HlP8ehj76Veu4sHHvh19Ne5otZrpRT+mwyV5yRnzxYC/9zzI7F/nuT06c19tEYD++BBjOlp9MmJQtAnJjGmJguhn5zCmJxAq5w/WPrGonJFvxPSGy0lWJhtenhrxagL0YSJBZfZg81C6A82GJt2LvB/szEzdkPkn1x9khc6L2z25g82D3L/1P2b4VDz0I6RN0opguAUvd5jrKw+xivHX2FxETqdOeK4aARbls/txhIH1VFa1TOENaHfcOm6LonlgyhUrqPat+Os3ctU+y1UB/uIleIV7QzPJo8y6D3HoWTAdGXApBsw7kZM5T3ynsZw3eSsN85S1KAjFh3HxretjQJioTGsVXhpYZKjd9zLyuwRAn2c8X7OdC9lTzdgf3fARE8w/a0nnor0cY11NCNgaMWsWwmeEaKNfpucnJ4+oK+tsm6fY7neYVgp3tm4vsVYZxy3t4cJ7y4m8gWaMoGDUZhXtCKkGWR5oQ+GpVDWGv30DGFlQFYp8nVNY6JRo4oCr83w9Ekir4uRZrT8kPk4Z2atixHFUK1i3n0X1QMHUb5PcqoQ+by/bclHEcy5Ocz9+7D27cfcs4DRau0Qc73RQGs20dzLT7h7LSilSHNFlOZESUaU5mSeR/3YEwRf+jLDP/9zkrNnATD37MF9xztw/9o7cL/2a9EbjcscHZZX/oBnnvlHzMx8K/fc/fPXxexUCv8NJPd9ohdeIHzuecLni5589Pzz5MPRSA8RrP37sY8coXL4TuzDR6gcOYwxN3fT+gTJc8WgE9JbCeitBnRX/CK94uOthWTp1hJ/tZY96sUXQj+1v45p7RwaqZRi2V/m6NpRjq4f5Zm1Zzi6fnTT141rukVvfiTy903dd0FvPstC+v1n6PUeY23tcU6fOUF7vUKnPU9/MAFoVLSQ22svMl89Tr3WJq7BoGaR6lvlTQZTqN4e7P5exoYHmOjehZ5V6egeL/IUKnqWyeBpZu0+U5UhDS0g6Zn46yZLvTGWwgZdZdJzbHpVm3xkm9cRQsfhzPwMz912hOPz92IlLlO9jOlewv7ekOleSs2zEFV8R8hwjS62HqL0lIEZsmYPCWWACERahGf06WsdfK3NwBzQt3361ZjRITBSg1Z3nunenRzoPsB8vIAGCBfeW7arYdgZuRaS5EP8rMdAb5OZQWEKUhkOCj0YEC8vIuEQAZqZYr/SmRiE2J0umrdNzHV9s+e8sb3x/sjatw9r/z7Mffuw9u/H3LPndY0OG0Qpy17Iihex0t+K1wYx4UjEozQjSvKtdJoTJTnhZn5GfhE5HHNMHtzX4sF9Y7y1EnPw5SfIv/wl/EceIfd90HWq991XNATv+Hqqb3kLcon5AidO/BIvH/9fOHjgH3Po0Idecz1fK7tS+P3HHiN66aU3dvI8R8UJKtkW4njn9qvkZe028alTbBiutVoN+/BhKocPYx85TOXIEezbb0e7Cd2+bvigL8TdpzsS+d6KT28t2FxdCkA3NZpTVcamHZpTVZrTVZrTDq0ZB3fswsfa9WCdo+tHObp2lGfWn+Ho2tFNPze66NzRuoN7Ju7hnsl7uH/qfm5r3nbBOPowWqLXe4xO51HOnj3G0lIXrzdO35vEDxqYVkjNbTPvnma8toTlDogd2NS7zCLx9hB19mD291Lr72Mq2EtdFdciI6NjnCHLn6aZ/gUzxvO4ekzUNQnWTVa74ywFNdoY9Ko2vW0mGwGSqsPSzCzPHzjCuYnDVLI6072c+V7AfC9izNPR0606VfU2FW2IphskhmJoDlg314m0iIE5KERd7zHUOvTNAf2KT2RuCaqWa9SCBq1gilawQCtaoOXPMjlcQENHAZoOViXHskKU1iFMu/hpn5AhsalQ9ta7BfIMPQ7RgyGm16XW6+IGIRN2lYkM3P4Qoz8A39+8vzcxDMzpaaxDh7AOHtwUeGv/fsz5+Su2l3thwsqmoEeFuG+LV0dpP84u+G7F1Jis2TiWjm3o2IaGbWpbaWOUNrelz9snzRVPn+nx6KkOL60UQ5kNTbhnvsGDe5vcm/c4/MoTuI/8GeHTTxcjjxoN3Le/fdQQvANrz8JmmZRSHDv2oywufZp77v55Zmffc0W/w+tlVwr/0kc+Quc3Pnn1C2SaaKZZ3LxWEWumhVgmmDu3tXoD+847CoE/fOSaOR27WsRhypljHU4+s8bJo22G3WjzM8PUCkGfckbxSOinq7hN+5KuintRj6PrR3l2/dnNnvyGjxtBONQ8xD2T92wK/eHW4Z1uioE8TxgMjtHrPcbS8uOcPn2KTlvD86YYDsewbR/X7dCsr9NylrFdD6ytXryWjpN5+xmuLZC099AczDEZt2jpFQwpepi5DBHtRRx5hpr2NEb2ErGXE7Qt2t0my36N9dzcFPnI3OrVpY7N4swcL87fQ1Tdg6UmmPZyFnoBk57Cirb2NbU+VVZBU6SWRW5l9I02PX1AoqV07A5ds0PHXKNrdxlYIUrb+p9WI5tmMM5YMEMr3EMjnKceTDMWjWOwzdQlCaL6JMlpEjlDavTIKhVyu0puV2FjFrVSmFGI221T63Zx+x614ZB6llPPFHYQoQfBRa+tWBbG7Az27XdQffABnIcextq3F318/Irv8zDJOLE+5JXVIcfXhryyLbSHF3rUdCydmUaFqbrNdN1mplHZEU83bKYbFeq2cVX/a51hzOOnO3z1RIdHT3Z48kyXMCnusYWxKg/OudybrHPnK08w96UvoBbPAdB633uZ/uEf3nzHlucxjz/x/fR6T/Dgg59grPnQVSvj+exK4c88r3hB80YQQbMsZEPoTfOmFu7XilLFyJqTz6xz8pl1Fl/qkmcKq6Kz9+5x9hwZpzXr0JxycMcuPxyuH/c5tn5sU+iPrh/d9G8DsK++b0vkJ+7h7om7Lxhpk6YDfP8VfP8Vet5znD51jMXFDp7XxPfrmGaMW+tQczs0622siodsCGOuoQUzZIND5J0Wekeo9ccZy8eoai005oFC6HVZxJZjmOpZ8uHLRF2PaGDiDTTakUk3t+g7FbpVm8De6qEmFZtz03tZnridzJ7DzieZ76fM9DKqwZbAaxJiG8ukDBiaJpgChk9gDIklJSOjZ/UYmF3a5hpr9jpe1UeNfmI3dBgbTFMJ5zHjPTjRHC1/mrHUoZErDMnItCEZbZT0yGRIroUoPUFsA7FMUtHIECTPqQYBtb5Hrduh5nnUBgOcIKQWJ1hRvNPwo2nFKC+RwiQ50glzzwLVBx/C/fqvw3n4bVfckclyxZmOXwj76k5xP9cLdjwwzDRsDk66HJyscWDCYbZZYbpeYbpRiHvNvjl84SRZzrPnPB49WTQEXz3ZZtkrOkqOpXPfpM2968f5W5/+RVqzk8x/7N/gPFxocJJ0+MpXv4M07fO2hz9Ntbr3mpRxVwp/ycVJ4oyzz3c2xb6/Xrx0HZ932X/vBPvvnWD2tia6/up+dQbxgGPtY5sC/+z6s5z0Tm5+Pu/Oc/fE3dwzeQ/3Tt7LXeN3bdrl8zwlDM+MBP74SORfYWWlTberCIIGaWIDCsftUXM71GpdTGurIc+CGnF3L0F3L3a/RmtoMxHbNETHkhoZc2RqGtCADI3TEJ1B9c8Sd44TdVfpDxP6ysCr2nhVC6+6sycfWTZr43vpje0HcxY3G2fWExpDfZuNPEOnT6LHBEZYuGIwhuRmSGwU7iMUiqExILD6tM01lu011qtd8lGDZSQOlr8HM9hH1V+gFszQzGwaEtOSIZY2QElIriXkugJ9S2wlz6mEIdUgoBoEOL5PddCn6vtUgwDXD3DCCO38/7ptI6YBaYYKwwuurzE7i7VnD9UH3lo413vggYt6T/XjlLV+zOqgML2sbYuXvYgT60NOrg9Jsq3z1ysGh6ZqHJp0RyJfhAOT7k0j7K8VpRRnuwGPnuzw2MkOXz3Z4diixx5H58f/8tfY8/JTjL///Ux96B+jVSoMh8f56qPfgW3P8PBDv41h1K96mUrh3+X0Vrd69Wef75KlOYalsefI+KbY18cvPTrIT3yOtY9xdO0oz7af5ejaUU56J1GFs19m3dnNHvxG3Kq0iOP2prD7/nGG/nF8/wS93iKBXyWOq6SpjQCGFVCpDKhUhuj61hjsPNOJvDni7j7Em6U5dBkP6jQzB0scUNMotk0wUxGkHVTUIxuskbRfIFx6mmESjwTexnMKkc9GE7gU4LtjePW9ZNYEBjVqaYOxaAIjN0f7KHyri2ev07dWGVRWGVTW8Cs9kMKdsCAYSqeiLKq5hUJxzl5hqbJCulGnzMYK5qiHM4yF40zHTSZywdJidnSelcJMEmq+Ty0McYOAauBTGQ6xBgNs36caxthJcsFrWgXF06muFc5yzpush6YVo2fm57fmdmym5zGmp+mmwsn1Iav9iNVBxFo/3iHqRV7E8CL2dYBx12K6brN/wuHQVI2Dk+6m0I+7138y1Y3gKyfafPDXH2MQJvwYL/K23/4lrEOHmP+pj1G97z7anS/zxBPvp9X6Ou6/71euumfPUvhvQbI0J4ky4jAlCTOSKCMJM+Io3Uz3VgJOHl3fXBh8bMZh/z2F0M/fMYZuXtirXw/WeaHzAi90XuD59vMcXT/KK71XNkV+2pneFPgNkZ+oTpCmfTzvKTzvaXreE3jeE0TRKknkEEcuCg3dSLCsAYa5U4jSxCL2WyT+BMqfQqJJrKSGGzVp+eM00hpW3oJtNmyyDoSr5H6b1Fsm7ZwhWz9NlHh4FZN+1cJzLbyazVC3Np2m5ZpO4EyTWXMYMoWbTWIxgUjxp8skxaus4dnrREYfr7JCz1mhV1kl0SOUKBRFT15gZJpRZJKTkZNKTiYZueSIEtx4nFbYYjpuMh6PUU/qSK6hZQorjGhGA5qhTz0McIZDKl4fq9+nMvAx8pzzyUwDVamgmSaariNZDr5/Qa/dmJ/D2rsPc8+WsG/M6TBmZhDDIIhH9vW1IcdXB5s29uOrQ3pBcsG5W47JZM1mqm4zWbO3pa3NvKm6zbhrYV7miXG3sOKF/MPfeIyvnOjwfQdt3vebH4OVZSZ+4B8w9cEPsrj2exx77sdYWPgeDt/54avaIJbCf5OjlCLyU/rrId56gLcW0l8PGfYikjAdCXy2KfBxlO4YVXMpdENj4fAY+++dYN89E4xNb9nToyziePc4L3Re4MXOi5tivzG6BmCyOrlD4O+euJspZ4o8j+gPnsPznqLXe4Ju9zEG3TZxVEO0DKvaxzC3hCjLdKLQJQqaxP44KppERbNUoika4QRjcYWxzKKa15Dt4q5SiJZQ/gqpt0LaXiT3FskHy2RZWJhnJky8hkXfthkomzTf+n6mV8mtaUxmMLVpRJ9CtDGUBkO7j2evsF49Q6+6imevoUlGLbOZiiZphWNUMp1abtOiwZS0aCqHpnLIc8WaBLT1Lm3NY03rE0oCCrTcQsssJAErCnH6bZqxx1jmU08CKsMhZn+A2R9gxDvFNdWEpGKD66K5Lmalgq5AC0Lydhs1cpIHbI55tw7sHw2HPFCMnNm3D3PvXjS7GE2V5YqznYDjawOOb7OvH18dcK63s7GYa1Y2zS6Hpgob+3S9sinm1hUsKl9yIUmW868/d4xf+9IJ3ra3wb88/Z8wfu+3sA8fZv6nf4rTxu9x6tR/4I47/gX79v69q3beUvhvAuIwHQn6lrB76wHeekh/LdicxbqBVdFxx2ysqoFp61gVA7Oij9I6pl3km5XRZ5vprc9sx0DThWV/eVPYX2gX8QnvBJkqzmlpFre3bufO1p3cMXYHd44X8UR1AqVyfP84nvcU3c6TrC4+S7fdJcsE3R5iO+3NHnyea/jDMQaDCYL+HNFwD0Ywy3Q4zt7cZTyvYnGeSSnto4JV8uEqqbeOGq6RD1fJ/TVU0CGzFP0ZHW+86MX39QpeUiUNtwQ+FwNlTmDIFLo2iehF6NUd/EpKrHcYmqdoV17iXO1FetV1FDmtqMX8cJr9wznu8PcxzQSTtGjiUFMVdDT6BKxpfdY0jzXp01ERSa5jKBtD6ZiDPnb7NG5/mdpgnSYJbpJQGQaYwU5hzQQiwyCzTKRSwbBtDN3AyHIYDOB8VxuaVvTU9+0rJjXt34+1b38h9nv2IKZJx09Y6oUseQFLvYglL2SpF7DkRZzrBpxa94mzrSeH8+3rh6a2bOyO9ea0r79Z+H+eOMv/9LtP0aiY/OxdMPNzHybr9Zj84AdYfPsTrHa+yP33fZzJyW+6Kucrhf8qk6U5kZ8SBymRnxIFydZ2kBL7RRx4Md5I4De8Sm5gWBqNySr1iQqNiVE8uZW2nSsfjpZkCUv+EouDRc4Nz7E4XGRxsMhJ7yQvdl+kH29NqJl35wuBbxUCf2frTvbV92FoBlkWMBy+SGf9BdbOnWJ95WX6/VWUFmA6Xaq1VXSj6KXmucZw0GI4mCLqL4C3j4n+fvan44xTRxuZVlSeQliIedZfJ/dXC3H3C3FPHR9/XMdvGQSuQWAbDA2bAVWC0Cb3QBu9GFRAbrbQZRJjm8APnTqeEzK0PYb2Mr51jsA4i292WK2skuhFmSfCMQ77B3nQv5u3De9mKh9DEEIV42kBngT0ieirjGGeEwjoplCJelTbp7DWTmH2VnFCHyeKceMUO93ZYCeaRmJoZLqBmAaGpmNkOXoQIOf5j5FqtbCtz82NbO6F3d2Ym4OZOTrVBstBdoGgL/dCFr2AZS8iTneag0RgqmYz26ww26hwcKqwrW/Y2Sd2iX39ZuXYosd//4lHOdsJ+GfvPMC7Pv8r9D//B9j33kP7ewcMWud46KHfol478obPtSuF/7lHFjn7XOcNnTvL1EjQk5HAF6KeJhfaXrcjmmBXDSo1k8ZEhfpktYhHIt+YrFCpXfnQUD/xWRwucm5wbjM+Nzy3KfSr/uqmDX6DqeoUC7UF7mwV4n7n+J3cPnY7datOmsasLb3M6pkTtJeW6a706bYjEuli1Rdxx09Ra53Z7Mlnmc5gME7Yn0YN9tDoHWRqcBsTWZMqFkplhb29v0zmrZIPlskHyyTJCkG1hz+m4ddM+hWHgVFjiE2U6WRRjhZd+HIyFwOl19Cljq5NIfokmj5JWKnSdXp0qou0nTOsO6dZd86RGBHnY+YGE+kYb/Hv4K3DwxwZHMBIdTwtxJOcoUoZpDFhHmHnfRphB9tbxfRWqfbaVIc+lSTBSnP08/4nG1uvdvXENNFbLcy5OYz5Ocy5+U1xN+bm8JqTLOUmi72iZ77YCzjXC4t0N2SlH14wk9Q2NOaaFWYalU1h34hnRvFU3S7t6zc5vSDhhz71BF94boVvf2CBf+qeo/eTHyX3hwy+zSB8V5W3fc1nsO3pN3SeXSn8f/nZ4zz/yNIbOremC7ZjYFUNbMfArhpYjok92ra2x9vSpq2/pl5VlEWcHZzlTP8MZwdnOds/W8SDsywOF+lG3R37G2Iw484wX5tnzp1jvjbPvDvPXG2OeXeemeoMeSQMOhFr586wvrhId6VLbzWi7ymCSCczhlTGj+NOHqc+fppGc2VzNE3gN4m6C+jdQ9S92xjvH6Sl6uhJFzVcJuutkHkrhbj7Kwy0Dn7LoFd36VSb9HSHJM3Q4guHCeaajdLr6FJDpwZaDdHqiFZDtBqxaTKwBwwqXQZWh46zRNtZpG+tofSUSlrBzSqMpS4TWYvZbILptMV42qSWVtEShUqEKM/pS4Yf9lHDZarhOk7QpTLsYfp97DDCiVMqaYr2Kn8DpWlg2+jNBsbEBNbsHMb4OPpYE73ZLPzGNBqk9TEit05YcQlthwAdL0pZ7IYs9gLOjgT9XC9gsRde0FO3DY2FsSpzYxXmm1XmxqrMNwtBnxuJerN6a80j2c3kueIXvvgS//YLL3BktsEv/pcHMX/+pxh84QvEt0HygUM88F/8Drr++r3m7krhv5nI8oxlf3lT3M8Mdgr8arC6Y39btwsxr82z4C4wV5tj1plj1pqjmU1QjWuE/Qzfixl0+/Q7XYZdH78XEQ4g8g0yLSHTfbJKl9zyyK0+1foS9eYyzbFl6vU1dL0wU6SDSfTubTjtw4yv3YbTS8gHS2SdVXJvJO5xh6GjGDQseo6DZ7kEGJDtHKGT6S6ij2EyhmhjiFYHrQaaS2K5JJYQGX18c42Btcx6dZF2dYWh3cU3PdzUYTKcYCaYZCGcYiGZYF++wLQao6kcdCUE4TqDYBk/WCeO2mRRBxW1IehgRiFWkmClGXaSYZ43MkYBuYAo2N4vTipV+gfuoH/X/Qxvu5t2a4ZOpYGfFWPVh3GGH43iOMWPMobb4ov5edlAE5htVAoxHwn6/FiVuVE8P1al5bwGUVcK0hCSANII0gCScFu8PUTFNUpjyKKt7SvJyzdMUzJy5bARA6Kdl3exWAPDAqMC+ig27CLoo9ioXHofowKmA5YDpguWW2xf47Wbryd//PwKH/rk4wD82+96Kw+98AiLH/0wWeyjvu8wd//Q76Lpr+/dSyn8V0iWZwySAV7s0Y/7DJMhcRYXIS/iJE+Ismgzff7n29Ne7HG2f5al4RKp2rLvaqIx68yyUF9gobbAvLvAnOxhIp3FjcZgaNJb9ems9Bl2fcJBQhIIKlfo1hDd7iO2h7IGZMaQVIvIEZJMJ8504kzQ9JR6Y5Vmc5lWc5VafRXRcpQS9P48zupe3NPjGC/bJN0+QbjKMG7jayFxVSOwTYaWzVC3yc67T3LdRbQxDBmJu95CtDFiq05iK5Q+RBiQ6wF9u0O7usSyc5Z2ZZ2u3UNJcbxmXONAuMCd/j7u8mY50mlR8yOI+qRhlyRsk4Yd8qiHivposY+RJFzsb6+ATNfINI1cKHzFK4ohkUpYrzRZrzZZrY3Tnt1PZ2Yv3eY0a4bLcqqxOkw2vVFuoGuCa+m4toGzPbYMHNvAtXQcy8C1i9gxNRwjw5UERyLqWsRcJWHaDDGykRAnfiHOiX+J7WArvJqgv1E0cyTA1la8PW3YsH1cuVLFr7w9VvmFeTvifGdDshGy7Y3K68CoFo2B5Y4aBGfUQNTOS7tQHYNqC6rjo3hbMK79cqFXwql1n//uE4/y3JLHh775Dj5wT50T/+TvkT96Evdfvo997/vx13XcXSn8G2aTftynH/c3xXx78GKPfrK1PUyGr6uMhmZgaRa2bmPqJpZmYekWNbPGQm2BhcpeZvIFxuIpnGGFeN2n127jD3ukaYCoEN0M0Cwf3fLRTR9lDsm1FJXZqMQmTSrEcQU/thmmGtGOS6eo2yHT9S6NxhrV5iJ6baVQvlww1ifh3Bjh6SrdMxq9JCQyhETYdBGw42hioHQXkdoOcUdrkth1UitHaUOUNiA0ewzNNYbWOrE5IDIiAiNiaPq0LY/AKJ4IqonBHZ0md6xVuX1R49DZhGbHR+I+kl7okwUgFyHXZNPDJSjIFSk6Q7NK33LxNoLt0rNc+pZDz3Lxqg261QbtapOudqGjONfSmR2ZUWbqFnOOMOtkzFZSZu2IOXNIiyFa7EHUh3hQxNEAoovl9SHuj8TwNaDbYFaLsNHDNUexUSnSRrUQ4s19RvH2zy+230ZveqN3vV3Yb7TJKM9GDUE4etIIi0Zix3YIsV80ivHgEulhES5I+5e/HlZt1AiMXbxxcCdh+i6Yvrv43a4hQZzxzz/zNJ9+/CzvPDLNz/439zH8g19g/u98CM14fYvA7Erh/zdf+nE+9/Kn0YSRG1rQRaiZDnXTpWY5uIZDTVxqhosjVRytSlWrUhEbGxMTAy0vei5aphCVQ54XvZU8hzxBpSF5FpEmIWmWkOUxWZ6hSFEk6MZI1M0A3RqimQGaKIyoCf4k8WCKyB8jCmsEcRU/0RnmMFQpqWy7aRW42NTzCm4ONWeNSuMcxtgZZHwRqRbuDPJEx192GCzbDJeqDJer5Mlo6KM4iOYi4iKaC5qzmU4Nh8yskhkVlA5ITC5DUr1NYKwztFboW8v41pCBGeDZIZ4dX9BwOCFM9BUTnmK6B7edU9xxTjG/vjHhSXa8HE1Ex7NH4m06eLY7EvTCnNS1XTy7hlep4VkufbNKqF26p1Y3csbtnJaVMm7EzJo+c7rHjNZjTtrM5qvM5ovU0/WRaPevsPcpYDfAroFdL4K1kb5YfqPocVrOSJgvIu5G9ZYyW9x0KFVc36ADQXsUj4Lf2bl9/j7b7wnNgKm7YO5+mLuviGfuLa75VS2u4hOPnOQjv/8s82NVfvl7H+Kuucv7+r8Uu1L4//Az34fR/NLVKYQClI7kJlpuILmB5OaOWKU2ktmQWajMRjKLPHGI/HHi0CFOTMJcI8gVQxL6EhDJeUM8lYaTm1RSDTPN0NMUiUNEa2M2zmJMrlGZ7uNMhWhGce0ir4K/Ms5wZYpgdRZ/MEVi1Ahsl8hyiC2H1KqSmopMG5LjAT1yOqRal0TrEek9Ym1ILCGBEeNbKaF14b1RiRQT/ZGw9zXGB8LYwKTmmzhDA2doQGYRGhaBYeMbNn2r6Il3bZfeKPQtl4Hp0DcdEv3SvZmaRIzrAS0ZMC59WqpHS3UZp0eLPi0pwvgoPcYQU85zIWDVtgl0fSTQjYvn7RDzGlh1qDS2zAY3updccn1Qqniq6C/D8tOw+BQsPlkEf220k8DE7Tsbg9n7wBl/w6d/9GSHD/76o/SChI9/38N8w51Tr+s4u1L4P/3hX+VAOgFKGznUktELvcKnilKKnGL6fUZOPoozcnJRRVrl5AIZkJKRSkZKPkoXcbYtne5I56TniZCmhEqmY2ZgJAkSB6h4SBr1QOtgVcByTWwXLDfFbg6oTq9j1guXC3ku+P0ma36Ls+E4x6MWy2IR6T6pFI68RCUolQAJmSQkWkSkx5t29fMxUnBCk0psYaY2ZlJFS6uorAqJi0R1VOySxg1iaoRGhVDfEvfsPD/5F0OUwlEBDYaMMaAlfSbFY0rvMb4h3EbEmJEwbuWM2YoxGyy7Ouo1b4Taa0ubTuGEvqTkaqAU9BdHjcC2xsA7s7VPc9/OhmD8EIztK57wXgMr/ZCP/v4xfuJb72ay9vrMTLtS+H/1n/3PhJpOKlkh4iNfKtlI1PNLCOGrIQo0paEpQZSgqSJPRuYgyTNQGWQp5CmihuhaB8scYFkBph1juWC4GaaboDsRhhNjWikXc2ffy+BEpHMi1ngl0jgTa6TbRo9ruY6uzCLODbRMR8s0JNORXEdyGzKXLHfJshpxViNUDULVRKUuKnNBXdxsUkkjqmmElcdYeYKZJ5gkWMRUtCLUjISanlG3choWNKo6Y45Jq27Tataouw4t16bpOui2U5g6NgR5c6SGUwp0yZub4TosndcYtF/euU99Dsb2Q+sAtEbx2P4iXZ+/Jia/Swn/LT0/e623ROxaI7u8QlSGrsXYeoyuxxh6jK4n6EaCbsQYeopmJOhmgm6k6GaGrqeIliOaQkQV8XkBjYvkj/IuIuaZgkEGw0zDTwXfNxj2XHqxSztqsRpN0gnHaYfjhFmNXJmgTFQ+ipUBuQnKgIuObwE7S6ikEZU0xk0CGmlINQ2p5hGV3KPKGg4xVT2lamTU7JxaVaNWM2g2LcZck6rjYDs1nNYEzvgC1fFpzPpkYfqwG6VQl5Rs4E7Abe8swgahByvPQucEdE5C92SRPvHn8NSnYPuES92C5t6tRmF7AzF5Z9FZuorcEOEXkXcD/yuFq8VfUUr91LU4z9Rdf8nMwjq6laOZObpxZT38LBXyRMhSDZUBCoTRi11gY7iaKAVZDikbg02K4dVKJ1UaiTJIMpNBVKMbtlgO5zjtH+B0cDspY9hAFcFWUBGhgmADcwgHlcJSOUaeoaUhkgwh7mAkHRwZ0NAiKpZG1dJxKibVio3r2Dj1Gm69gdVsYjansVrj2PPzVMZa6JdYB7SkpOQaUGnAvrcX4XzSCHpnioage7JoGDbS5x4vXjJv8N5PweF3X9WiXXclEBEd+EXgbwJngK+IyGeVUs9e7XMdMlNqcYzmC5JKYQZJNbRURxKjMImk+mYsmY6W6aA0FBqgoTBIcciUQ0oRMhwy5Y5ihxyHNHfIqKKUiYWgIegCJoIjGhWEChs/uEJpAaqSQl1HxmtoM1OYkw2siSr2pIPpmBialLM0S0puRQwbJm4rwsUIva0GYc8Flpo3fvqrfsTL8zXAS0qp4wAi8pvAtwFXXfhbp36SiVcu/UJlw7eNvKrHFXj11yoZQoCmhYjmIVqCpieInqEZGWKB0XLQp8fR9+xF33sQvVW75Pq0JSUlJVQaMPuWIlwDboTwLwCnt22fAb72WpzowNsaRK8MEH00u1wf2d91QFej/JGNXoCRbb6YaT7K1wWtYiFVG6naaNUq4lQR1ykWT7YdRC9t3SUlJW8eblqjr4j8IPCDAPv27Xtdx2j+nWvSnpSUlJS8qbkRUwbPAtuXlN8zytuBUurjSqmHlVIPT029vskLJSUlJSUXciOE/yvAHSJyUEQs4LuBz96AcpSUlJTsSq67qUcplYrIPwL+kGI45/+hlDp6vctRUlJSslu5ITZ+pdTngc/fiHOXlJSU7HZKt4AlJSUlu4xS+EtKSkp2GaXwl5SUlOwySuEvKSkp2WW8Kdwyi8gqcPJ1fn0SWLvsXrcuZf3L+pf1373sV0pdMBHqTSH8bwQR+erF/FHvFsr6l/Uv6797638pSlNPSUlJyS6jFP6SkpKSXcZuEP6P3+gC3GDK+u9uyvqXXMAtb+MvKSkpKdnJbujxl5SUlJRsoxT+kpKSkl3GLSP8IvJuEXleRF4SkX96kc9tEfnU6PO/FJEDN6CY14wrqP8PicizIvKUiHxBRPbfiHJeKy5X/237fYeIKBG5pYb4XUn9ReTvju6BoyLyG9e7jNeSK7j/94nIH4vI46P/wN++EeW8aVBKvekDhXvnl4FDgAU8Cdx93j4fBH55lP5u4FM3utzXuf7fBDij9Ad2W/1H+9WBPwUeAR6+0eW+ztf/DuBxoDXanr7R5b7O9f848IFR+m7gxI0u940Mt0qPf3MBd6VUDGws4L6dbwP+r1H6d4BvFpFbZcXzy9ZfKfXHSil/tPkIxcpntwpXcv0BPgr8NBBez8JdB66k/j8A/KJSqgOglFq5zmW8llxJ/RXQGKWbwLnrWL6bjltF+C+2gPvCpfZRSqVAD5i4LqW79lxJ/bfz94E/uKYlur5ctv4i8iCwVyn1uetZsOvElVz/O4E7ReQvROQREXn3dSvdtedK6v+vgO8VkTMUa4H8D9enaDcnN+1i6yXXBhH5XuBh4G/c6LJcL0REA34OeP8NLsqNxKAw93wjxdPen4rIW5RS3RtZqOvIe4FfU0r9rIh8HfB/i8i9Sqn8RhfsRnCr9PivZAH3zX1ExKB43Fu/LqW79lzRAvYi8i3APwfeo5SKrlPZrgeXq38duBf4ExE5Abwd+Owt9IL3Sq7/GeCzSqlEKfUK8AJFQ3ArcCX1//vAbwEopb4MVCgcuO1KbhXhv5IF3D8LfP8o/Z3AF9XoTc8twGXrLyIPAP+eQvRvJfsuXKb+SqmeUmpSKXVAKXWA4h3He5RSX70xxb3qXMn9/3sUvX1EZJLC9HP8OpbxWnIl9T8FfDOAiNxFIfyr17WUNxG3hPCPbPYbC7gfA35LKXVURD4iIu8Z7farwISIvAT8EHDJIX9vNq6w/j8D1IDfFpEnROT8P8ablius/y3LFdb/D4F1EXkW+GPgR5RSt8QT7xXW/4eBHxCRJ4FPAu+/hTp+r5nSZUNJSUnJLuOW6PGXlJSUlFw5pfCXlJSU7DJK4S8pKSnZZZTCX1JSUrLLKIW/pKSkZJdRCn/JTYOIzIrIb4rIyyLyqIh8XkTufJ3H+usjL5RPiMiCiPzOJfb7k+s9kUtEvl9EPnle3qSIrIqIfYnvvF9E/t31KWHJrU4p/CU3BSOHeZ8B/kQpdZtS6iHgx4CZ13nI7wE+ppR6q1LqrFLqO69WWa8CnwH+pog42/K+E/h/b7EZ1SU3KaXwl9wsfBOQKKV+eSNDKfWkUurPpOBnROQZEXlaRL4LQES+cdRj/x0ReU5Efn207z8A/i7w0VHeARF5ZvSd6uip4piIfAaobpxPRN4lIl8WkcdE5LdFpDbKPyEiHx7lPy0iR0b5NRH5P0d5T4nId7zacbbVywP+M/Ct27K/G/ikiHyrFOtFPC4i/0lELmj4ROTXROQ7t20PtqV/RES+MirPh0d5roh8TkSeHP2G3/X6LlHJrUIp/CU3C/cCj17is/8aeCtwP/AtwM+IyNzosweA/5HCx/oh4B1KqV+hmLL/I0qp7znvWB8AfKXUXcBPAA/BphuDfwF8i1LqQeCrFDO8N1gb5f8S8E9GeT8O9JRSb1FK3Qd88QqOs8EnKcQeEZmncKHwReDPgbcrpR6gcC/8o5f4TS5ARN5F4X/na0a/10Mi8g3Au4FzSqn7lVL3Av/xSo9ZcmtSeucseTPw14BPKqUyYFlE/jPwNsAD/kopdQZARJ4ADlCI56X4BuB/A1BKPSUiT43y307RePxFYXXCAr687XufHsWPUjREUDRC372xg1KqIyL/1WWOs8HngP9dRBoUTye/q5TKRGQP8KlRw2YBr7xKXc7nXaPw+Gi7RtEQ/BnwsyLy08DvK6X+7DUcs+QWpBT+kpuFoxR27tfKdpt4xuu/pwX4I6XUey9znsud43LHAUApFYjIfwS+naLx2Hgq+AXg55RSnxWRb6TwI38+KaOndSlcTlvbzv0xpdS/v6BQxXoEfxv4SRH5glLqI69WvpJbm9LUU3Kz8EXAFpEf3MgQkftE5K9T9Fi/S0R0EZmi6LX/1es8z58C7xsd/17gvlH+I8A7ROT20WfuFYwo+iPgH24rb+s1HueTFII/w9ZTQZMtl8Lff7EvAScYmaiA9wDmKP2HwH+77d3EgohMj0xJvlLqExTO+h68TL1KbnFK4S+5KRh5Svx24FtGwzmPAh8DlihGwTxFsZbqF4EfVUotvc5T/RJQE5FjwEcYvVdQSq1SLNTyyZH558vAkcsc6yeB1uiF6ZPAN73G4/wRME+x/vGGt8R/ReFB9VFg7RLf+w/A3xid8+uA4agO/x/wG8CXReRpiiVG68BbgL8amcJ+YlTukl1M6Z2zpKSkZJdR9vhLSkpKdhml8JeUlJTsMkrhLykpKdlllMJfUlJSsssohb+kpKRkl1EKf0lJSckuoxT+kpKSkl3G/w/OA/EQEo++qAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(no_of_decisions)):\n",
    "    plt.plot([x * 0.01 for x in range(0, 100, 5)], no_of_decisions[i])\n",
    "    plt.xlabel('Confidence Values') \n",
    "    plt.ylabel('No. of Predictions') \n",
    "    plt.title('No. of Predictions VS Confidence Values')\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(mean_profit)):\n",
    "    plt.plot([x * 0.01 for x in range(0, 100, 5)], mean_profit[i])\n",
    "    plt.xlabel('Confidence Values') \n",
    "    plt.ylabel('Mean Profit') \n",
    "    plt.title('Mean Profit VS Confidence Values')\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(min_rofit)):\n",
    "    plt.plot([x * 0.01 for x in range(0, 100, 5)], min_rofit[i])\n",
    "    plt.xlabel('Confidence Values') \n",
    "    plt.ylabel('Minimum Profit') \n",
    "    plt.title('Minimum Profit VS Confidence Values')\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(max_profit)):\n",
    "    plt.plot([x * 0.01 for x in range(0, 100, 5)], max_profit[i])\n",
    "    plt.xlabel('Confidence Values') \n",
    "    plt.ylabel('Maximum Profit') \n",
    "    plt.title('Maximum Profit VS Confidence Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9359793 ADANIGREEN.NS\n",
      "0.8540027 AMBUJACEM.NS\n",
      "0.954243 BAJAJHLDNG.NS\n",
      "error with  CADILAHC.NS\n",
      "0.89655924 HAVELLS.NS\n",
      "error with  MOTHERSUMI.NS\n",
      "0.8532959 SIEMENS.NS\n",
      "0.96391875 YESBANK.NS\n"
     ]
    }
   ],
   "source": [
    "chosen_stocks = []\n",
    "todays_date = datetime.datetime(2022,4,5)\n",
    "# todays_date = datetime.date.today()\n",
    "\n",
    "nifty_tickers = get_tickers(2)\n",
    "i = 0\n",
    "\n",
    "for t in nifty_tickers:\n",
    "    try:\n",
    "        prediction_prob = predict_given_date_ticker(all_model, todays_date, t)  \n",
    "        if(prediction_prob > 0.85): # Threshold\n",
    "            print(prediction_prob, t)\n",
    "            chosen_stocks.append(t)\n",
    "    except Exception as e:\n",
    "        print(\"error with \", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest with Realistic Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC.NS\n",
      "ABBOTINDIA.NS\n",
      "ADANIENT.NS\n",
      "ADANIGREEN.NS\n",
      "ADANITRANS.NS\n",
      "ALKEM.NS\n",
      "AMBUJACEM.NS\n",
      "APOLLOHOSP.NS\n",
      "AUROPHARMA.NS\n",
      "DMART.NS\n",
      "BAJAJHLDNG.NS\n",
      "BANDHANBNK.NS\n",
      "BERGEPAINT.NS\n",
      "Error with  BIOCON.NS\n",
      "BOSCHLTD.NS\n",
      "Error with  CADILAHC.NS\n",
      "COLPAL.NS\n",
      "DLF.NS\n",
      "DABUR.NS\n",
      "GAIL.NS\n",
      "GODREJCP.NS\n",
      "HDFCAMC.NS\n",
      "HAVELLS.NS\n",
      "HINDPETRO.NS\n",
      "ICICIGI.NS\n",
      "ICICIPRULI.NS\n",
      "IGL.NS\n",
      "INDUSTOWER.NS\n",
      "NAUKRI.NS\n",
      "INDIGO.NS\n",
      "JUBLFOOD.NS\n",
      "LTI.NS\n",
      "LUPIN.NS\n",
      "MRF.NS\n",
      "MARICO.NS\n",
      "Error with  MOTHERSUMI.NS\n",
      "MUTHOOTFIN.NS\n",
      "NMDC.NS\n",
      "PETRONET.NS\n",
      "PIDILITIND.NS\n",
      "PEL.NS\n",
      "PGHH.NS\n",
      "PNB.NS\n",
      "SBICARD.NS\n",
      "SIEMENS.NS\n",
      "TORNTPHARM.NS\n",
      "UBL.NS\n",
      "MCDOWELL-N.NS\n",
      "VEDL.NS\n",
      "YESBANK.NS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>RSI</th>\n",
       "      <th>STOCH14K</th>\n",
       "      <th>STOCH14D</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Prediction Probability</th>\n",
       "      <th>Buy Signal</th>\n",
       "      <th>Buy Price</th>\n",
       "      <th>Sell Price</th>\n",
       "      <th>Profit</th>\n",
       "      <th>Profit Percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-15</th>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1766.650024</td>\n",
       "      <td>1785.000000</td>\n",
       "      <td>1788.050049</td>\n",
       "      <td>911884.0</td>\n",
       "      <td>1726.754883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACC.NS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-16</th>\n",
       "      <td>1843.550049</td>\n",
       "      <td>1784.949951</td>\n",
       "      <td>1791.000000</td>\n",
       "      <td>1819.300049</td>\n",
       "      <td>1937730.0</td>\n",
       "      <td>1756.933594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACC.NS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-17</th>\n",
       "      <td>1837.000000</td>\n",
       "      <td>1807.000000</td>\n",
       "      <td>1820.000000</td>\n",
       "      <td>1819.750000</td>\n",
       "      <td>860949.0</td>\n",
       "      <td>1757.368164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACC.NS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-18</th>\n",
       "      <td>1838.099976</td>\n",
       "      <td>1775.150024</td>\n",
       "      <td>1833.099976</td>\n",
       "      <td>1787.900024</td>\n",
       "      <td>634760.0</td>\n",
       "      <td>1726.609985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACC.NS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-19</th>\n",
       "      <td>1801.750000</td>\n",
       "      <td>1735.500000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1747.349976</td>\n",
       "      <td>767848.0</td>\n",
       "      <td>1687.450073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACC.NS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-22</th>\n",
       "      <td>12.600000</td>\n",
       "      <td>12.450000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>12.450000</td>\n",
       "      <td>26497740.0</td>\n",
       "      <td>12.450000</td>\n",
       "      <td>38.670505</td>\n",
       "      <td>16.666679</td>\n",
       "      <td>12.613317</td>\n",
       "      <td>YESBANK.NS</td>\n",
       "      <td>0.594261</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-23</th>\n",
       "      <td>12.650000</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>12.450000</td>\n",
       "      <td>27841070.0</td>\n",
       "      <td>12.450000</td>\n",
       "      <td>38.670505</td>\n",
       "      <td>19.658118</td>\n",
       "      <td>16.880346</td>\n",
       "      <td>YESBANK.NS</td>\n",
       "      <td>0.532501</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-24</th>\n",
       "      <td>13.050000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>86625594.0</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>46.384349</td>\n",
       "      <td>24.572642</td>\n",
       "      <td>20.299146</td>\n",
       "      <td>YESBANK.NS</td>\n",
       "      <td>0.585843</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-27</th>\n",
       "      <td>12.950000</td>\n",
       "      <td>12.700000</td>\n",
       "      <td>12.900000</td>\n",
       "      <td>12.850000</td>\n",
       "      <td>33445377.0</td>\n",
       "      <td>12.850000</td>\n",
       "      <td>48.700564</td>\n",
       "      <td>36.111122</td>\n",
       "      <td>26.780627</td>\n",
       "      <td>YESBANK.NS</td>\n",
       "      <td>0.605869</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-28</th>\n",
       "      <td>12.900000</td>\n",
       "      <td>12.650000</td>\n",
       "      <td>12.850000</td>\n",
       "      <td>12.850000</td>\n",
       "      <td>32879307.0</td>\n",
       "      <td>12.850000</td>\n",
       "      <td>48.700564</td>\n",
       "      <td>50.555584</td>\n",
       "      <td>37.079783</td>\n",
       "      <td>YESBANK.NS</td>\n",
       "      <td>0.776334</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15933 rows  16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   High          Low         Open        Close      Volume  \\\n",
       "Date                                                                         \n",
       "2021-02-15  1798.000000  1766.650024  1785.000000  1788.050049    911884.0   \n",
       "2021-02-16  1843.550049  1784.949951  1791.000000  1819.300049   1937730.0   \n",
       "2021-02-17  1837.000000  1807.000000  1820.000000  1819.750000    860949.0   \n",
       "2021-02-18  1838.099976  1775.150024  1833.099976  1787.900024    634760.0   \n",
       "2021-02-19  1801.750000  1735.500000  1800.000000  1747.349976    767848.0   \n",
       "...                 ...          ...          ...          ...         ...   \n",
       "2022-06-22    12.600000    12.450000    12.600000    12.450000  26497740.0   \n",
       "2022-06-23    12.650000    12.400000    12.500000    12.450000  27841070.0   \n",
       "2022-06-24    13.050000    12.500000    12.500000    12.750000  86625594.0   \n",
       "2022-06-27    12.950000    12.700000    12.900000    12.850000  33445377.0   \n",
       "2022-06-28    12.900000    12.650000    12.850000    12.850000  32879307.0   \n",
       "\n",
       "              Adj Close        RSI   STOCH14K   STOCH14D      Ticker  \\\n",
       "Date                                                                   \n",
       "2021-02-15  1726.754883        NaN        NaN        NaN      ACC.NS   \n",
       "2021-02-16  1756.933594        NaN        NaN        NaN      ACC.NS   \n",
       "2021-02-17  1757.368164        NaN        NaN        NaN      ACC.NS   \n",
       "2021-02-18  1726.609985        NaN        NaN        NaN      ACC.NS   \n",
       "2021-02-19  1687.450073        NaN        NaN        NaN      ACC.NS   \n",
       "...                 ...        ...        ...        ...         ...   \n",
       "2022-06-22    12.450000  38.670505  16.666679  12.613317  YESBANK.NS   \n",
       "2022-06-23    12.450000  38.670505  19.658118  16.880346  YESBANK.NS   \n",
       "2022-06-24    12.750000  46.384349  24.572642  20.299146  YESBANK.NS   \n",
       "2022-06-27    12.850000  48.700564  36.111122  26.780627  YESBANK.NS   \n",
       "2022-06-28    12.850000  48.700564  50.555584  37.079783  YESBANK.NS   \n",
       "\n",
       "            Prediction Probability  Buy Signal  Buy Price  Sell Price  Profit  \\\n",
       "Date                                                                            \n",
       "2021-02-15                     NaN           0        NaN         NaN     NaN   \n",
       "2021-02-16                     NaN           0        NaN         NaN     NaN   \n",
       "2021-02-17                     NaN           0        NaN         NaN     NaN   \n",
       "2021-02-18                     NaN           0        NaN         NaN     NaN   \n",
       "2021-02-19                     NaN           0        NaN         NaN     NaN   \n",
       "...                            ...         ...        ...         ...     ...   \n",
       "2022-06-22                0.594261           0        NaN         NaN     NaN   \n",
       "2022-06-23                0.532501           0        NaN         NaN     NaN   \n",
       "2022-06-24                0.585843           0        NaN         NaN     NaN   \n",
       "2022-06-27                0.605869           0        NaN         NaN     NaN   \n",
       "2022-06-28                0.776334           0        NaN         NaN     NaN   \n",
       "\n",
       "            Profit Percentage  \n",
       "Date                           \n",
       "2021-02-15                NaN  \n",
       "2021-02-16                NaN  \n",
       "2021-02-17                NaN  \n",
       "2021-02-18                NaN  \n",
       "2021-02-19                NaN  \n",
       "...                       ...  \n",
       "2022-06-22                NaN  \n",
       "2022-06-23                NaN  \n",
       "2022-06-24                NaN  \n",
       "2022-06-27                NaN  \n",
       "2022-06-28                NaN  \n",
       "\n",
       "[15933 rows x 16 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf = realistic_back_test(all_model, 500, 2, 0.9)\n",
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    477.000000\n",
       "mean      -0.013155\n",
       "std        5.285240\n",
       "min      -16.261332\n",
       "25%       -2.451024\n",
       "50%       -0.186194\n",
       "75%        1.859842\n",
       "max       32.239153\n",
       "Name: Profit Percentage, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf['Profit Percentage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x27131bb9550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(asdf['Prediction Probability'], asdf['Profit Percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "ABNB\n",
      "ADBE\n",
      "ADI\n",
      "ADP\n",
      "ADSK\n",
      "AEP\n",
      "ALGN\n",
      "AMAT\n",
      "AMD\n",
      "AMGN\n",
      "AMZN\n",
      "ANSS\n",
      "ASML\n",
      "ATVI\n",
      "AVGO\n",
      "AZN\n",
      "BIDU\n",
      "BIIB\n",
      "BKNG\n",
      "CDNS\n",
      "CEG\n",
      "CHTR\n",
      "CMCSA\n",
      "COST\n",
      "CPRT\n",
      "CRWD\n",
      "CSCO\n",
      "CSX\n",
      "CTAS\n",
      "CTSH\n",
      "DDOG\n",
      "DLTR\n",
      "DOCU\n",
      "DXCM\n",
      "EA\n",
      "EBAY\n",
      "EXC\n",
      "FAST\n",
      "FISV\n",
      "FTNT\n",
      "GILD\n",
      "GOOG\n",
      "GOOGL\n",
      "HON\n",
      "IDXX\n",
      "ILMN\n",
      "INTC\n",
      "INTU\n",
      "ISRG\n",
      "JD\n",
      "KDP\n",
      "KHC\n",
      "KLAC\n",
      "LCID\n",
      "LRCX\n",
      "LULU\n",
      "MAR\n",
      "MCHP\n",
      "MDLZ\n",
      "MELI\n",
      "META\n",
      "MNST\n",
      "MRNA\n",
      "MRVL\n",
      "MSFT\n",
      "MTCH\n",
      "MU\n",
      "NFLX\n",
      "NTES\n",
      "NVDA\n",
      "NXPI\n",
      "ODFL\n",
      "OKTA\n",
      "ORLY\n",
      "PANW\n",
      "PAYX\n",
      "PCAR\n",
      "PDD\n",
      "PEP\n",
      "PYPL\n",
      "QCOM\n",
      "REGN\n",
      "ROST\n",
      "SBUX\n",
      "SGEN\n",
      "SIRI\n",
      "SNPS\n",
      "SPLK\n",
      "SWKS\n",
      "TEAM\n",
      "TMUS\n",
      "TSLA\n",
      "TXN\n",
      "VRSK\n",
      "VRSN\n",
      "VRTX\n",
      "WBA\n",
      "WDAY\n",
      "XEL\n",
      "ZM\n",
      "ZS\n",
      "count    2249.000000\n",
      "mean        0.171125\n",
      "std         4.655109\n",
      "min       -24.714130\n",
      "25%        -1.940525\n",
      "50%         0.230888\n",
      "75%         2.257007\n",
      "max        36.245400\n",
      "Name: Profit Percentage, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1ec00460100>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# American Stocks\n",
    "american_df = realistic_back_test(all_model, 500, 3, 0.85)\n",
    "print(american_df['Profit Percentage'].describe())\n",
    "plt.scatter(american_df['Prediction Probability'], american_df['Profit Percentage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mean values shown in the above cells show that this kind of a model is basically a coin flip. There a minimal gains to be had when using this model even with a very high confidence value. Will be working on a CNN-LSTM, Transformer, etc based model in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backtest_df_abhi(mode, ticker_for_backtest, thresh_long, thresh_short, train_start_date, test_end_date, dur = 5):\n",
    "    pd_datareader_data = data.DataReader(ticker_for_backtest, 'yahoo', train_start_date, test_end_date)\n",
    "    total = len(pd_datareader_data)\n",
    "    pd_datareader_data['RSI_14'] = RSI(pd_datareader_data['Close'], timeperiod=14)\n",
    "    \n",
    "\n",
    "    if(mode == \"ALL\"):\n",
    "        backtest_company_price_data = data_cleaning_for_all_indicators_abhi(pd_datareader_data, 5)\n",
    "        model = load_model('ALL_model.h5')\n",
    "    elif(mode == \"RSI\"):\n",
    "        backtest_company_price_data = data_cleaning_RSI(pd_datareader_data, 5)\n",
    "        model = load_model('RSI_Model.h5')\n",
    "    \n",
    "    \n",
    "    #Changing price data to drop rows after creating df\n",
    "    pd_datareader_data = pd_datareader_data[15 :] # Row mismatch fix requires logic for dropping rows\n",
    "    # Model prediction on test data\n",
    "    y_pred_backtest = model.predict(backtest_company_price_data.drop(columns = ['long_or_short']))\n",
    "    backtest_df = backtest_company_price_data.copy()\n",
    "    backtest_df['prob_pred'] = y_pred_backtest\n",
    "    # Creating a column with high confidence outputs for long/short\n",
    "    backtest_df['thresh_signal'] = backtest_df['prob_pred'].apply(lambda x: 1 if x > thresh_long else (0 if x < thresh_short else np.nan))\n",
    "    backtest_df.reset_index(inplace = True, drop = True)\n",
    "    high_conf_stats =  backtest_df[['thresh_signal', 'long_or_short']].dropna()\n",
    "#     print(\"\\nPredictions of high confidence = \", \n",
    "#           len(high_conf_stats), \"\\n\")\n",
    "#     print(\"\\nClassification Report\", classification_report(high_conf_stats['long_or_short'],\n",
    "#                             high_conf_stats['thresh_signal']), \"\\n\")\n",
    "    \n",
    "    \n",
    "    high_conf_colormap = []\n",
    "    buy_sell_signal_from_model = []\n",
    "    for s in backtest_df['thresh_signal'].values:\n",
    "        if (s == 0.0):\n",
    "            for i in range(1):\n",
    "                high_conf_colormap.append('k')\n",
    "                buy_sell_signal_from_model.append(0)\n",
    "        elif(s == 1.0):\n",
    "            for i in range(1):\n",
    "                high_conf_colormap.append('y')\n",
    "                buy_sell_signal_from_model.append(1)\n",
    "        else:\n",
    "            for i in range(1):\n",
    "                high_conf_colormap.append('b')\n",
    "                buy_sell_signal_from_model.append(np.nan)\n",
    "\n",
    "    \n",
    "    # Fix for df shape mismatch\n",
    "    row_mismatch_count = pd_datareader_data.shape[0] - len(buy_sell_signal_from_model)\n",
    "    pd_datareader_data = pd_datareader_data[ : -row_mismatch_count]\n",
    "    pd_datareader_data['buy_sell_signal_from_model'] = buy_sell_signal_from_model\n",
    "    \n",
    "    # Profit/Loss calculation for long signals\n",
    "    profit_long = []\n",
    "    buy_price = []\n",
    "    sell_price = []\n",
    "    \n",
    "    for i in range(len(pd_datareader_data) - dur):\n",
    "        if(pd_datareader_data.iloc[i]['buy_sell_signal_from_model'] == 1.0):\n",
    "            profit = pd_datareader_data.iloc[i + dur]['Close'] - pd_datareader_data.iloc[i + 1]['Close']\n",
    "            profit_long.append(profit)\n",
    "            buy_price.append(pd_datareader_data.iloc[i + 1]['Close'])\n",
    "            sell_price.append(pd_datareader_data.iloc[i + dur]['Close'])\n",
    "        else:\n",
    "            profit_long.append(np.nan)\n",
    "            buy_price.append(np.nan)\n",
    "            sell_price.append(np.nan)\n",
    "            \n",
    "    # For the last \"dur\" days we cannot get a profit since those days dont exist. Hence just adding nans. \n",
    "    for i in range(dur):\n",
    "        profit_long.append(np.nan)\n",
    "        buy_price.append(np.nan)\n",
    "        sell_price.append(np.nan)\n",
    "\n",
    "    pd_datareader_data['profit_long'] = profit_long\n",
    "    pd_datareader_data['buy_price'] = buy_price\n",
    "    pd_datareader_data['sell_price'] = sell_price\n",
    "    pd_datareader_data['profit_percentage_per_trade'] = (pd_datareader_data['sell_price'] - pd_datareader_data['buy_price']) / pd_datareader_data['buy_price'] * 100\n",
    "    return(pd_datareader_data[pd_datareader_data['buy_sell_signal_from_model'] == 1], total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1\n",
      "15 6\n",
      "20 11\n",
      "25 16\n",
      "30 21\n",
      "35 26\n",
      "40 31\n",
      "45 36\n",
      "50 41\n",
      "55 46\n",
      "60 51\n",
      "65 56\n",
      "70 61\n",
      "75 66\n",
      "80 71\n",
      "85 76\n",
      "90 81\n",
      "95 86\n",
      "100 91\n",
      "105 96\n",
      "110 101\n",
      "115 106\n",
      "120 111\n",
      "125 116\n",
      "130 121\n",
      "135 126\n",
      "140 131\n",
      "145 136\n",
      "150 141\n",
      "155 146\n",
      "160 151\n",
      "165 156\n",
      "170 161\n",
      "175 166\n",
      "180 171\n",
      "185 176\n",
      "190 181\n",
      "195 186\n",
      "200 191\n",
      "205 196\n",
      "210 201\n",
      "215 206\n",
      "220 211\n",
      "225 216\n",
      "230 221\n",
      "235 226\n",
      "240 231\n",
      "245 236\n",
      "250 241\n",
      "255 246\n",
      "260 251\n",
      "265 256\n",
      "270 261\n",
      "275 266\n",
      "280 271\n",
      "285 276\n",
      "290 281\n",
      "295 286\n",
      "300 291\n",
      "305 296\n",
      "310 301\n",
      "315 306\n",
      "320 311\n",
      "325 316\n",
      "330 321\n",
      "335 326\n",
      "340 331\n",
      "345 336\n",
      "350 341\n",
      "355 346\n",
      "360 351\n",
      "365 356\n",
      "370 361\n",
      "375 366\n",
      "380 371\n",
      "385 376\n",
      "390 381\n",
      "395 386\n",
      "400 391\n",
      "405 396\n",
      "410 401\n",
      "415 406\n",
      "420 411\n",
      "425 416\n",
      "430 421\n",
      "435 426\n",
      "440 431\n",
      "445 436\n",
      "450 441\n",
      "455 446\n",
      "460 451\n",
      "465 456\n",
      "470 461\n",
      "475 466\n",
      "480 471\n",
      "485 476\n",
      "490 481\n",
      "495 486\n",
      "500 491\n",
      "505 496\n",
      "510 501\n",
      "515 506\n",
      "520 511\n",
      "525 516\n",
      "530 521\n",
      "535 526\n",
      "540 531\n",
      "545 536\n",
      "550 541\n",
      "555 546\n",
      "560 551\n",
      "565 556\n",
      "570 561\n",
      "575 566\n",
      "580 571\n",
      "585 576\n",
      "590 581\n",
      "595 586\n",
      "600 591\n",
      "605 596\n",
      "610 601\n",
      "615 606\n",
      "620 611\n",
      "625 616\n",
      "630 621\n",
      "635 626\n",
      "640 631\n",
      "645 636\n",
      "650 641\n",
      "655 646\n",
      "660 651\n",
      "665 656\n",
      "670 661\n",
      "675 666\n",
      "680 671\n",
      "685 676\n",
      "690 681\n",
      "695 686\n",
      "700 691\n",
      "705 696\n",
      "710 701\n",
      "715 706\n",
      "720 711\n",
      "725 716\n",
      "730 721\n",
      "735 726\n",
      "740 731\n",
      "745 736\n",
      "750 741\n",
      "755 746\n",
      "760 751\n",
      "765 756\n",
      "770 761\n",
      "775 766\n",
      "780 771\n",
      "785 776\n",
      "790 781\n",
      "795 786\n",
      "800 791\n",
      "805 796\n",
      "810 801\n",
      "815 806\n",
      "820 811\n",
      "825 816\n",
      "830 821\n",
      "835 826\n",
      "840 831\n",
      "845 836\n",
      "850 841\n",
      "855 846\n",
      "860 851\n",
      "865 856\n",
      "870 861\n",
      "875 866\n",
      "880 871\n",
      "885 876\n",
      "890 881\n",
      "895 886\n",
      "900 891\n",
      "905 896\n",
      "910 901\n",
      "915 906\n",
      "920 911\n",
      "925 916\n",
      "930 921\n",
      "935 926\n",
      "940 931\n",
      "945 936\n",
      "950 941\n",
      "955 946\n",
      "960 951\n",
      "965 956\n",
      "970 961\n",
      "975 966\n",
      "980 971\n",
      "985 976\n",
      "990 981\n",
      "995 986\n",
      "1000 991\n",
      "1005 996\n",
      "1010 1001\n",
      "1015 1006\n",
      "1020 1011\n",
      "1025 1016\n",
      "1030 1021\n",
      "1035 1026\n",
      "1040 1031\n",
      "1045 1036\n",
      "1050 1041\n",
      "1055 1046\n",
      "1060 1051\n",
      "1065 1056\n",
      "1070 1061\n",
      "1075 1066\n",
      "1080 1071\n",
      "1085 1076\n",
      "1090 1081\n",
      "1095 1086\n",
      "1100 1091\n",
      "1105 1096\n",
      "1110 1101\n",
      "1115 1106\n",
      "1120 1111\n",
      "1125 1116\n",
      "1130 1121\n",
      "1135 1126\n",
      "1140 1131\n",
      "1145 1136\n",
      "1150 1141\n",
      "1155 1146\n",
      "1160 1151\n",
      "1165 1156\n",
      "1170 1161\n",
      "1175 1166\n",
      "1180 1171\n",
      "1185 1176\n",
      "1190 1181\n",
      "1195 1186\n",
      "1200 1191\n",
      "1205 1196\n",
      "1210 1201\n",
      "1215 1206\n",
      "1220 1211\n",
      "1225 1216\n",
      "1230 1221\n",
      "1235 1226\n",
      "1240 1231\n",
      "1245 1236\n",
      "1250 1241\n",
      "1255 1246\n",
      "1260 1251\n",
      "1265 1256\n",
      "1270 1261\n",
      "1275 1266\n",
      "1280 1271\n",
      "1285 1276\n",
      "1290 1281\n",
      "1295 1286\n",
      "1300 1291\n",
      "1305 1296\n",
      "1310 1301\n",
      "1315 1306\n",
      "1320 1311\n",
      "1325 1316\n",
      "1330 1321\n",
      "1335 1326\n",
      "1340 1331\n",
      "1345 1336\n",
      "1350 1341\n",
      "1355 1346\n",
      "1360 1351\n",
      "1365 1356\n",
      "1370 1361\n",
      "1375 1366\n",
      "1380 1371\n",
      "1385 1376\n",
      "1390 1381\n",
      "1395 1386\n",
      "1400 1391\n",
      "1405 1396\n",
      "1410 1401\n",
      "1415 1406\n",
      "1420 1411\n",
      "1425 1416\n",
      "1430 1421\n",
      "1435 1426\n",
      "1440 1431\n",
      "1445 1436\n",
      "1450 1441\n",
      "1455 1446\n",
      "1460 1451\n",
      "1465 1456\n",
      "1470 1461\n",
      "1475 1466\n",
      "1480 1471\n",
      "1485 1476\n",
      "1490 1481\n",
      "1495 1486\n",
      "1500 1491\n",
      "1505 1496\n",
      "1510 1501\n",
      "1515 1506\n",
      "1520 1511\n",
      "1525 1516\n",
      "1530 1521\n",
      "1535 1526\n",
      "1540 1531\n",
      "1545 1536\n",
      "1550 1541\n",
      "1555 1546\n",
      "1560 1551\n",
      "1565 1556\n",
      "1570 1561\n",
      "1575 1566\n",
      "1580 1571\n",
      "1585 1576\n",
      "1590 1581\n",
      "1595 1586\n",
      "1600 1591\n",
      "1605 1596\n",
      "1610 1601\n",
      "1615 1606\n",
      "1620 1611\n",
      "1625 1616\n",
      "1630 1621\n",
      "1635 1626\n",
      "1640 1631\n",
      "1645 1636\n",
      "1650 1641\n",
      "1655 1646\n",
      "1660 1651\n",
      "1665 1656\n",
      "1670 1661\n",
      "1675 1666\n",
      "1680 1671\n",
      "1685 1676\n",
      "1690 1681\n",
      "1695 1686\n",
      "1700 1691\n",
      "1705 1696\n",
      "1710 1701\n",
      "1715 1706\n",
      "1720 1711\n",
      "1725 1716\n",
      "1730 1721\n",
      "1735 1726\n",
      "1740 1731\n",
      "1745 1736\n",
      "1750 1741\n",
      "1755 1746\n",
      "1760 1751\n",
      "1765 1756\n",
      "1770 1761\n",
      "1775 1766\n",
      "1780 1771\n",
      "1785 1776\n",
      "1790 1781\n",
      "1795 1786\n",
      "1800 1791\n",
      "1805 1796\n",
      "1810 1801\n",
      "1815 1806\n",
      "1820 1811\n",
      "1825 1816\n",
      "1830 1821\n",
      "1835 1826\n",
      "1840 1831\n",
      "1845 1836\n",
      "1850 1841\n",
      "1855 1846\n",
      "1860 1851\n",
      "1865 1856\n",
      "1870 1861\n",
      "1875 1866\n",
      "1880 1871\n",
      "1885 1876\n",
      "1890 1881\n",
      "1895 1886\n",
      "1900 1891\n",
      "1905 1896\n",
      "1910 1901\n",
      "1915 1906\n",
      "1920 1911\n",
      "1925 1916\n",
      "1930 1921\n",
      "1935 1926\n",
      "1940 1931\n",
      "1945 1936\n",
      "1950 1941\n",
      "1955 1946\n",
      "1960 1951\n",
      "1965 1956\n",
      "1970 1961\n",
      "1975 1966\n",
      "1980 1971\n",
      "1985 1976\n",
      "1990 1981\n",
      "1995 1986\n",
      "2000 1991\n",
      "2005 1996\n",
      "2010 2001\n",
      "2015 2006\n",
      "2020 2011\n",
      "2025 2016\n",
      "2030 2021\n",
      "2035 2026\n",
      "2040 2031\n",
      "2045 2036\n",
      "2050 2041\n",
      "2055 2046\n",
      "2060 2051\n",
      "2065 2056\n",
      "2070 2061\n",
      "2075 2066\n",
      "2080 2071\n",
      "2085 2076\n",
      "2090 2081\n",
      "2095 2086\n",
      "2100 2091\n",
      "2105 2096\n",
      "2110 2101\n",
      "2115 2106\n",
      "2120 2111\n",
      "2125 2116\n",
      "2130 2121\n",
      "2135 2126\n",
      "2140 2131\n",
      "2145 2136\n",
      "2150 2141\n",
      "2155 2146\n",
      "2160 2151\n",
      "2165 2156\n",
      "2170 2161\n",
      "2175 2166\n",
      "2180 2171\n",
      "2185 2176\n",
      "2190 2181\n",
      "2195 2186\n",
      "2200 2191\n",
      "2205 2196\n",
      "2210 2201\n",
      "2215 2206\n",
      "2220 2211\n",
      "2225 2216\n",
      "2230 2221\n",
      "2235 2226\n",
      "2240 2231\n",
      "2245 2236\n",
      "2250 2241\n",
      "2255 2246\n",
      "2260 2251\n",
      "2265 2256\n",
      "2270 2261\n",
      "2275 2266\n",
      "2280 2271\n",
      "2285 2276\n",
      "2290 2281\n",
      "2295 2286\n",
      "2300 2291\n",
      "2305 2296\n",
      "2310 2301\n",
      "2315 2306\n",
      "2320 2311\n",
      "2325 2316\n",
      "2330 2321\n",
      "2335 2326\n",
      "2340 2331\n",
      "2345 2336\n",
      "2350 2341\n",
      "2355 2346\n",
      "2360 2351\n",
      "2365 2356\n",
      "2370 2361\n",
      "2375 2366\n",
      "2380 2371\n",
      "2385 2376\n",
      "2390 2381\n",
      "2395 2386\n",
      "2400 2391\n",
      "2405 2396\n",
      "2410 2401\n",
      "2415 2406\n",
      "2420 2411\n",
      "2425 2416\n",
      "2430 2421\n",
      "2435 2426\n",
      "2440 2431\n",
      "2445 2436\n",
      "2450 2441\n",
      "2455 2446\n",
      "2460 2451\n",
      "2465 2456\n",
      "2470 2461\n",
      "2475 2466\n",
      "2480 2471\n",
      "2485 2476\n",
      "2490 2481\n",
      "2495 2486\n",
      "2500 2491\n",
      "2505 2496\n",
      "2510 2501\n",
      "2515 2506\n",
      "2520 2511\n",
      "2525 2516\n",
      "2530 2521\n",
      "2535 2526\n",
      "2540 2531\n",
      "2545 2536\n",
      "2550 2541\n",
      "2555 2546\n",
      "2560 2551\n",
      "2565 2556\n",
      "2570 2561\n",
      "2575 2566\n",
      "2580 2571\n",
      "2585 2576\n",
      "2590 2581\n",
      "2595 2586\n",
      "2600 2591\n",
      "2605 2596\n",
      "2610 2601\n",
      "2615 2606\n",
      "2620 2611\n",
      "2625 2616\n",
      "2630 2621\n",
      "2635 2626\n",
      "2640 2631\n",
      "2645 2636\n",
      "2650 2641\n",
      "2655 2646\n",
      "2660 2651\n",
      "2665 2656\n",
      "2670 2661\n",
      "2675 2666\n",
      "2680 2671\n",
      "2685 2676\n",
      "2690 2681\n",
      "2695 2686\n",
      "2700 2691\n",
      "2705 2696\n",
      "2710 2701\n",
      "2715 2706\n",
      "2720 2711\n",
      "2725 2716\n",
      "2730 2721\n",
      "2735 2726\n",
      "2740 2731\n",
      "2745 2736\n",
      "2750 2741\n",
      "2755 2746\n",
      "2760 2751\n",
      "2765 2756\n",
      "2770 2761\n",
      "2775 2766\n",
      "2780 2771\n",
      "2785 2776\n",
      "2790 2781\n",
      "2795 2786\n",
      "2800 2791\n",
      "2805 2796\n",
      "2810 2801\n",
      "2815 2806\n",
      "2820 2811\n",
      "2825 2816\n",
      "2830 2821\n",
      "2835 2826\n",
      "2840 2831\n",
      "2845 2836\n",
      "2850 2841\n",
      "2855 2846\n",
      "2860 2851\n",
      "2865 2856\n",
      "2870 2861\n",
      "2875 2866\n",
      "2880 2871\n",
      "2885 2876\n",
      "2890 2881\n",
      "2895 2886\n",
      "2900 2891\n",
      "2905 2896\n",
      "2910 2901\n",
      "2915 2906\n",
      "2920 2911\n",
      "2925 2916\n",
      "2930 2921\n",
      "2935 2926\n",
      "2940 2931\n",
      "2945 2936\n",
      "2950 2941\n",
      "2955 2946\n",
      "2960 2951\n",
      "2965 2956\n",
      "2970 2961\n",
      "2975 2966\n",
      "2980 2971\n",
      "2985 2976\n",
      "2990 2981\n",
      "2995 2986\n",
      "3000 2991\n",
      "3005 2996\n",
      "3010 3001\n",
      "3015 3006\n",
      "3020 3011\n",
      "3025 3016\n",
      "3030 3021\n",
      "3035 3026\n",
      "3040 3031\n",
      "3045 3036\n",
      "3050 3041\n",
      "3055 3046\n",
      "3060 3051\n",
      "3065 3056\n",
      "3070 3061\n",
      "3075 3066\n",
      "3080 3071\n",
      "3085 3076\n",
      "3090 3081\n",
      "3095 3086\n",
      "3100 3091\n",
      "3105 3096\n",
      "3110 3101\n",
      "3115 3106\n",
      "3120 3111\n",
      "3125 3116\n",
      "3130 3121\n",
      "3135 3126\n",
      "3140 3131\n",
      "3145 3136\n",
      "3150 3141\n",
      "3155 3146\n",
      "3160 3151\n",
      "3165 3156\n",
      "3170 3161\n",
      "3175 3166\n",
      "3180 3171\n",
      "3185 3176\n",
      "3190 3181\n",
      "3195 3186\n",
      "3200 3191\n",
      "3205 3196\n",
      "3210 3201\n",
      "3215 3206\n",
      "3220 3211\n",
      "3225 3216\n",
      "3230 3221\n",
      "3235 3226\n",
      "3240 3231\n",
      "3245 3236\n",
      "3250 3241\n",
      "3255 3246\n",
      "3260 3251\n",
      "3265 3256\n",
      "3270 3261\n",
      "3275 3266\n",
      "3280 3271\n",
      "3285 3276\n",
      "3290 3281\n",
      "3295 3286\n",
      "3300 3291\n",
      "3305 3296\n",
      "3310 3301\n",
      "3315 3306\n",
      "3320 3311\n",
      "3325 3316\n",
      "3330 3321\n",
      "3335 3326\n",
      "3340 3331\n",
      "3345 3336\n",
      "3350 3341\n",
      "3355 3346\n",
      "3360 3351\n",
      "3365 3356\n",
      "3370 3361\n",
      "3375 3366\n",
      "3380 3371\n",
      "3385 3376\n",
      "3390 3381\n",
      "3395 3386\n",
      "3400 3391\n",
      "3405 3396\n",
      "3410 3401\n",
      "3415 3406\n",
      "3420 3411\n",
      "3425 3416\n",
      "3430 3421\n",
      "3435 3426\n",
      "3440 3431\n",
      "3445 3436\n",
      "3450 3441\n",
      "3455 3446\n",
      "3460 3451\n",
      "3465 3456\n",
      "3470 3461\n",
      "3475 3466\n",
      "3480 3471\n",
      "3485 3476\n",
      "3490 3481\n",
      "3495 3486\n",
      "3500 3491\n",
      "3505 3496\n",
      "3510 3501\n",
      "3515 3506\n",
      "3520 3511\n",
      "3525 3516\n",
      "3530 3521\n",
      "3535 3526\n",
      "3540 3531\n",
      "3545 3536\n",
      "3550 3541\n",
      "3555 3546\n",
      "3560 3551\n",
      "3565 3556\n",
      "3570 3561\n",
      "3575 3566\n",
      "3580 3571\n",
      "3585 3576\n",
      "3590 3581\n",
      "3595 3586\n",
      "3600 3591\n",
      "3605 3596\n",
      "3610 3601\n",
      "3615 3606\n",
      "3620 3611\n",
      "3625 3616\n",
      "3630 3621\n",
      "3635 3626\n",
      "3640 3631\n",
      "3645 3636\n",
      "3650 3641\n",
      "3655 3646\n",
      "3660 3651\n",
      "3665 3656\n",
      "3670 3661\n",
      "3675 3666\n",
      "3680 3671\n",
      "3685 3676\n",
      "3690 3681\n",
      "3695 3686\n",
      "3700 3691\n",
      "3705 3696\n",
      "3710 3701\n",
      "3715 3706\n",
      "3720 3711\n",
      "3725 3716\n",
      "3730 3721\n",
      "3735 3726\n",
      "3740 3731\n",
      "3745 3736\n",
      "3750 3741\n",
      "3755 3746\n",
      "3760 3751\n",
      "3765 3756\n",
      "3770 3761\n",
      "3775 3766\n",
      "3780 3771\n",
      "3785 3776\n",
      "3790 3781\n",
      "3795 3786\n",
      "3800 3791\n",
      "3805 3796\n",
      "3810 3801\n",
      "3815 3806\n",
      "3820 3811\n",
      "3825 3816\n",
      "3830 3821\n",
      "3835 3826\n",
      "3840 3831\n",
      "3845 3836\n",
      "3850 3841\n",
      "3855 3846\n",
      "3860 3851\n",
      "3865 3856\n",
      "3870 3861\n",
      "3875 3866\n",
      "3880 3871\n",
      "3885 3876\n",
      "3890 3881\n",
      "3895 3886\n",
      "3900 3891\n",
      "3905 3896\n",
      "3910 3901\n",
      "3915 3906\n",
      "3920 3911\n",
      "3925 3916\n",
      "3930 3921\n",
      "3935 3926\n",
      "3940 3931\n",
      "3945 3936\n",
      "3950 3941\n",
      "3955 3946\n",
      "3960 3951\n",
      "3965 3956\n",
      "3970 3961\n",
      "3975 3966\n",
      "3980 3971\n",
      "3985 3976\n",
      "3990 3981\n",
      "3995 3986\n",
      "4000 3991\n",
      "4005 3996\n",
      "4010 4001\n",
      "4015 4006\n",
      "4020 4011\n",
      "4025 4016\n",
      "4030 4021\n",
      "4035 4026\n",
      "4040 4031\n",
      "4045 4036\n",
      "4050 4041\n",
      "4055 4046\n",
      "4060 4051\n",
      "4065 4056\n",
      "4070 4061\n",
      "4075 4066\n",
      "4080 4071\n",
      "4085 4076\n",
      "4090 4081\n",
      "4095 4086\n",
      "4100 4091\n",
      "4105 4096\n",
      "4110 4101\n",
      "4115 4106\n",
      "4120 4111\n",
      "4125 4116\n",
      "4130 4121\n",
      "4135 4126\n",
      "4140 4131\n",
      "4145 4136\n",
      "4150 4141\n",
      "4155 4146\n",
      "4160 4151\n",
      "4165 4156\n",
      "4170 4161\n",
      "4175 4166\n",
      "4180 4171\n",
      "4185 4176\n",
      "4190 4181\n",
      "4195 4186\n",
      "4200 4191\n",
      "4205 4196\n",
      "4210 4201\n",
      "4215 4206\n",
      "4220 4211\n",
      "4225 4216\n",
      "4230 4221\n",
      "4235 4226\n",
      "4240 4231\n",
      "4245 4236\n",
      "4250 4241\n",
      "4255 4246\n",
      "4260 4251\n",
      "4265 4256\n",
      "4270 4261\n",
      "4275 4266\n",
      "4280 4271\n",
      "4285 4276\n",
      "4290 4281\n",
      "4295 4286\n",
      "4300 4291\n",
      "4305 4296\n",
      "4310 4301\n",
      "4315 4306\n",
      "4320 4311\n",
      "4325 4316\n",
      "4330 4321\n",
      "4335 4326\n",
      "4340 4331\n",
      "4345 4336\n",
      "4350 4341\n",
      "4355 4346\n",
      "4360 4351\n",
      "4365 4356\n",
      "4370 4361\n",
      "4375 4366\n",
      "4380 4371\n",
      "4385 4376\n",
      "4390 4381\n",
      "4395 4386\n",
      "4400 4391\n",
      "4405 4396\n",
      "4410 4401\n",
      "4415 4406\n",
      "4420 4411\n",
      "4425 4416\n",
      "4430 4421\n",
      "4435 4426\n",
      "4440 4431\n",
      "4445 4436\n",
      "4450 4441\n",
      "4455 4446\n",
      "4460 4451\n",
      "4465 4456\n",
      "4470 4461\n",
      "4475 4466\n",
      "4480 4471\n",
      "4485 4476\n",
      "4490 4481\n",
      "4495 4486\n",
      "4500 4491\n",
      "4505 4496\n",
      "4510 4501\n",
      "4515 4506\n",
      "4520 4511\n",
      "4525 4516\n",
      "4530 4521\n",
      "4535 4526\n",
      "4540 4531\n",
      "4545 4536\n",
      "4550 4541\n",
      "4555 4546\n",
      "4560 4551\n",
      "4565 4556\n",
      "4570 4561\n",
      "4575 4566\n",
      "4580 4571\n",
      "4585 4576\n",
      "4590 4581\n",
      "4595 4586\n",
      "4600 4591\n",
      "4605 4596\n",
      "4610 4601\n",
      "4615 4606\n",
      "4620 4611\n",
      "4625 4616\n",
      "4630 4621\n",
      "4635 4626\n",
      "4640 4631\n",
      "4645 4636\n",
      "4650 4641\n",
      "4655 4646\n",
      "4660 4651\n",
      "4665 4656\n",
      "4670 4661\n",
      "4675 4666\n",
      "4680 4671\n",
      "4685 4676\n",
      "4690 4681\n",
      "4695 4686\n",
      "4700 4691\n",
      "4705 4696\n",
      "4710 4701\n",
      "4715 4706\n",
      "4720 4711\n",
      "4725 4716\n",
      "4730 4721\n",
      "4735 4726\n",
      "4740 4731\n",
      "4745 4736\n",
      "4750 4741\n",
      "4755 4746\n",
      "4760 4751\n",
      "4765 4756\n",
      "4770 4761\n",
      "4775 4766\n",
      "4780 4771\n",
      "4785 4776\n",
      "4790 4781\n",
      "4795 4786\n",
      "4800 4791\n",
      "4805 4796\n",
      "4810 4801\n",
      "4815 4806\n",
      "4820 4811\n",
      "4825 4816\n",
      "4830 4821\n",
      "4835 4826\n",
      "4840 4831\n",
      "4845 4836\n",
      "4850 4841\n",
      "4855 4846\n",
      "4860 4851\n",
      "4865 4856\n",
      "4870 4861\n",
      "4875 4866\n",
      "4880 4871\n",
      "4885 4876\n",
      "4890 4881\n",
      "4895 4886\n",
      "4900 4891\n",
      "4905 4896\n",
      "4910 4901\n",
      "4915 4906\n",
      "4920 4911\n",
      "4925 4916\n",
      "4930 4921\n",
      "4935 4926\n",
      "4940 4931\n",
      "4945 4936\n",
      "4950 4941\n",
      "4955 4946\n",
      "4960 4951\n",
      "4965 4956\n",
      "4970 4961\n",
      "4975 4966\n",
      "4980 4971\n",
      "4985 4976\n",
      "4990 4981\n",
      "4995 4986\n",
      "5000 4991\n",
      "5005 4996\n",
      "5010 5001\n",
      "5015 5006\n",
      "5020 5011\n",
      "5025 5016\n",
      "5030 5021\n",
      "5035 5026\n",
      "5040 5031\n",
      "5045 5036\n",
      "5050 5041\n",
      "5055 5046\n",
      "5060 5051\n",
      "5065 5056\n",
      "5070 5061\n",
      "5075 5066\n",
      "5080 5071\n",
      "5085 5076\n",
      "5090 5081\n",
      "5095 5086\n",
      "5100 5091\n",
      "5105 5096\n",
      "5110 5101\n",
      "5115 5106\n",
      "5120 5111\n",
      "5125 5116\n",
      "5130 5121\n",
      "5135 5126\n",
      "5140 5131\n",
      "5145 5136\n",
      "5150 5141\n",
      "5155 5146\n",
      "5160 5151\n",
      "5165 5156\n",
      "5170 5161\n",
      "5175 5166\n",
      "5180 5171\n",
      "5185 5176\n",
      "5190 5181\n",
      "5195 5186\n",
      "5200 5191\n",
      "5205 5196\n",
      "5210 5201\n",
      "5215 5206\n",
      "5220 5211\n",
      "5225 5216\n",
      "5230 5221\n",
      "5235 5226\n",
      "5240 5231\n",
      "5245 5236\n",
      "5250 5241\n",
      "5255 5246\n",
      "5260 5251\n",
      "5265 5256\n",
      "5270 5261\n",
      "5275 5266\n",
      "5280 5271\n",
      "5285 5276\n",
      "5290 5281\n",
      "5295 5286\n",
      "5300 5291\n",
      "5305 5296\n",
      "5310 5301\n",
      "5315 5306\n",
      "5320 5311\n",
      "5325 5316\n",
      "5330 5321\n",
      "5335 5326\n",
      "5340 5331\n",
      "5345 5336\n",
      "5350 5341\n",
      "5355 5346\n",
      "5360 5351\n",
      "5365 5356\n",
      "5370 5361\n",
      "5375 5366\n",
      "5380 5371\n",
      "5385 5376\n",
      "5390 5381\n",
      "5395 5386\n",
      "5400 5391\n",
      "5405 5396\n",
      "5410 5401\n",
      "5415 5406\n",
      "5420 5411\n",
      "5425 5416\n",
      "5430 5421\n",
      "5435 5426\n",
      "5440 5431\n",
      "5445 5436\n",
      "5450 5441\n",
      "5455 5446\n",
      "5460 5451\n",
      "5465 5456\n",
      "5470 5461\n",
      "5475 5466\n",
      "5480 5471\n",
      "5485 5476\n",
      "5490 5481\n",
      "5495 5486\n",
      "5500 5491\n",
      "5505 5496\n",
      "5510 5501\n",
      "5515 5506\n",
      "5520 5511\n",
      "5525 5516\n",
      "5530 5521\n",
      "5535 5526\n",
      "5540 5531\n",
      "5545 5536\n",
      "5550 5541\n",
      "5555 5546\n",
      "5560 5551\n",
      "5565 5556\n",
      "5570 5561\n",
      "5575 5566\n",
      "5580 5571\n",
      "5585 5576\n",
      "5590 5581\n",
      "5595 5586\n",
      "5600 5591\n",
      "5605 5596\n",
      "5610 5601\n",
      "5615 5606\n",
      "5620 5611\n",
      "5625 5616\n",
      "5630 5621\n",
      "5635 5626\n",
      "5640 5631\n",
      "5645 5636\n",
      "5650 5641\n",
      "5655 5646\n",
      "5660 5651\n",
      "5665 5656\n",
      "5670 5661\n",
      "5675 5666\n",
      "5680 5671\n",
      "5685 5676\n",
      "5690 5681\n",
      "5695 5686\n",
      "5700 5691\n",
      "5705 5696\n",
      "5710 5701\n",
      "5715 5706\n",
      "5720 5711\n",
      "5725 5716\n",
      "5730 5721\n",
      "5735 5726\n",
      "5740 5731\n",
      "5745 5736\n",
      "5750 5741\n",
      "5755 5746\n",
      "5760 5751\n",
      "5765 5756\n",
      "5770 5761\n",
      "5775 5766\n",
      "5780 5771\n",
      "5785 5776\n",
      "5790 5781\n",
      "5795 5786\n",
      "5800 5791\n",
      "5805 5796\n",
      "5810 5801\n",
      "5815 5806\n",
      "5820 5811\n",
      "5825 5816\n",
      "5830 5821\n",
      "5835 5826\n",
      "5840 5831\n",
      "5845 5836\n",
      "5850 5841\n",
      "5855 5846\n",
      "5860 5851\n",
      "5865 5856\n",
      "5870 5861\n",
      "5875 5866\n",
      "5880 5871\n",
      "5885 5876\n",
      "5890 5881\n",
      "5895 5886\n",
      "5900 5891\n",
      "5905 5896\n",
      "5910 5901\n",
      "5915 5906\n",
      "5920 5911\n",
      "5925 5916\n",
      "5930 5921\n",
      "5935 5926\n",
      "5940 5931\n",
      "5945 5936\n",
      "5950 5941\n",
      "5955 5946\n",
      "5960 5951\n",
      "5965 5956\n",
      "5970 5961\n",
      "5975 5966\n",
      "5980 5971\n",
      "5985 5976\n",
      "5990 5981\n",
      "5995 5986\n",
      "6000 5991\n",
      "6005 5996\n",
      "6010 6001\n",
      "6015 6006\n",
      "6020 6011\n",
      "6025 6016\n",
      "6030 6021\n",
      "6035 6026\n",
      "6040 6031\n",
      "6045 6036\n",
      "6050 6041\n",
      "6055 6046\n",
      "6060 6051\n",
      "6065 6056\n",
      "6070 6061\n",
      "6075 6066\n",
      "6080 6071\n",
      "6085 6076\n",
      "6090 6081\n",
      "6095 6086\n",
      "6100 6091\n",
      "6105 6096\n",
      "6110 6101\n",
      "6115 6106\n",
      "6120 6111\n",
      "6125 6116\n",
      "6130 6121\n",
      "6135 6126\n",
      "6140 6131\n",
      "6145 6136\n",
      "6150 6141\n",
      "6155 6146\n",
      "6160 6151\n",
      "6165 6156\n",
      "6170 6161\n",
      "6175 6166\n",
      "6180 6171\n",
      "6185 6176\n",
      "6190 6181\n",
      "6195 6186\n",
      "6200 6191\n",
      "6205 6196\n",
      "6210 6201\n",
      "6215 6206\n",
      "6220 6211\n",
      "6225 6216\n",
      "6230 6221\n",
      "6235 6226\n",
      "6240 6231\n",
      "6245 6236\n",
      "6250 6241\n",
      "6255 6246\n",
      "6260 6251\n",
      "6265 6256\n",
      "6270 6261\n",
      "6275 6266\n",
      "6280 6271\n",
      "6285 6276\n",
      "6290 6281\n",
      "6295 6286\n",
      "6300 6291\n",
      "6305 6296\n",
      "6310 6301\n",
      "6315 6306\n",
      "6320 6311\n",
      "6325 6316\n",
      "6330 6321\n",
      "6335 6326\n",
      "6340 6331\n",
      "6345 6336\n",
      "6350 6341\n",
      "6355 6346\n",
      "6360 6351\n",
      "6365 6356\n",
      "6370 6361\n",
      "6375 6366\n",
      "6380 6371\n",
      "6385 6376\n",
      "6390 6381\n",
      "6395 6386\n",
      "6400 6391\n",
      "6405 6396\n",
      "6410 6401\n",
      "6415 6406\n",
      "6420 6411\n",
      "6425 6416\n",
      "6430 6421\n",
      "6435 6426\n",
      "6440 6431\n",
      "6445 6436\n",
      "6450 6441\n",
      "6455 6446\n",
      "6460 6451\n",
      "6465 6456\n",
      "6470 6461\n",
      "6475 6466\n",
      "6480 6471\n",
      "6485 6476\n",
      "6490 6481\n",
      "6495 6486\n",
      "6500 6491\n",
      "6505 6496\n",
      "6510 6501\n",
      "6515 6506\n",
      "6520 6511\n",
      "6525 6516\n",
      "6530 6521\n",
      "6535 6526\n",
      "6540 6531\n",
      "6545 6536\n",
      "6550 6541\n",
      "6555 6546\n",
      "6560 6551\n",
      "6565 6556\n",
      "6570 6561\n",
      "6575 6566\n",
      "6580 6571\n",
      "6585 6576\n",
      "6590 6581\n",
      "6595 6586\n",
      "6600 6591\n",
      "6605 6596\n",
      "6610 6601\n",
      "6615 6606\n",
      "6620 6611\n"
     ]
    }
   ],
   "source": [
    "asdf, a = get_backtest_df_abhi(\"ALL\", \"INFY.NS\", \n",
    "                       0.8, 0.5, \n",
    "                       train_start_date, \n",
    "                       datetime.datetime(2022,5,24), \n",
    "                       5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asdf = asdf.iloc[4: : 5]\n",
    "# asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.296099160393432"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf['profit_percentage_per_trade'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>buy_sell_signal_from_model</th>\n",
       "      <th>profit_long</th>\n",
       "      <th>buy_price</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>profit_percentage_per_trade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-01-24</th>\n",
       "      <td>0.770507</td>\n",
       "      <td>0.752929</td>\n",
       "      <td>0.773925</td>\n",
       "      <td>0.770507</td>\n",
       "      <td>409600.0</td>\n",
       "      <td>0.526763</td>\n",
       "      <td>37.946161</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>1.781186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-25</th>\n",
       "      <td>0.770996</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.770996</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>102400.0</td>\n",
       "      <td>0.524760</td>\n",
       "      <td>36.648004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.042968</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.810546</td>\n",
       "      <td>5.597864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-26</th>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524760</td>\n",
       "      <td>36.648004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.076563</td>\n",
       "      <td>0.763183</td>\n",
       "      <td>0.839746</td>\n",
       "      <td>10.032063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-29</th>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.763183</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.763183</td>\n",
       "      <td>102400.0</td>\n",
       "      <td>0.521756</td>\n",
       "      <td>34.588757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.772460</td>\n",
       "      <td>0.839843</td>\n",
       "      <td>8.723169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-02-02</th>\n",
       "      <td>0.839843</td>\n",
       "      <td>0.810546</td>\n",
       "      <td>0.810546</td>\n",
       "      <td>0.839746</td>\n",
       "      <td>2355200.0</td>\n",
       "      <td>0.574099</td>\n",
       "      <td>70.641245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.082715</td>\n",
       "      <td>0.839843</td>\n",
       "      <td>0.922558</td>\n",
       "      <td>9.848869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-21</th>\n",
       "      <td>1808.000000</td>\n",
       "      <td>1776.000000</td>\n",
       "      <td>1795.000000</td>\n",
       "      <td>1785.699951</td>\n",
       "      <td>8252758.0</td>\n",
       "      <td>1785.699951</td>\n",
       "      <td>40.070210</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.600098</td>\n",
       "      <td>1736.800049</td>\n",
       "      <td>1736.199951</td>\n",
       "      <td>-0.034552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-24</th>\n",
       "      <td>1768.650024</td>\n",
       "      <td>1728.000000</td>\n",
       "      <td>1765.000000</td>\n",
       "      <td>1736.800049</td>\n",
       "      <td>7116712.0</td>\n",
       "      <td>1736.800049</td>\n",
       "      <td>34.579623</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.900024</td>\n",
       "      <td>1722.150024</td>\n",
       "      <td>1772.050049</td>\n",
       "      <td>2.897542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-28</th>\n",
       "      <td>1728.050049</td>\n",
       "      <td>1665.000000</td>\n",
       "      <td>1682.199951</td>\n",
       "      <td>1715.599976</td>\n",
       "      <td>15076542.0</td>\n",
       "      <td>1715.599976</td>\n",
       "      <td>45.973248</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.399902</td>\n",
       "      <td>1702.800049</td>\n",
       "      <td>1778.199951</td>\n",
       "      <td>4.427995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-02</th>\n",
       "      <td>1714.400024</td>\n",
       "      <td>1691.000000</td>\n",
       "      <td>1710.000000</td>\n",
       "      <td>1702.800049</td>\n",
       "      <td>6771489.0</td>\n",
       "      <td>1702.800049</td>\n",
       "      <td>44.221658</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.500000</td>\n",
       "      <td>1720.849976</td>\n",
       "      <td>1813.349976</td>\n",
       "      <td>5.375251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-04</th>\n",
       "      <td>1736.000000</td>\n",
       "      <td>1681.000000</td>\n",
       "      <td>1695.250000</td>\n",
       "      <td>1723.300049</td>\n",
       "      <td>9074499.0</td>\n",
       "      <td>1723.300049</td>\n",
       "      <td>47.690715</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82.150024</td>\n",
       "      <td>1739.849976</td>\n",
       "      <td>1822.000000</td>\n",
       "      <td>4.721673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   High          Low         Open        Close      Volume  \\\n",
       "Date                                                                         \n",
       "1996-01-24     0.770507     0.752929     0.773925     0.770507    409600.0   \n",
       "1996-01-25     0.770996     0.767578     0.770996     0.767578    102400.0   \n",
       "1996-01-26     0.767578     0.767578     0.767578     0.767578         0.0   \n",
       "1996-01-29     0.765625     0.763183     0.765625     0.763183    102400.0   \n",
       "1996-02-02     0.839843     0.810546     0.810546     0.839746   2355200.0   \n",
       "...                 ...          ...          ...          ...         ...   \n",
       "2022-01-21  1808.000000  1776.000000  1795.000000  1785.699951   8252758.0   \n",
       "2022-01-24  1768.650024  1728.000000  1765.000000  1736.800049   7116712.0   \n",
       "2022-02-28  1728.050049  1665.000000  1682.199951  1715.599976  15076542.0   \n",
       "2022-03-02  1714.400024  1691.000000  1710.000000  1702.800049   6771489.0   \n",
       "2022-03-04  1736.000000  1681.000000  1695.250000  1723.300049   9074499.0   \n",
       "\n",
       "              Adj Close     RSI_14  buy_sell_signal_from_model  profit_long  \\\n",
       "Date                                                                          \n",
       "1996-01-24     0.526763  37.946161                         1.0     0.013672   \n",
       "1996-01-25     0.524760  36.648004                         1.0     0.042968   \n",
       "1996-01-26     0.524760  36.648004                         1.0     0.076563   \n",
       "1996-01-29     0.521756  34.588757                         1.0     0.067383   \n",
       "1996-02-02     0.574099  70.641245                         1.0     0.082715   \n",
       "...                 ...        ...                         ...          ...   \n",
       "2022-01-21  1785.699951  40.070210                         1.0    -0.600098   \n",
       "2022-01-24  1736.800049  34.579623                         1.0    49.900024   \n",
       "2022-02-28  1715.599976  45.973248                         1.0    75.399902   \n",
       "2022-03-02  1702.800049  44.221658                         1.0    92.500000   \n",
       "2022-03-04  1723.300049  47.690715                         1.0    82.150024   \n",
       "\n",
       "              buy_price   sell_price  profit_percentage_per_trade  \n",
       "Date                                                               \n",
       "1996-01-24     0.767578     0.781250                     1.781186  \n",
       "1996-01-25     0.767578     0.810546                     5.597864  \n",
       "1996-01-26     0.763183     0.839746                    10.032063  \n",
       "1996-01-29     0.772460     0.839843                     8.723169  \n",
       "1996-02-02     0.839843     0.922558                     9.848869  \n",
       "...                 ...          ...                          ...  \n",
       "2022-01-21  1736.800049  1736.199951                    -0.034552  \n",
       "2022-01-24  1722.150024  1772.050049                     2.897542  \n",
       "2022-02-28  1702.800049  1778.199951                     4.427995  \n",
       "2022-03-02  1720.849976  1813.349976                     5.375251  \n",
       "2022-03-04  1739.849976  1822.000000                     4.721673  \n",
       "\n",
       "[1030 rows x 12 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf[asdf['buy_sell_signal_from_model'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>buy_sell_signal_from_model</th>\n",
       "      <th>profit_long</th>\n",
       "      <th>buy_price</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>profit_percentage_per_trade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-01-24</th>\n",
       "      <td>0.770507</td>\n",
       "      <td>0.752929</td>\n",
       "      <td>0.773925</td>\n",
       "      <td>0.770507</td>\n",
       "      <td>409600.0</td>\n",
       "      <td>0.526763</td>\n",
       "      <td>37.946161</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>1.781186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-25</th>\n",
       "      <td>0.770996</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.770996</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>102400.0</td>\n",
       "      <td>0.524760</td>\n",
       "      <td>36.648004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.042968</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.810546</td>\n",
       "      <td>5.597864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-26</th>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524760</td>\n",
       "      <td>36.648004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.076563</td>\n",
       "      <td>0.763183</td>\n",
       "      <td>0.839746</td>\n",
       "      <td>10.032063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-29</th>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.763183</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.763183</td>\n",
       "      <td>102400.0</td>\n",
       "      <td>0.521756</td>\n",
       "      <td>34.588757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.772460</td>\n",
       "      <td>0.839843</td>\n",
       "      <td>8.723169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-02-02</th>\n",
       "      <td>0.839843</td>\n",
       "      <td>0.810546</td>\n",
       "      <td>0.810546</td>\n",
       "      <td>0.839746</td>\n",
       "      <td>2355200.0</td>\n",
       "      <td>0.574099</td>\n",
       "      <td>70.641245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.082715</td>\n",
       "      <td>0.839843</td>\n",
       "      <td>0.922558</td>\n",
       "      <td>9.848869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-21</th>\n",
       "      <td>1808.000000</td>\n",
       "      <td>1776.000000</td>\n",
       "      <td>1795.000000</td>\n",
       "      <td>1785.699951</td>\n",
       "      <td>8252758.0</td>\n",
       "      <td>1785.699951</td>\n",
       "      <td>40.070210</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.600098</td>\n",
       "      <td>1736.800049</td>\n",
       "      <td>1736.199951</td>\n",
       "      <td>-0.034552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-24</th>\n",
       "      <td>1768.650024</td>\n",
       "      <td>1728.000000</td>\n",
       "      <td>1765.000000</td>\n",
       "      <td>1736.800049</td>\n",
       "      <td>7116712.0</td>\n",
       "      <td>1736.800049</td>\n",
       "      <td>34.579623</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.900024</td>\n",
       "      <td>1722.150024</td>\n",
       "      <td>1772.050049</td>\n",
       "      <td>2.897542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-28</th>\n",
       "      <td>1728.050049</td>\n",
       "      <td>1665.000000</td>\n",
       "      <td>1682.199951</td>\n",
       "      <td>1715.599976</td>\n",
       "      <td>15076542.0</td>\n",
       "      <td>1715.599976</td>\n",
       "      <td>45.973248</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.399902</td>\n",
       "      <td>1702.800049</td>\n",
       "      <td>1778.199951</td>\n",
       "      <td>4.427995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-02</th>\n",
       "      <td>1714.400024</td>\n",
       "      <td>1691.000000</td>\n",
       "      <td>1710.000000</td>\n",
       "      <td>1702.800049</td>\n",
       "      <td>6771489.0</td>\n",
       "      <td>1702.800049</td>\n",
       "      <td>44.221658</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.500000</td>\n",
       "      <td>1720.849976</td>\n",
       "      <td>1813.349976</td>\n",
       "      <td>5.375251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-04</th>\n",
       "      <td>1736.000000</td>\n",
       "      <td>1681.000000</td>\n",
       "      <td>1695.250000</td>\n",
       "      <td>1723.300049</td>\n",
       "      <td>9074499.0</td>\n",
       "      <td>1723.300049</td>\n",
       "      <td>47.690715</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82.150024</td>\n",
       "      <td>1739.849976</td>\n",
       "      <td>1822.000000</td>\n",
       "      <td>4.721673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   High          Low         Open        Close      Volume  \\\n",
       "Date                                                                         \n",
       "1996-01-24     0.770507     0.752929     0.773925     0.770507    409600.0   \n",
       "1996-01-25     0.770996     0.767578     0.770996     0.767578    102400.0   \n",
       "1996-01-26     0.767578     0.767578     0.767578     0.767578         0.0   \n",
       "1996-01-29     0.765625     0.763183     0.765625     0.763183    102400.0   \n",
       "1996-02-02     0.839843     0.810546     0.810546     0.839746   2355200.0   \n",
       "...                 ...          ...          ...          ...         ...   \n",
       "2022-01-21  1808.000000  1776.000000  1795.000000  1785.699951   8252758.0   \n",
       "2022-01-24  1768.650024  1728.000000  1765.000000  1736.800049   7116712.0   \n",
       "2022-02-28  1728.050049  1665.000000  1682.199951  1715.599976  15076542.0   \n",
       "2022-03-02  1714.400024  1691.000000  1710.000000  1702.800049   6771489.0   \n",
       "2022-03-04  1736.000000  1681.000000  1695.250000  1723.300049   9074499.0   \n",
       "\n",
       "              Adj Close     RSI_14  buy_sell_signal_from_model  profit_long  \\\n",
       "Date                                                                          \n",
       "1996-01-24     0.526763  37.946161                         1.0     0.013672   \n",
       "1996-01-25     0.524760  36.648004                         1.0     0.042968   \n",
       "1996-01-26     0.524760  36.648004                         1.0     0.076563   \n",
       "1996-01-29     0.521756  34.588757                         1.0     0.067383   \n",
       "1996-02-02     0.574099  70.641245                         1.0     0.082715   \n",
       "...                 ...        ...                         ...          ...   \n",
       "2022-01-21  1785.699951  40.070210                         1.0    -0.600098   \n",
       "2022-01-24  1736.800049  34.579623                         1.0    49.900024   \n",
       "2022-02-28  1715.599976  45.973248                         1.0    75.399902   \n",
       "2022-03-02  1702.800049  44.221658                         1.0    92.500000   \n",
       "2022-03-04  1723.300049  47.690715                         1.0    82.150024   \n",
       "\n",
       "              buy_price   sell_price  profit_percentage_per_trade  \n",
       "Date                                                               \n",
       "1996-01-24     0.767578     0.781250                     1.781186  \n",
       "1996-01-25     0.767578     0.810546                     5.597864  \n",
       "1996-01-26     0.763183     0.839746                    10.032063  \n",
       "1996-01-29     0.772460     0.839843                     8.723169  \n",
       "1996-02-02     0.839843     0.922558                     9.848869  \n",
       "...                 ...          ...                          ...  \n",
       "2022-01-21  1736.800049  1736.199951                    -0.034552  \n",
       "2022-01-24  1722.150024  1772.050049                     2.897542  \n",
       "2022-02-28  1702.800049  1778.199951                     4.427995  \n",
       "2022-03-02  1720.849976  1813.349976                     5.375251  \n",
       "2022-03-04  1739.849976  1822.000000                     4.721673  \n",
       "\n",
       "[1030 rows x 12 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6641"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_length_of_data': 1030,\n",
       " 'length_of_high_confidence_data': 1030,\n",
       " 'mean_profit_per_trade': 4.296099160393432,\n",
       " 'layman_profit': 223557.9418427934,\n",
       " 'returns': 1.8597751064125804e+20}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_backtest_stats(asdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rsi0</th>\n",
       "      <th>rsi1</th>\n",
       "      <th>rsi2</th>\n",
       "      <th>rsi3</th>\n",
       "      <th>rsi4</th>\n",
       "      <th>stochk0</th>\n",
       "      <th>stochk1</th>\n",
       "      <th>stochk2</th>\n",
       "      <th>stochk3</th>\n",
       "      <th>stochk4</th>\n",
       "      <th>...</th>\n",
       "      <th>stochd1</th>\n",
       "      <th>stochd2</th>\n",
       "      <th>stochd3</th>\n",
       "      <th>stochd4</th>\n",
       "      <th>close0</th>\n",
       "      <th>close1</th>\n",
       "      <th>close2</th>\n",
       "      <th>close3</th>\n",
       "      <th>close4</th>\n",
       "      <th>long_or_short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52.376308</td>\n",
       "      <td>56.819433</td>\n",
       "      <td>55.954525</td>\n",
       "      <td>52.110399</td>\n",
       "      <td>52.500873</td>\n",
       "      <td>50.436016</td>\n",
       "      <td>44.324026</td>\n",
       "      <td>38.320328</td>\n",
       "      <td>27.019217</td>\n",
       "      <td>13.802802</td>\n",
       "      <td>...</td>\n",
       "      <td>52.843555</td>\n",
       "      <td>44.360123</td>\n",
       "      <td>36.554524</td>\n",
       "      <td>26.380782</td>\n",
       "      <td>0.135045</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.141183</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.819433</td>\n",
       "      <td>55.954525</td>\n",
       "      <td>52.110399</td>\n",
       "      <td>52.500873</td>\n",
       "      <td>54.498568</td>\n",
       "      <td>44.324026</td>\n",
       "      <td>38.320328</td>\n",
       "      <td>27.019217</td>\n",
       "      <td>13.802802</td>\n",
       "      <td>9.219494</td>\n",
       "      <td>...</td>\n",
       "      <td>44.360123</td>\n",
       "      <td>36.554524</td>\n",
       "      <td>26.380782</td>\n",
       "      <td>16.680504</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.141183</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.139509</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55.954525</td>\n",
       "      <td>52.110399</td>\n",
       "      <td>52.500873</td>\n",
       "      <td>54.498568</td>\n",
       "      <td>53.528777</td>\n",
       "      <td>38.320328</td>\n",
       "      <td>27.019217</td>\n",
       "      <td>13.802802</td>\n",
       "      <td>9.219494</td>\n",
       "      <td>12.056266</td>\n",
       "      <td>...</td>\n",
       "      <td>36.554524</td>\n",
       "      <td>26.380782</td>\n",
       "      <td>16.680504</td>\n",
       "      <td>11.692854</td>\n",
       "      <td>0.141183</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.139509</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52.110399</td>\n",
       "      <td>52.500873</td>\n",
       "      <td>54.498568</td>\n",
       "      <td>53.528777</td>\n",
       "      <td>59.368572</td>\n",
       "      <td>27.019217</td>\n",
       "      <td>13.802802</td>\n",
       "      <td>9.219494</td>\n",
       "      <td>12.056266</td>\n",
       "      <td>27.427904</td>\n",
       "      <td>...</td>\n",
       "      <td>26.380782</td>\n",
       "      <td>16.680504</td>\n",
       "      <td>11.692854</td>\n",
       "      <td>16.234555</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.139509</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.500873</td>\n",
       "      <td>54.498568</td>\n",
       "      <td>53.528777</td>\n",
       "      <td>59.368572</td>\n",
       "      <td>55.372003</td>\n",
       "      <td>13.802802</td>\n",
       "      <td>9.219494</td>\n",
       "      <td>12.056266</td>\n",
       "      <td>27.427904</td>\n",
       "      <td>33.790781</td>\n",
       "      <td>...</td>\n",
       "      <td>16.680504</td>\n",
       "      <td>11.692854</td>\n",
       "      <td>16.234555</td>\n",
       "      <td>24.424984</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.139509</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10414</th>\n",
       "      <td>34.680043</td>\n",
       "      <td>46.628192</td>\n",
       "      <td>39.959143</td>\n",
       "      <td>40.433987</td>\n",
       "      <td>42.821693</td>\n",
       "      <td>9.789935</td>\n",
       "      <td>19.152474</td>\n",
       "      <td>23.679995</td>\n",
       "      <td>30.035360</td>\n",
       "      <td>24.582989</td>\n",
       "      <td>...</td>\n",
       "      <td>12.365044</td>\n",
       "      <td>17.540801</td>\n",
       "      <td>24.289276</td>\n",
       "      <td>26.099448</td>\n",
       "      <td>156.570007</td>\n",
       "      <td>163.639999</td>\n",
       "      <td>157.649994</td>\n",
       "      <td>157.960007</td>\n",
       "      <td>159.479996</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10415</th>\n",
       "      <td>46.628192</td>\n",
       "      <td>39.959143</td>\n",
       "      <td>40.433987</td>\n",
       "      <td>42.821693</td>\n",
       "      <td>51.778367</td>\n",
       "      <td>19.152474</td>\n",
       "      <td>23.679995</td>\n",
       "      <td>30.035360</td>\n",
       "      <td>24.582989</td>\n",
       "      <td>43.172693</td>\n",
       "      <td>...</td>\n",
       "      <td>17.540801</td>\n",
       "      <td>24.289276</td>\n",
       "      <td>26.099448</td>\n",
       "      <td>32.597014</td>\n",
       "      <td>163.639999</td>\n",
       "      <td>157.649994</td>\n",
       "      <td>157.960007</td>\n",
       "      <td>159.479996</td>\n",
       "      <td>166.020004</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10416</th>\n",
       "      <td>39.959143</td>\n",
       "      <td>40.433987</td>\n",
       "      <td>42.821693</td>\n",
       "      <td>51.778367</td>\n",
       "      <td>41.804073</td>\n",
       "      <td>23.679995</td>\n",
       "      <td>30.035360</td>\n",
       "      <td>24.582989</td>\n",
       "      <td>43.172693</td>\n",
       "      <td>41.000362</td>\n",
       "      <td>...</td>\n",
       "      <td>24.289276</td>\n",
       "      <td>26.099448</td>\n",
       "      <td>32.597014</td>\n",
       "      <td>36.252015</td>\n",
       "      <td>157.649994</td>\n",
       "      <td>157.960007</td>\n",
       "      <td>159.479996</td>\n",
       "      <td>166.020004</td>\n",
       "      <td>156.770004</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10417</th>\n",
       "      <td>40.433987</td>\n",
       "      <td>42.821693</td>\n",
       "      <td>51.778367</td>\n",
       "      <td>41.804073</td>\n",
       "      <td>42.462179</td>\n",
       "      <td>30.035360</td>\n",
       "      <td>24.582989</td>\n",
       "      <td>43.172693</td>\n",
       "      <td>41.000362</td>\n",
       "      <td>36.984302</td>\n",
       "      <td>...</td>\n",
       "      <td>26.099448</td>\n",
       "      <td>32.597014</td>\n",
       "      <td>36.252015</td>\n",
       "      <td>40.385785</td>\n",
       "      <td>157.960007</td>\n",
       "      <td>159.479996</td>\n",
       "      <td>166.020004</td>\n",
       "      <td>156.770004</td>\n",
       "      <td>157.279999</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10418</th>\n",
       "      <td>42.821693</td>\n",
       "      <td>51.778367</td>\n",
       "      <td>41.804073</td>\n",
       "      <td>42.462179</td>\n",
       "      <td>37.755899</td>\n",
       "      <td>24.582989</td>\n",
       "      <td>43.172693</td>\n",
       "      <td>41.000362</td>\n",
       "      <td>36.984302</td>\n",
       "      <td>14.657468</td>\n",
       "      <td>...</td>\n",
       "      <td>32.597014</td>\n",
       "      <td>36.252015</td>\n",
       "      <td>40.385785</td>\n",
       "      <td>30.880711</td>\n",
       "      <td>159.479996</td>\n",
       "      <td>166.020004</td>\n",
       "      <td>156.770004</td>\n",
       "      <td>157.279999</td>\n",
       "      <td>152.059998</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10419 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            rsi0       rsi1       rsi2       rsi3       rsi4    stochk0  \\\n",
       "0      52.376308  56.819433  55.954525  52.110399  52.500873  50.436016   \n",
       "1      56.819433  55.954525  52.110399  52.500873  54.498568  44.324026   \n",
       "2      55.954525  52.110399  52.500873  54.498568  53.528777  38.320328   \n",
       "3      52.110399  52.500873  54.498568  53.528777  59.368572  27.019217   \n",
       "4      52.500873  54.498568  53.528777  59.368572  55.372003  13.802802   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "10414  34.680043  46.628192  39.959143  40.433987  42.821693   9.789935   \n",
       "10415  46.628192  39.959143  40.433987  42.821693  51.778367  19.152474   \n",
       "10416  39.959143  40.433987  42.821693  51.778367  41.804073  23.679995   \n",
       "10417  40.433987  42.821693  51.778367  41.804073  42.462179  30.035360   \n",
       "10418  42.821693  51.778367  41.804073  42.462179  37.755899  24.582989   \n",
       "\n",
       "         stochk1    stochk2    stochk3    stochk4  ...    stochd1    stochd2  \\\n",
       "0      44.324026  38.320328  27.019217  13.802802  ...  52.843555  44.360123   \n",
       "1      38.320328  27.019217  13.802802   9.219494  ...  44.360123  36.554524   \n",
       "2      27.019217  13.802802   9.219494  12.056266  ...  36.554524  26.380782   \n",
       "3      13.802802   9.219494  12.056266  27.427904  ...  26.380782  16.680504   \n",
       "4       9.219494  12.056266  27.427904  33.790781  ...  16.680504  11.692854   \n",
       "...          ...        ...        ...        ...  ...        ...        ...   \n",
       "10414  19.152474  23.679995  30.035360  24.582989  ...  12.365044  17.540801   \n",
       "10415  23.679995  30.035360  24.582989  43.172693  ...  17.540801  24.289276   \n",
       "10416  30.035360  24.582989  43.172693  41.000362  ...  24.289276  26.099448   \n",
       "10417  24.582989  43.172693  41.000362  36.984302  ...  26.099448  32.597014   \n",
       "10418  43.172693  41.000362  36.984302  14.657468  ...  32.597014  36.252015   \n",
       "\n",
       "         stochd3    stochd4      close0      close1      close2      close3  \\\n",
       "0      36.554524  26.380782    0.135045    0.142299    0.141183    0.136161   \n",
       "1      26.380782  16.680504    0.142299    0.141183    0.136161    0.136719   \n",
       "2      16.680504  11.692854    0.141183    0.136161    0.136719    0.139509   \n",
       "3      11.692854  16.234555    0.136161    0.136719    0.139509    0.138393   \n",
       "4      16.234555  24.424984    0.136719    0.139509    0.138393    0.146763   \n",
       "...          ...        ...         ...         ...         ...         ...   \n",
       "10414  24.289276  26.099448  156.570007  163.639999  157.649994  157.960007   \n",
       "10415  26.099448  32.597014  163.639999  157.649994  157.960007  159.479996   \n",
       "10416  32.597014  36.252015  157.649994  157.960007  159.479996  166.020004   \n",
       "10417  36.252015  40.385785  157.960007  159.479996  166.020004  156.770004   \n",
       "10418  40.385785  30.880711  159.479996  166.020004  156.770004  157.279999   \n",
       "\n",
       "           close4  long_or_short  \n",
       "0        0.136719            1.0  \n",
       "1        0.139509            1.0  \n",
       "2        0.138393            0.0  \n",
       "3        0.146763            1.0  \n",
       "4        0.142299            0.0  \n",
       "...           ...            ...  \n",
       "10414  159.479996            NaN  \n",
       "10415  166.020004            NaN  \n",
       "10416  156.770004            NaN  \n",
       "10417  157.279999            NaN  \n",
       "10418  152.059998            NaN  \n",
       "\n",
       "[10419 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_datareader_data = data.DataReader('AAPL', 'yahoo', train_start_date, test_end_date)\n",
    "pd_datareader_data['RSI_14'] = RSI(pd_datareader_data['Close'], timeperiod=14)\n",
    "    \n",
    "\n",
    "\n",
    "backtest_company_price_data = data_cleaning_for_all_indicators_abhi(pd_datareader_data, 5)\n",
    "backtest_company_price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rsi0</th>\n",
       "      <th>rsi1</th>\n",
       "      <th>rsi2</th>\n",
       "      <th>rsi3</th>\n",
       "      <th>rsi4</th>\n",
       "      <th>stochk0</th>\n",
       "      <th>stochk1</th>\n",
       "      <th>stochk2</th>\n",
       "      <th>stochk3</th>\n",
       "      <th>stochk4</th>\n",
       "      <th>...</th>\n",
       "      <th>stochd1</th>\n",
       "      <th>stochd2</th>\n",
       "      <th>stochd3</th>\n",
       "      <th>stochd4</th>\n",
       "      <th>close0</th>\n",
       "      <th>close1</th>\n",
       "      <th>close2</th>\n",
       "      <th>close3</th>\n",
       "      <th>close4</th>\n",
       "      <th>long_or_short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52.376308</td>\n",
       "      <td>56.819433</td>\n",
       "      <td>55.954525</td>\n",
       "      <td>52.110399</td>\n",
       "      <td>52.500873</td>\n",
       "      <td>5.043602e+01</td>\n",
       "      <td>4.432403e+01</td>\n",
       "      <td>3.832033e+01</td>\n",
       "      <td>2.701922e+01</td>\n",
       "      <td>1.380280e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>52.843555</td>\n",
       "      <td>44.360123</td>\n",
       "      <td>36.554524</td>\n",
       "      <td>26.380782</td>\n",
       "      <td>0.135045</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.141183</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.819433</td>\n",
       "      <td>55.954525</td>\n",
       "      <td>52.110399</td>\n",
       "      <td>52.500873</td>\n",
       "      <td>54.498568</td>\n",
       "      <td>4.432403e+01</td>\n",
       "      <td>3.832033e+01</td>\n",
       "      <td>2.701922e+01</td>\n",
       "      <td>1.380280e+01</td>\n",
       "      <td>9.219494e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>44.360123</td>\n",
       "      <td>36.554524</td>\n",
       "      <td>26.380782</td>\n",
       "      <td>16.680504</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.141183</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.139509</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55.954525</td>\n",
       "      <td>52.110399</td>\n",
       "      <td>52.500873</td>\n",
       "      <td>54.498568</td>\n",
       "      <td>53.528777</td>\n",
       "      <td>3.832033e+01</td>\n",
       "      <td>2.701922e+01</td>\n",
       "      <td>1.380280e+01</td>\n",
       "      <td>9.219494e+00</td>\n",
       "      <td>1.205627e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>36.554524</td>\n",
       "      <td>26.380782</td>\n",
       "      <td>16.680504</td>\n",
       "      <td>11.692854</td>\n",
       "      <td>0.141183</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.139509</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52.110399</td>\n",
       "      <td>52.500873</td>\n",
       "      <td>54.498568</td>\n",
       "      <td>53.528777</td>\n",
       "      <td>59.368572</td>\n",
       "      <td>2.701922e+01</td>\n",
       "      <td>1.380280e+01</td>\n",
       "      <td>9.219494e+00</td>\n",
       "      <td>1.205627e+01</td>\n",
       "      <td>2.742790e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>26.380782</td>\n",
       "      <td>16.680504</td>\n",
       "      <td>11.692854</td>\n",
       "      <td>16.234555</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.139509</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.500873</td>\n",
       "      <td>54.498568</td>\n",
       "      <td>53.528777</td>\n",
       "      <td>59.368572</td>\n",
       "      <td>55.372003</td>\n",
       "      <td>1.380280e+01</td>\n",
       "      <td>9.219494e+00</td>\n",
       "      <td>1.205627e+01</td>\n",
       "      <td>2.742790e+01</td>\n",
       "      <td>3.379078e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>16.680504</td>\n",
       "      <td>11.692854</td>\n",
       "      <td>16.234555</td>\n",
       "      <td>24.424984</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.139509</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>54.498568</td>\n",
       "      <td>53.528777</td>\n",
       "      <td>59.368572</td>\n",
       "      <td>55.372003</td>\n",
       "      <td>57.306456</td>\n",
       "      <td>9.219494e+00</td>\n",
       "      <td>1.205627e+01</td>\n",
       "      <td>2.742790e+01</td>\n",
       "      <td>3.379078e+01</td>\n",
       "      <td>4.620146e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>11.692854</td>\n",
       "      <td>16.234555</td>\n",
       "      <td>24.424984</td>\n",
       "      <td>35.806715</td>\n",
       "      <td>0.139509</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.145089</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>53.528777</td>\n",
       "      <td>59.368572</td>\n",
       "      <td>55.372003</td>\n",
       "      <td>57.306456</td>\n",
       "      <td>58.469647</td>\n",
       "      <td>1.205627e+01</td>\n",
       "      <td>2.742790e+01</td>\n",
       "      <td>3.379078e+01</td>\n",
       "      <td>4.620146e+01</td>\n",
       "      <td>5.283868e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>16.234555</td>\n",
       "      <td>24.424984</td>\n",
       "      <td>35.806715</td>\n",
       "      <td>44.276974</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.145089</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>59.368572</td>\n",
       "      <td>55.372003</td>\n",
       "      <td>57.306456</td>\n",
       "      <td>58.469647</td>\n",
       "      <td>57.903330</td>\n",
       "      <td>2.742790e+01</td>\n",
       "      <td>3.379078e+01</td>\n",
       "      <td>4.620146e+01</td>\n",
       "      <td>5.283868e+01</td>\n",
       "      <td>6.978774e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>24.424984</td>\n",
       "      <td>35.806715</td>\n",
       "      <td>44.276974</td>\n",
       "      <td>56.275960</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.145089</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.146205</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>55.372003</td>\n",
       "      <td>57.306456</td>\n",
       "      <td>58.469647</td>\n",
       "      <td>57.903330</td>\n",
       "      <td>55.584215</td>\n",
       "      <td>3.379078e+01</td>\n",
       "      <td>4.620146e+01</td>\n",
       "      <td>5.283868e+01</td>\n",
       "      <td>6.978774e+01</td>\n",
       "      <td>7.631029e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>35.806715</td>\n",
       "      <td>44.276974</td>\n",
       "      <td>56.275960</td>\n",
       "      <td>66.312236</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.145089</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.146205</td>\n",
       "      <td>0.143973</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57.306456</td>\n",
       "      <td>58.469647</td>\n",
       "      <td>57.903330</td>\n",
       "      <td>55.584215</td>\n",
       "      <td>54.410790</td>\n",
       "      <td>4.620146e+01</td>\n",
       "      <td>5.283868e+01</td>\n",
       "      <td>6.978774e+01</td>\n",
       "      <td>7.631029e+01</td>\n",
       "      <td>7.246369e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>44.276974</td>\n",
       "      <td>56.275960</td>\n",
       "      <td>66.312236</td>\n",
       "      <td>72.853906</td>\n",
       "      <td>0.145089</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.146205</td>\n",
       "      <td>0.143973</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>58.469647</td>\n",
       "      <td>57.903330</td>\n",
       "      <td>55.584215</td>\n",
       "      <td>54.410790</td>\n",
       "      <td>49.875176</td>\n",
       "      <td>5.283868e+01</td>\n",
       "      <td>6.978774e+01</td>\n",
       "      <td>7.631029e+01</td>\n",
       "      <td>7.246369e+01</td>\n",
       "      <td>4.982742e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>56.275960</td>\n",
       "      <td>66.312236</td>\n",
       "      <td>72.853906</td>\n",
       "      <td>66.200466</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.146205</td>\n",
       "      <td>0.143973</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>57.903330</td>\n",
       "      <td>55.584215</td>\n",
       "      <td>54.410790</td>\n",
       "      <td>49.875176</td>\n",
       "      <td>45.300202</td>\n",
       "      <td>6.978774e+01</td>\n",
       "      <td>7.631029e+01</td>\n",
       "      <td>7.246369e+01</td>\n",
       "      <td>4.982742e+01</td>\n",
       "      <td>2.663905e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>66.312236</td>\n",
       "      <td>72.853906</td>\n",
       "      <td>66.200466</td>\n",
       "      <td>49.643386</td>\n",
       "      <td>0.146205</td>\n",
       "      <td>0.143973</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.133371</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>55.584215</td>\n",
       "      <td>54.410790</td>\n",
       "      <td>49.875176</td>\n",
       "      <td>45.300202</td>\n",
       "      <td>39.642837</td>\n",
       "      <td>7.631029e+01</td>\n",
       "      <td>7.246369e+01</td>\n",
       "      <td>4.982742e+01</td>\n",
       "      <td>2.663905e+01</td>\n",
       "      <td>6.349204e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>72.853906</td>\n",
       "      <td>66.200466</td>\n",
       "      <td>49.643386</td>\n",
       "      <td>27.605223</td>\n",
       "      <td>0.143973</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.133371</td>\n",
       "      <td>0.126116</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>54.410790</td>\n",
       "      <td>49.875176</td>\n",
       "      <td>45.300202</td>\n",
       "      <td>39.642837</td>\n",
       "      <td>34.943807</td>\n",
       "      <td>7.246369e+01</td>\n",
       "      <td>4.982742e+01</td>\n",
       "      <td>2.663905e+01</td>\n",
       "      <td>6.349204e+00</td>\n",
       "      <td>3.434290e-14</td>\n",
       "      <td>...</td>\n",
       "      <td>66.200466</td>\n",
       "      <td>49.643386</td>\n",
       "      <td>27.605223</td>\n",
       "      <td>10.996083</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.133371</td>\n",
       "      <td>0.126116</td>\n",
       "      <td>0.118862</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>49.875176</td>\n",
       "      <td>45.300202</td>\n",
       "      <td>39.642837</td>\n",
       "      <td>34.943807</td>\n",
       "      <td>39.682098</td>\n",
       "      <td>4.982742e+01</td>\n",
       "      <td>2.663905e+01</td>\n",
       "      <td>6.349204e+00</td>\n",
       "      <td>3.434290e-14</td>\n",
       "      <td>5.128029e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>49.643386</td>\n",
       "      <td>27.605223</td>\n",
       "      <td>10.996083</td>\n",
       "      <td>3.825744</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.133371</td>\n",
       "      <td>0.126116</td>\n",
       "      <td>0.118862</td>\n",
       "      <td>0.123326</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>45.300202</td>\n",
       "      <td>39.642837</td>\n",
       "      <td>34.943807</td>\n",
       "      <td>39.682098</td>\n",
       "      <td>44.069116</td>\n",
       "      <td>2.663905e+01</td>\n",
       "      <td>6.349204e+00</td>\n",
       "      <td>3.434290e-14</td>\n",
       "      <td>5.128029e+00</td>\n",
       "      <td>1.538409e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>27.605223</td>\n",
       "      <td>10.996083</td>\n",
       "      <td>3.825744</td>\n",
       "      <td>6.837372</td>\n",
       "      <td>0.133371</td>\n",
       "      <td>0.126116</td>\n",
       "      <td>0.118862</td>\n",
       "      <td>0.123326</td>\n",
       "      <td>0.127790</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>39.642837</td>\n",
       "      <td>34.943807</td>\n",
       "      <td>39.682098</td>\n",
       "      <td>44.069116</td>\n",
       "      <td>44.069116</td>\n",
       "      <td>6.349204e+00</td>\n",
       "      <td>3.434290e-14</td>\n",
       "      <td>5.128029e+00</td>\n",
       "      <td>1.538409e+01</td>\n",
       "      <td>2.564014e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>10.996083</td>\n",
       "      <td>3.825744</td>\n",
       "      <td>6.837372</td>\n",
       "      <td>15.384086</td>\n",
       "      <td>0.126116</td>\n",
       "      <td>0.118862</td>\n",
       "      <td>0.123326</td>\n",
       "      <td>0.127790</td>\n",
       "      <td>0.127790</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>34.943807</td>\n",
       "      <td>39.682098</td>\n",
       "      <td>44.069116</td>\n",
       "      <td>44.069116</td>\n",
       "      <td>44.652683</td>\n",
       "      <td>3.434290e-14</td>\n",
       "      <td>5.128029e+00</td>\n",
       "      <td>1.538409e+01</td>\n",
       "      <td>2.564014e+01</td>\n",
       "      <td>3.140916e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.825744</td>\n",
       "      <td>6.837372</td>\n",
       "      <td>15.384086</td>\n",
       "      <td>24.144464</td>\n",
       "      <td>0.118862</td>\n",
       "      <td>0.123326</td>\n",
       "      <td>0.127790</td>\n",
       "      <td>0.127790</td>\n",
       "      <td>0.128348</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>39.682098</td>\n",
       "      <td>44.069116</td>\n",
       "      <td>44.069116</td>\n",
       "      <td>44.652683</td>\n",
       "      <td>39.347180</td>\n",
       "      <td>5.128029e+00</td>\n",
       "      <td>1.538409e+01</td>\n",
       "      <td>2.564014e+01</td>\n",
       "      <td>3.140916e+01</td>\n",
       "      <td>2.435812e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>6.837372</td>\n",
       "      <td>15.384086</td>\n",
       "      <td>24.144464</td>\n",
       "      <td>27.135808</td>\n",
       "      <td>0.123326</td>\n",
       "      <td>0.127790</td>\n",
       "      <td>0.127790</td>\n",
       "      <td>0.128348</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>44.069116</td>\n",
       "      <td>44.069116</td>\n",
       "      <td>44.652683</td>\n",
       "      <td>39.347180</td>\n",
       "      <td>39.347180</td>\n",
       "      <td>1.538409e+01</td>\n",
       "      <td>2.564014e+01</td>\n",
       "      <td>3.140916e+01</td>\n",
       "      <td>2.435812e+01</td>\n",
       "      <td>1.730708e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>15.384086</td>\n",
       "      <td>24.144464</td>\n",
       "      <td>27.135808</td>\n",
       "      <td>24.358119</td>\n",
       "      <td>0.127790</td>\n",
       "      <td>0.127790</td>\n",
       "      <td>0.128348</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>44.069116</td>\n",
       "      <td>44.652683</td>\n",
       "      <td>39.347180</td>\n",
       "      <td>39.347180</td>\n",
       "      <td>36.419652</td>\n",
       "      <td>2.564014e+01</td>\n",
       "      <td>3.140916e+01</td>\n",
       "      <td>2.435812e+01</td>\n",
       "      <td>1.730708e+01</td>\n",
       "      <td>6.410027e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>24.144464</td>\n",
       "      <td>27.135808</td>\n",
       "      <td>24.358119</td>\n",
       "      <td>16.025074</td>\n",
       "      <td>0.127790</td>\n",
       "      <td>0.128348</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>0.117746</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>44.652683</td>\n",
       "      <td>39.347180</td>\n",
       "      <td>39.347180</td>\n",
       "      <td>36.419652</td>\n",
       "      <td>35.603836</td>\n",
       "      <td>3.140916e+01</td>\n",
       "      <td>2.435812e+01</td>\n",
       "      <td>1.730708e+01</td>\n",
       "      <td>6.410027e+00</td>\n",
       "      <td>3.205014e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>27.135808</td>\n",
       "      <td>24.358119</td>\n",
       "      <td>16.025074</td>\n",
       "      <td>8.974039</td>\n",
       "      <td>0.128348</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>0.117746</td>\n",
       "      <td>0.116629</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>39.347180</td>\n",
       "      <td>39.347180</td>\n",
       "      <td>36.419652</td>\n",
       "      <td>35.603836</td>\n",
       "      <td>33.580461</td>\n",
       "      <td>2.435812e+01</td>\n",
       "      <td>1.730708e+01</td>\n",
       "      <td>6.410027e+00</td>\n",
       "      <td>3.205014e+00</td>\n",
       "      <td>3.197442e-14</td>\n",
       "      <td>...</td>\n",
       "      <td>24.358119</td>\n",
       "      <td>16.025074</td>\n",
       "      <td>8.974039</td>\n",
       "      <td>3.205014</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>0.117746</td>\n",
       "      <td>0.116629</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>39.347180</td>\n",
       "      <td>36.419652</td>\n",
       "      <td>35.603836</td>\n",
       "      <td>33.580461</td>\n",
       "      <td>37.411019</td>\n",
       "      <td>1.730708e+01</td>\n",
       "      <td>6.410027e+00</td>\n",
       "      <td>3.205014e+00</td>\n",
       "      <td>3.197442e-14</td>\n",
       "      <td>3.703404e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>16.025074</td>\n",
       "      <td>8.974039</td>\n",
       "      <td>3.205014</td>\n",
       "      <td>2.302806</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>0.117746</td>\n",
       "      <td>0.116629</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.116629</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36.419652</td>\n",
       "      <td>35.603836</td>\n",
       "      <td>33.580461</td>\n",
       "      <td>37.411019</td>\n",
       "      <td>43.705717</td>\n",
       "      <td>6.410027e+00</td>\n",
       "      <td>3.205014e+00</td>\n",
       "      <td>3.197442e-14</td>\n",
       "      <td>3.703404e+00</td>\n",
       "      <td>1.666673e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>8.974039</td>\n",
       "      <td>3.205014</td>\n",
       "      <td>2.302806</td>\n",
       "      <td>6.790046</td>\n",
       "      <td>0.117746</td>\n",
       "      <td>0.116629</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.116629</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>35.603836</td>\n",
       "      <td>33.580461</td>\n",
       "      <td>37.411019</td>\n",
       "      <td>43.705717</td>\n",
       "      <td>37.793469</td>\n",
       "      <td>3.205014e+00</td>\n",
       "      <td>3.197442e-14</td>\n",
       "      <td>3.703404e+00</td>\n",
       "      <td>1.666673e+01</td>\n",
       "      <td>1.790121e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.205014</td>\n",
       "      <td>2.302806</td>\n",
       "      <td>6.790046</td>\n",
       "      <td>12.757117</td>\n",
       "      <td>0.116629</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.116629</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>0.114397</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>33.580461</td>\n",
       "      <td>37.411019</td>\n",
       "      <td>43.705717</td>\n",
       "      <td>37.793469</td>\n",
       "      <td>33.646524</td>\n",
       "      <td>3.197442e-14</td>\n",
       "      <td>3.703404e+00</td>\n",
       "      <td>1.666673e+01</td>\n",
       "      <td>1.790121e+01</td>\n",
       "      <td>1.419781e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.302806</td>\n",
       "      <td>6.790046</td>\n",
       "      <td>12.757117</td>\n",
       "      <td>16.255252</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.116629</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>0.114397</td>\n",
       "      <td>0.108259</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>37.411019</td>\n",
       "      <td>43.705717</td>\n",
       "      <td>37.793469</td>\n",
       "      <td>33.646524</td>\n",
       "      <td>35.718155</td>\n",
       "      <td>3.703404e+00</td>\n",
       "      <td>1.666673e+01</td>\n",
       "      <td>1.790121e+01</td>\n",
       "      <td>1.419781e+01</td>\n",
       "      <td>3.937045e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.790046</td>\n",
       "      <td>12.757117</td>\n",
       "      <td>16.255252</td>\n",
       "      <td>12.012022</td>\n",
       "      <td>0.116629</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>0.114397</td>\n",
       "      <td>0.108259</td>\n",
       "      <td>0.109933</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>43.705717</td>\n",
       "      <td>37.793469</td>\n",
       "      <td>33.646524</td>\n",
       "      <td>35.718155</td>\n",
       "      <td>33.119800</td>\n",
       "      <td>1.666673e+01</td>\n",
       "      <td>1.790121e+01</td>\n",
       "      <td>1.419781e+01</td>\n",
       "      <td>3.937045e+00</td>\n",
       "      <td>2.702566e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>12.757117</td>\n",
       "      <td>16.255252</td>\n",
       "      <td>12.012022</td>\n",
       "      <td>6.945807</td>\n",
       "      <td>0.121652</td>\n",
       "      <td>0.114397</td>\n",
       "      <td>0.108259</td>\n",
       "      <td>0.109933</td>\n",
       "      <td>0.106027</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>37.793469</td>\n",
       "      <td>33.646524</td>\n",
       "      <td>35.718155</td>\n",
       "      <td>33.119800</td>\n",
       "      <td>41.038378</td>\n",
       "      <td>1.790121e+01</td>\n",
       "      <td>1.419781e+01</td>\n",
       "      <td>3.937045e+00</td>\n",
       "      <td>2.702566e+00</td>\n",
       "      <td>1.245824e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>16.255252</td>\n",
       "      <td>12.012022</td>\n",
       "      <td>6.945807</td>\n",
       "      <td>6.365950</td>\n",
       "      <td>0.114397</td>\n",
       "      <td>0.108259</td>\n",
       "      <td>0.109933</td>\n",
       "      <td>0.106027</td>\n",
       "      <td>0.112723</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         rsi0       rsi1       rsi2       rsi3       rsi4       stochk0  \\\n",
       "0   52.376308  56.819433  55.954525  52.110399  52.500873  5.043602e+01   \n",
       "1   56.819433  55.954525  52.110399  52.500873  54.498568  4.432403e+01   \n",
       "2   55.954525  52.110399  52.500873  54.498568  53.528777  3.832033e+01   \n",
       "3   52.110399  52.500873  54.498568  53.528777  59.368572  2.701922e+01   \n",
       "4   52.500873  54.498568  53.528777  59.368572  55.372003  1.380280e+01   \n",
       "5   54.498568  53.528777  59.368572  55.372003  57.306456  9.219494e+00   \n",
       "6   53.528777  59.368572  55.372003  57.306456  58.469647  1.205627e+01   \n",
       "7   59.368572  55.372003  57.306456  58.469647  57.903330  2.742790e+01   \n",
       "8   55.372003  57.306456  58.469647  57.903330  55.584215  3.379078e+01   \n",
       "9   57.306456  58.469647  57.903330  55.584215  54.410790  4.620146e+01   \n",
       "10  58.469647  57.903330  55.584215  54.410790  49.875176  5.283868e+01   \n",
       "11  57.903330  55.584215  54.410790  49.875176  45.300202  6.978774e+01   \n",
       "12  55.584215  54.410790  49.875176  45.300202  39.642837  7.631029e+01   \n",
       "13  54.410790  49.875176  45.300202  39.642837  34.943807  7.246369e+01   \n",
       "14  49.875176  45.300202  39.642837  34.943807  39.682098  4.982742e+01   \n",
       "15  45.300202  39.642837  34.943807  39.682098  44.069116  2.663905e+01   \n",
       "16  39.642837  34.943807  39.682098  44.069116  44.069116  6.349204e+00   \n",
       "17  34.943807  39.682098  44.069116  44.069116  44.652683  3.434290e-14   \n",
       "18  39.682098  44.069116  44.069116  44.652683  39.347180  5.128029e+00   \n",
       "19  44.069116  44.069116  44.652683  39.347180  39.347180  1.538409e+01   \n",
       "20  44.069116  44.652683  39.347180  39.347180  36.419652  2.564014e+01   \n",
       "21  44.652683  39.347180  39.347180  36.419652  35.603836  3.140916e+01   \n",
       "22  39.347180  39.347180  36.419652  35.603836  33.580461  2.435812e+01   \n",
       "23  39.347180  36.419652  35.603836  33.580461  37.411019  1.730708e+01   \n",
       "24  36.419652  35.603836  33.580461  37.411019  43.705717  6.410027e+00   \n",
       "25  35.603836  33.580461  37.411019  43.705717  37.793469  3.205014e+00   \n",
       "26  33.580461  37.411019  43.705717  37.793469  33.646524  3.197442e-14   \n",
       "27  37.411019  43.705717  37.793469  33.646524  35.718155  3.703404e+00   \n",
       "28  43.705717  37.793469  33.646524  35.718155  33.119800  1.666673e+01   \n",
       "29  37.793469  33.646524  35.718155  33.119800  41.038378  1.790121e+01   \n",
       "\n",
       "         stochk1       stochk2       stochk3       stochk4  ...    stochd1  \\\n",
       "0   4.432403e+01  3.832033e+01  2.701922e+01  1.380280e+01  ...  52.843555   \n",
       "1   3.832033e+01  2.701922e+01  1.380280e+01  9.219494e+00  ...  44.360123   \n",
       "2   2.701922e+01  1.380280e+01  9.219494e+00  1.205627e+01  ...  36.554524   \n",
       "3   1.380280e+01  9.219494e+00  1.205627e+01  2.742790e+01  ...  26.380782   \n",
       "4   9.219494e+00  1.205627e+01  2.742790e+01  3.379078e+01  ...  16.680504   \n",
       "5   1.205627e+01  2.742790e+01  3.379078e+01  4.620146e+01  ...  11.692854   \n",
       "6   2.742790e+01  3.379078e+01  4.620146e+01  5.283868e+01  ...  16.234555   \n",
       "7   3.379078e+01  4.620146e+01  5.283868e+01  6.978774e+01  ...  24.424984   \n",
       "8   4.620146e+01  5.283868e+01  6.978774e+01  7.631029e+01  ...  35.806715   \n",
       "9   5.283868e+01  6.978774e+01  7.631029e+01  7.246369e+01  ...  44.276974   \n",
       "10  6.978774e+01  7.631029e+01  7.246369e+01  4.982742e+01  ...  56.275960   \n",
       "11  7.631029e+01  7.246369e+01  4.982742e+01  2.663905e+01  ...  66.312236   \n",
       "12  7.246369e+01  4.982742e+01  2.663905e+01  6.349204e+00  ...  72.853906   \n",
       "13  4.982742e+01  2.663905e+01  6.349204e+00  3.434290e-14  ...  66.200466   \n",
       "14  2.663905e+01  6.349204e+00  3.434290e-14  5.128029e+00  ...  49.643386   \n",
       "15  6.349204e+00  3.434290e-14  5.128029e+00  1.538409e+01  ...  27.605223   \n",
       "16  3.434290e-14  5.128029e+00  1.538409e+01  2.564014e+01  ...  10.996083   \n",
       "17  5.128029e+00  1.538409e+01  2.564014e+01  3.140916e+01  ...   3.825744   \n",
       "18  1.538409e+01  2.564014e+01  3.140916e+01  2.435812e+01  ...   6.837372   \n",
       "19  2.564014e+01  3.140916e+01  2.435812e+01  1.730708e+01  ...  15.384086   \n",
       "20  3.140916e+01  2.435812e+01  1.730708e+01  6.410027e+00  ...  24.144464   \n",
       "21  2.435812e+01  1.730708e+01  6.410027e+00  3.205014e+00  ...  27.135808   \n",
       "22  1.730708e+01  6.410027e+00  3.205014e+00  3.197442e-14  ...  24.358119   \n",
       "23  6.410027e+00  3.205014e+00  3.197442e-14  3.703404e+00  ...  16.025074   \n",
       "24  3.205014e+00  3.197442e-14  3.703404e+00  1.666673e+01  ...   8.974039   \n",
       "25  3.197442e-14  3.703404e+00  1.666673e+01  1.790121e+01  ...   3.205014   \n",
       "26  3.703404e+00  1.666673e+01  1.790121e+01  1.419781e+01  ...   2.302806   \n",
       "27  1.666673e+01  1.790121e+01  1.419781e+01  3.937045e+00  ...   6.790046   \n",
       "28  1.790121e+01  1.419781e+01  3.937045e+00  2.702566e+00  ...  12.757117   \n",
       "29  1.419781e+01  3.937045e+00  2.702566e+00  1.245824e+01  ...  16.255252   \n",
       "\n",
       "      stochd2    stochd3    stochd4    close0    close1    close2    close3  \\\n",
       "0   44.360123  36.554524  26.380782  0.135045  0.142299  0.141183  0.136161   \n",
       "1   36.554524  26.380782  16.680504  0.142299  0.141183  0.136161  0.136719   \n",
       "2   26.380782  16.680504  11.692854  0.141183  0.136161  0.136719  0.139509   \n",
       "3   16.680504  11.692854  16.234555  0.136161  0.136719  0.139509  0.138393   \n",
       "4   11.692854  16.234555  24.424984  0.136719  0.139509  0.138393  0.146763   \n",
       "5   16.234555  24.424984  35.806715  0.139509  0.138393  0.146763  0.142299   \n",
       "6   24.424984  35.806715  44.276974  0.138393  0.146763  0.142299  0.145089   \n",
       "7   35.806715  44.276974  56.275960  0.146763  0.142299  0.145089  0.146763   \n",
       "8   44.276974  56.275960  66.312236  0.142299  0.145089  0.146763  0.146205   \n",
       "9   56.275960  66.312236  72.853906  0.145089  0.146763  0.146205  0.143973   \n",
       "10  66.312236  72.853906  66.200466  0.146763  0.146205  0.143973  0.142857   \n",
       "11  72.853906  66.200466  49.643386  0.146205  0.143973  0.142857  0.138393   \n",
       "12  66.200466  49.643386  27.605223  0.143973  0.142857  0.138393  0.133371   \n",
       "13  49.643386  27.605223  10.996083  0.142857  0.138393  0.133371  0.126116   \n",
       "14  27.605223  10.996083   3.825744  0.138393  0.133371  0.126116  0.118862   \n",
       "15  10.996083   3.825744   6.837372  0.133371  0.126116  0.118862  0.123326   \n",
       "16   3.825744   6.837372  15.384086  0.126116  0.118862  0.123326  0.127790   \n",
       "17   6.837372  15.384086  24.144464  0.118862  0.123326  0.127790  0.127790   \n",
       "18  15.384086  24.144464  27.135808  0.123326  0.127790  0.127790  0.128348   \n",
       "19  24.144464  27.135808  24.358119  0.127790  0.127790  0.128348  0.121652   \n",
       "20  27.135808  24.358119  16.025074  0.127790  0.128348  0.121652  0.121652   \n",
       "21  24.358119  16.025074   8.974039  0.128348  0.121652  0.121652  0.117746   \n",
       "22  16.025074   8.974039   3.205014  0.121652  0.121652  0.117746  0.116629   \n",
       "23   8.974039   3.205014   2.302806  0.121652  0.117746  0.116629  0.113839   \n",
       "24   3.205014   2.302806   6.790046  0.117746  0.116629  0.113839  0.116629   \n",
       "25   2.302806   6.790046  12.757117  0.116629  0.113839  0.116629  0.121652   \n",
       "26   6.790046  12.757117  16.255252  0.113839  0.116629  0.121652  0.114397   \n",
       "27  12.757117  16.255252  12.012022  0.116629  0.121652  0.114397  0.108259   \n",
       "28  16.255252  12.012022   6.945807  0.121652  0.114397  0.108259  0.109933   \n",
       "29  12.012022   6.945807   6.365950  0.114397  0.108259  0.109933  0.106027   \n",
       "\n",
       "      close4  long_or_short  \n",
       "0   0.136719            1.0  \n",
       "1   0.139509            1.0  \n",
       "2   0.138393            0.0  \n",
       "3   0.146763            1.0  \n",
       "4   0.142299            0.0  \n",
       "5   0.145089            0.0  \n",
       "6   0.146763            0.0  \n",
       "7   0.146205            0.0  \n",
       "8   0.143973            0.0  \n",
       "9   0.142857            0.0  \n",
       "10  0.138393            0.0  \n",
       "11  0.133371            1.0  \n",
       "12  0.126116            1.0  \n",
       "13  0.118862            0.0  \n",
       "14  0.123326            0.0  \n",
       "15  0.127790            0.0  \n",
       "16  0.127790            0.0  \n",
       "17  0.128348            0.0  \n",
       "18  0.121652            1.0  \n",
       "19  0.121652            0.0  \n",
       "20  0.117746            0.0  \n",
       "21  0.116629            0.0  \n",
       "22  0.113839            0.0  \n",
       "23  0.116629            0.0  \n",
       "24  0.121652            1.0  \n",
       "25  0.114397            1.0  \n",
       "26  0.108259            1.0  \n",
       "27  0.109933            1.0  \n",
       "28  0.106027            1.0  \n",
       "29  0.112723            1.0  \n",
       "\n",
       "[30 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtest_company_price_data.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>RSI_14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1981-01-05</th>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.150670</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.150670</td>\n",
       "      <td>35728000.0</td>\n",
       "      <td>0.118289</td>\n",
       "      <td>64.493312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-06</th>\n",
       "      <td>0.144531</td>\n",
       "      <td>0.143973</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>0.143973</td>\n",
       "      <td>45158400.0</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>58.970446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-07</th>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.137835</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.137835</td>\n",
       "      <td>55686400.0</td>\n",
       "      <td>0.108213</td>\n",
       "      <td>54.374496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-08</th>\n",
       "      <td>0.135603</td>\n",
       "      <td>0.135045</td>\n",
       "      <td>0.135603</td>\n",
       "      <td>0.135045</td>\n",
       "      <td>39827200.0</td>\n",
       "      <td>0.106022</td>\n",
       "      <td>52.376308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-09</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>21504000.0</td>\n",
       "      <td>0.111717</td>\n",
       "      <td>56.819433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-12</th>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.141183</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.141183</td>\n",
       "      <td>23699200.0</td>\n",
       "      <td>0.110841</td>\n",
       "      <td>55.954525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-13</th>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>23049600.0</td>\n",
       "      <td>0.106898</td>\n",
       "      <td>52.110399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-14</th>\n",
       "      <td>0.137277</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>14291200.0</td>\n",
       "      <td>0.107337</td>\n",
       "      <td>52.500873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-15</th>\n",
       "      <td>0.140625</td>\n",
       "      <td>0.139509</td>\n",
       "      <td>0.139509</td>\n",
       "      <td>0.139509</td>\n",
       "      <td>14067200.0</td>\n",
       "      <td>0.109527</td>\n",
       "      <td>54.498568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-16</th>\n",
       "      <td>0.138951</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.138951</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>13395200.0</td>\n",
       "      <td>0.108651</td>\n",
       "      <td>53.528777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-19</th>\n",
       "      <td>0.147321</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>41574400.0</td>\n",
       "      <td>0.115222</td>\n",
       "      <td>59.368572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-20</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>30083200.0</td>\n",
       "      <td>0.111717</td>\n",
       "      <td>55.372003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-21</th>\n",
       "      <td>0.146205</td>\n",
       "      <td>0.145089</td>\n",
       "      <td>0.145089</td>\n",
       "      <td>0.145089</td>\n",
       "      <td>15904000.0</td>\n",
       "      <td>0.113908</td>\n",
       "      <td>57.306456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-22</th>\n",
       "      <td>0.147879</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>35548800.0</td>\n",
       "      <td>0.115222</td>\n",
       "      <td>58.469647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-23</th>\n",
       "      <td>0.147321</td>\n",
       "      <td>0.146205</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.146205</td>\n",
       "      <td>11222400.0</td>\n",
       "      <td>0.114784</td>\n",
       "      <td>57.903330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-26</th>\n",
       "      <td>0.144531</td>\n",
       "      <td>0.143973</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>0.143973</td>\n",
       "      <td>24640000.0</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>55.584215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-27</th>\n",
       "      <td>0.143973</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.143973</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>23699200.0</td>\n",
       "      <td>0.112155</td>\n",
       "      <td>54.410790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-28</th>\n",
       "      <td>0.138951</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.138951</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>28156800.0</td>\n",
       "      <td>0.108651</td>\n",
       "      <td>49.875176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-29</th>\n",
       "      <td>0.133929</td>\n",
       "      <td>0.133371</td>\n",
       "      <td>0.133929</td>\n",
       "      <td>0.133371</td>\n",
       "      <td>43904000.0</td>\n",
       "      <td>0.104708</td>\n",
       "      <td>45.300202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-30</th>\n",
       "      <td>0.127232</td>\n",
       "      <td>0.126116</td>\n",
       "      <td>0.127232</td>\n",
       "      <td>0.126116</td>\n",
       "      <td>46188800.0</td>\n",
       "      <td>0.099012</td>\n",
       "      <td>39.642837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                High       Low      Open     Close      Volume  Adj Close  \\\n",
       "Date                                                                        \n",
       "1981-01-05  0.151228  0.150670  0.151228  0.150670  35728000.0   0.118289   \n",
       "1981-01-06  0.144531  0.143973  0.144531  0.143973  45158400.0   0.113032   \n",
       "1981-01-07  0.138393  0.137835  0.138393  0.137835  55686400.0   0.108213   \n",
       "1981-01-08  0.135603  0.135045  0.135603  0.135045  39827200.0   0.106022   \n",
       "1981-01-09  0.142857  0.142299  0.142299  0.142299  21504000.0   0.111717   \n",
       "1981-01-12  0.142299  0.141183  0.142299  0.141183  23699200.0   0.110841   \n",
       "1981-01-13  0.136719  0.136161  0.136719  0.136161  23049600.0   0.106898   \n",
       "1981-01-14  0.137277  0.136719  0.136719  0.136719  14291200.0   0.107337   \n",
       "1981-01-15  0.140625  0.139509  0.139509  0.139509  14067200.0   0.109527   \n",
       "1981-01-16  0.138951  0.138393  0.138951  0.138393  13395200.0   0.108651   \n",
       "1981-01-19  0.147321  0.146763  0.146763  0.146763  41574400.0   0.115222   \n",
       "1981-01-20  0.142857  0.142299  0.142857  0.142299  30083200.0   0.111717   \n",
       "1981-01-21  0.146205  0.145089  0.145089  0.145089  15904000.0   0.113908   \n",
       "1981-01-22  0.147879  0.146763  0.146763  0.146763  35548800.0   0.115222   \n",
       "1981-01-23  0.147321  0.146205  0.146763  0.146205  11222400.0   0.114784   \n",
       "1981-01-26  0.144531  0.143973  0.144531  0.143973  24640000.0   0.113032   \n",
       "1981-01-27  0.143973  0.142857  0.143973  0.142857  23699200.0   0.112155   \n",
       "1981-01-28  0.138951  0.138393  0.138951  0.138393  28156800.0   0.108651   \n",
       "1981-01-29  0.133929  0.133371  0.133929  0.133371  43904000.0   0.104708   \n",
       "1981-01-30  0.127232  0.126116  0.127232  0.126116  46188800.0   0.099012   \n",
       "\n",
       "               RSI_14  \n",
       "Date                   \n",
       "1981-01-05  64.493312  \n",
       "1981-01-06  58.970446  \n",
       "1981-01-07  54.374496  \n",
       "1981-01-08  52.376308  \n",
       "1981-01-09  56.819433  \n",
       "1981-01-12  55.954525  \n",
       "1981-01-13  52.110399  \n",
       "1981-01-14  52.500873  \n",
       "1981-01-15  54.498568  \n",
       "1981-01-16  53.528777  \n",
       "1981-01-19  59.368572  \n",
       "1981-01-20  55.372003  \n",
       "1981-01-21  57.306456  \n",
       "1981-01-22  58.469647  \n",
       "1981-01-23  57.903330  \n",
       "1981-01-26  55.584215  \n",
       "1981-01-27  54.410790  \n",
       "1981-01-28  49.875176  \n",
       "1981-01-29  45.300202  \n",
       "1981-01-30  39.642837  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_datareader_data.dropna().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    5686\n",
       "0.0    4728\n",
       "Name: long_or_short, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtest_company_price_data['long_or_short'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
